{
    "version": 3,
    "terraform_version": "0.9.2",
    "serial": 7,
    "lineage": "f0326d29-eaf4-4541-a919-86ba5dc25d5a",
    "modules": [
        {
            "path": [
                "root"
            ],
            "outputs": {},
            "resources": {
                "aws_autoscaling_group.elasticsearch_node_asg.0": {
                    "type": "aws_autoscaling_group",
                    "depends_on": [
                        "aws_launch_configuration.elasticsearch_node"
                    ],
                    "primary": {
                        "id": "test-ca-es-node-az1-asg",
                        "attributes": {
                            "arn": "arn:aws:autoscaling:us-west-2:999547976641:autoScalingGroup:7348dd2d-288b-4312-918c-4c86252f1b24:autoScalingGroupName/test-ca-es-node-az1-asg",
                            "availability_zones.#": "1",
                            "availability_zones.2487133097": "us-west-2a",
                            "default_cooldown": "300",
                            "desired_capacity": "1",
                            "force_delete": "false",
                            "health_check_grace_period": "300",
                            "health_check_type": "EC2",
                            "id": "test-ca-es-node-az1-asg",
                            "launch_configuration": "ca-elasticsearch-node-20170407210753623765560meh",
                            "load_balancers.#": "0",
                            "max_size": "1",
                            "metrics_granularity": "1Minute",
                            "min_size": "1",
                            "name": "test-ca-es-node-az1-asg",
                            "placement_group": "",
                            "protect_from_scale_in": "false",
                            "suspended_processes.#": "0",
                            "tag.#": "7",
                            "tag.1307843333.key": "jive_subservice",
                            "tag.1307843333.propagate_at_launch": "true",
                            "tag.1307843333.value": "cloudalytics",
                            "tag.1907490845.key": "make_dns",
                            "tag.1907490845.propagate_at_launch": "true",
                            "tag.1907490845.value": "node.aws-test-cloudalytics",
                            "tag.2220074062.key": "sla",
                            "tag.2220074062.propagate_at_launch": "true",
                            "tag.2220074062.value": "non-prod",
                            "tag.2499258522.key": "Name",
                            "tag.2499258522.propagate_at_launch": "true",
                            "tag.2499258522.value": "test-ca-es-node",
                            "tag.2950372525.key": "service_component",
                            "tag.2950372525.propagate_at_launch": "true",
                            "tag.2950372525.value": "es_node",
                            "tag.2981776880.key": "jive_service",
                            "tag.2981776880.propagate_at_launch": "true",
                            "tag.2981776880.value": "cloudalytics",
                            "tag.3730876864.key": "pipeline_phase",
                            "tag.3730876864.propagate_at_launch": "true",
                            "tag.3730876864.value": "test",
                            "target_group_arns.#": "0",
                            "termination_policies.#": "0",
                            "vpc_zone_identifier.#": "1",
                            "vpc_zone_identifier.1736361213": "subnet-5def242b",
                            "wait_for_capacity_timeout": "10m"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_autoscaling_group.elasticsearch_node_asg.1": {
                    "type": "aws_autoscaling_group",
                    "depends_on": [
                        "aws_launch_configuration.elasticsearch_node"
                    ],
                    "primary": {
                        "id": "test-ca-es-node-az2-asg",
                        "attributes": {
                            "arn": "arn:aws:autoscaling:us-west-2:999547976641:autoScalingGroup:75ce0c13-4e77-46cb-b0ae-4e790476354f:autoScalingGroupName/test-ca-es-node-az2-asg",
                            "availability_zones.#": "1",
                            "availability_zones.221770259": "us-west-2b",
                            "default_cooldown": "300",
                            "desired_capacity": "1",
                            "force_delete": "false",
                            "health_check_grace_period": "300",
                            "health_check_type": "EC2",
                            "id": "test-ca-es-node-az2-asg",
                            "launch_configuration": "ca-elasticsearch-node-20170407210753623765560meh",
                            "load_balancers.#": "0",
                            "max_size": "1",
                            "metrics_granularity": "1Minute",
                            "min_size": "1",
                            "name": "test-ca-es-node-az2-asg",
                            "placement_group": "",
                            "protect_from_scale_in": "false",
                            "suspended_processes.#": "0",
                            "tag.#": "7",
                            "tag.1307843333.key": "jive_subservice",
                            "tag.1307843333.propagate_at_launch": "true",
                            "tag.1307843333.value": "cloudalytics",
                            "tag.1907490845.key": "make_dns",
                            "tag.1907490845.propagate_at_launch": "true",
                            "tag.1907490845.value": "node.aws-test-cloudalytics",
                            "tag.2220074062.key": "sla",
                            "tag.2220074062.propagate_at_launch": "true",
                            "tag.2220074062.value": "non-prod",
                            "tag.2499258522.key": "Name",
                            "tag.2499258522.propagate_at_launch": "true",
                            "tag.2499258522.value": "test-ca-es-node",
                            "tag.2950372525.key": "service_component",
                            "tag.2950372525.propagate_at_launch": "true",
                            "tag.2950372525.value": "es_node",
                            "tag.2981776880.key": "jive_service",
                            "tag.2981776880.propagate_at_launch": "true",
                            "tag.2981776880.value": "cloudalytics",
                            "tag.3730876864.key": "pipeline_phase",
                            "tag.3730876864.propagate_at_launch": "true",
                            "tag.3730876864.value": "test",
                            "target_group_arns.#": "0",
                            "termination_policies.#": "0",
                            "vpc_zone_identifier.#": "1",
                            "vpc_zone_identifier.1328737188": "subnet-cb42f4af",
                            "wait_for_capacity_timeout": "10m"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_autoscaling_group.elasticsearch_node_asg.2": {
                    "type": "aws_autoscaling_group",
                    "depends_on": [
                        "aws_launch_configuration.elasticsearch_node"
                    ],
                    "primary": {
                        "id": "test-ca-es-node-az3-asg",
                        "attributes": {
                            "arn": "arn:aws:autoscaling:us-west-2:999547976641:autoScalingGroup:4e855f92-cfb5-43d9-bbbf-7616f6b73cd8:autoScalingGroupName/test-ca-es-node-az3-asg",
                            "availability_zones.#": "1",
                            "availability_zones.2050015877": "us-west-2c",
                            "default_cooldown": "300",
                            "desired_capacity": "1",
                            "force_delete": "false",
                            "health_check_grace_period": "300",
                            "health_check_type": "EC2",
                            "id": "test-ca-es-node-az3-asg",
                            "launch_configuration": "ca-elasticsearch-node-20170407210753623765560meh",
                            "load_balancers.#": "0",
                            "max_size": "1",
                            "metrics_granularity": "1Minute",
                            "min_size": "1",
                            "name": "test-ca-es-node-az3-asg",
                            "placement_group": "",
                            "protect_from_scale_in": "false",
                            "suspended_processes.#": "0",
                            "tag.#": "7",
                            "tag.1307843333.key": "jive_subservice",
                            "tag.1307843333.propagate_at_launch": "true",
                            "tag.1307843333.value": "cloudalytics",
                            "tag.1907490845.key": "make_dns",
                            "tag.1907490845.propagate_at_launch": "true",
                            "tag.1907490845.value": "node.aws-test-cloudalytics",
                            "tag.2220074062.key": "sla",
                            "tag.2220074062.propagate_at_launch": "true",
                            "tag.2220074062.value": "non-prod",
                            "tag.2499258522.key": "Name",
                            "tag.2499258522.propagate_at_launch": "true",
                            "tag.2499258522.value": "test-ca-es-node",
                            "tag.2950372525.key": "service_component",
                            "tag.2950372525.propagate_at_launch": "true",
                            "tag.2950372525.value": "es_node",
                            "tag.2981776880.key": "jive_service",
                            "tag.2981776880.propagate_at_launch": "true",
                            "tag.2981776880.value": "cloudalytics",
                            "tag.3730876864.key": "pipeline_phase",
                            "tag.3730876864.propagate_at_launch": "true",
                            "tag.3730876864.value": "test",
                            "target_group_arns.#": "0",
                            "termination_policies.#": "0",
                            "vpc_zone_identifier.#": "1",
                            "vpc_zone_identifier.1696862121": "subnet-47e4ff1e",
                            "wait_for_capacity_timeout": "10m"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_autoscaling_group.nginx_node_asg": {
                    "type": "aws_autoscaling_group",
                    "depends_on": [
                        "aws_launch_configuration.nginx_node"
                    ],
                    "primary": {
                        "id": "test-ca-es-nginx-az1-asg",
                        "attributes": {
                            "arn": "arn:aws:autoscaling:us-west-2:999547976641:autoScalingGroup:31db44b1-8e91-4e1a-9886-565ca26c77b7:autoScalingGroupName/test-ca-es-nginx-az1-asg",
                            "availability_zones.#": "1",
                            "availability_zones.2487133097": "us-west-2a",
                            "default_cooldown": "300",
                            "desired_capacity": "1",
                            "force_delete": "false",
                            "health_check_grace_period": "300",
                            "health_check_type": "EC2",
                            "id": "test-ca-es-nginx-az1-asg",
                            "launch_configuration": "ca-nginx-es-node-20170407210753622238823rpu",
                            "load_balancers.#": "0",
                            "max_size": "1",
                            "metrics_granularity": "1Minute",
                            "min_size": "1",
                            "name": "test-ca-es-nginx-az1-asg",
                            "placement_group": "",
                            "protect_from_scale_in": "false",
                            "suspended_processes.#": "0",
                            "tag.#": "7",
                            "tag.1307843333.key": "jive_subservice",
                            "tag.1307843333.propagate_at_launch": "true",
                            "tag.1307843333.value": "cloudalytics",
                            "tag.1411166644.key": "Name",
                            "tag.1411166644.propagate_at_launch": "true",
                            "tag.1411166644.value": "test-ca-es-nginx",
                            "tag.2220074062.key": "sla",
                            "tag.2220074062.propagate_at_launch": "true",
                            "tag.2220074062.value": "non-prod",
                            "tag.2981776880.key": "jive_service",
                            "tag.2981776880.propagate_at_launch": "true",
                            "tag.2981776880.value": "cloudalytics",
                            "tag.3730876864.key": "pipeline_phase",
                            "tag.3730876864.propagate_at_launch": "true",
                            "tag.3730876864.value": "test",
                            "tag.388375835.key": "make_dns",
                            "tag.388375835.propagate_at_launch": "true",
                            "tag.388375835.value": "es-nginx.aws-test-cloudalytics",
                            "tag.3969529871.key": "service_component",
                            "tag.3969529871.propagate_at_launch": "true",
                            "tag.3969529871.value": "es_nginx",
                            "target_group_arns.#": "0",
                            "termination_policies.#": "0",
                            "vpc_zone_identifier.#": "1",
                            "vpc_zone_identifier.1736361213": "subnet-5def242b",
                            "wait_for_capacity_timeout": "10m"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_iam_instance_profile.ebs-attach-and-secrets": {
                    "type": "aws_iam_instance_profile",
                    "depends_on": [
                        "aws_iam_role.ebs-attach-and-secrets"
                    ],
                    "primary": {
                        "id": "ebs-attach-and-secrets-profile-cloudalytics",
                        "attributes": {
                            "arn": "arn:aws:iam::999547976641:instance-profile/ebs-attach-and-secrets-profile-cloudalytics",
                            "id": "ebs-attach-and-secrets-profile-cloudalytics",
                            "name": "ebs-attach-and-secrets-profile-cloudalytics",
                            "path": "/",
                            "roles.#": "1",
                            "roles.1316683226": "ebs-attach-and-secrets-role-cloudalytics",
                            "unique_id": "AIPAJJJR5PQ3AIV6OPKZ6"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_iam_role.ebs-attach-and-secrets": {
                    "type": "aws_iam_role",
                    "depends_on": [],
                    "primary": {
                        "id": "ebs-attach-and-secrets-role-cloudalytics",
                        "attributes": {
                            "arn": "arn:aws:iam::999547976641:role/ebs-attach-and-secrets-role-cloudalytics",
                            "assume_role_policy": "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Sid\":\"\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"ec2.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}",
                            "create_date": "2017-03-23T20:02:47Z",
                            "id": "ebs-attach-and-secrets-role-cloudalytics",
                            "name": "ebs-attach-and-secrets-role-cloudalytics",
                            "path": "/",
                            "unique_id": "AROAJ5TV7RU2NHCB7VGQE"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_iam_role_policy.ebs-attach-and-secrets": {
                    "type": "aws_iam_role_policy",
                    "depends_on": [
                        "aws_iam_role.ebs-attach-and-secrets"
                    ],
                    "primary": {
                        "id": "ebs-attach-and-secrets-role-cloudalytics:ebs-attach-and-secrets-policy-cloudalytics",
                        "attributes": {
                            "id": "ebs-attach-and-secrets-role-cloudalytics:ebs-attach-and-secrets-policy-cloudalytics",
                            "name": "ebs-attach-and-secrets-policy-cloudalytics",
                            "policy": "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n              \"Effect\": \"Allow\",\n              \"Action\": [\n                \"ec2:AttachVolume\",\n                \"ec2:CreateTags\",\n                \"ec2:Describe*\",\n                \"ec2:DetachVolume\",\n                \"elasticache:Describe*\",\n                \"rds:Describe*\",\n                \"route53:ListHostedZones\",\n                \"route53:ListResourceRecordSets\",\n                \"s3:ListAllMyBuckets\"\n              ],\n              \"Resource\": \"*\"\n        },\n        {\n              \"Effect\": \"Allow\",\n              \"Action\": [\n                  \"s3:GetBucketLocation\",\n                  \"s3:GetObject\",\n                  \"s3:ListBucket\"\n              ],\n              \"Resource\": [\n                  \"arn:aws:s3:::us-west-2-jive-data-test-secrets\",\n                  \"arn:aws:s3:::us-west-2-jive-data-test-secrets/*\",\n                  \"arn:aws:s3:::us-west-2-jive-data-pipeline-playbooks\",\n                  \"arn:aws:s3:::us-west-2-jive-data-pipeline-playbooks/*\"\n              ]\n        }\n    ]\n}\n",
                            "role": "ebs-attach-and-secrets-role-cloudalytics"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_launch_configuration.elasticsearch_master": {
                    "type": "aws_launch_configuration",
                    "depends_on": [
                        "aws_security_group.elasticsearch_ports",
                        "data.template_file.elasticsearch_user_data"
                    ],
                    "primary": {
                        "id": "ca-elasticsearch-master-20170407210753631805090o7a",
                        "attributes": {
                            "associate_public_ip_address": "false",
                            "ebs_block_device.#": "0",
                            "ebs_optimized": "false",
                            "enable_monitoring": "true",
                            "ephemeral_block_device.#": "0",
                            "iam_instance_profile": "ebs-attach-and-secrets-profile-cloudalytics",
                            "id": "ca-elasticsearch-master-20170407210753631805090o7a",
                            "image_id": "ami-d2c924b2",
                            "instance_type": "r4.large",
                            "key_name": "data-pipeline",
                            "name": "ca-elasticsearch-master-20170407210753631805090o7a",
                            "name_prefix": "ca-elasticsearch-master-",
                            "root_block_device.#": "0",
                            "security_groups.#": "2",
                            "security_groups.144082702": "sg-0db8bc74",
                            "security_groups.929541476": "sg-1ecac279",
                            "spot_price": "",
                            "user_data": "ee61af0c214387b358cdfe085de75c686ebaaffb",
                            "vpc_classic_link_id": "",
                            "vpc_classic_link_security_groups.#": "0"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_launch_configuration.elasticsearch_node": {
                    "type": "aws_launch_configuration",
                    "depends_on": [
                        "aws_security_group.elasticsearch_ports",
                        "data.template_file.elasticsearch_user_data"
                    ],
                    "primary": {
                        "id": "ca-elasticsearch-node-20170407210753623765560meh",
                        "attributes": {
                            "associate_public_ip_address": "false",
                            "ebs_block_device.#": "1",
                            "ebs_block_device.3817660160.delete_on_termination": "false",
                            "ebs_block_device.3817660160.device_name": "/dev/xvdm",
                            "ebs_block_device.3817660160.encrypted": "true",
                            "ebs_block_device.3817660160.iops": "0",
                            "ebs_block_device.3817660160.snapshot_id": "",
                            "ebs_block_device.3817660160.volume_size": "500",
                            "ebs_block_device.3817660160.volume_type": "gp2",
                            "ebs_optimized": "true",
                            "enable_monitoring": "true",
                            "ephemeral_block_device.#": "0",
                            "iam_instance_profile": "ebs-attach-and-secrets-profile-cloudalytics",
                            "id": "ca-elasticsearch-node-20170407210753623765560meh",
                            "image_id": "ami-d2c924b2",
                            "instance_type": "r4.xlarge",
                            "key_name": "data-pipeline",
                            "name": "ca-elasticsearch-node-20170407210753623765560meh",
                            "name_prefix": "ca-elasticsearch-node-",
                            "root_block_device.#": "0",
                            "security_groups.#": "2",
                            "security_groups.144082702": "sg-0db8bc74",
                            "security_groups.929541476": "sg-1ecac279",
                            "spot_price": "",
                            "user_data": "ee61af0c214387b358cdfe085de75c686ebaaffb",
                            "vpc_classic_link_id": "",
                            "vpc_classic_link_security_groups.#": "0"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_launch_configuration.kibana_node": {
                    "type": "aws_launch_configuration",
                    "depends_on": [],
                    "primary": {
                        "id": "ca-kibana-es-node-001448910c164c347e408b24ee",
                        "attributes": {
                            "associate_public_ip_address": "false",
                            "ebs_block_device.#": "0",
                            "ebs_optimized": "false",
                            "enable_monitoring": "true",
                            "ephemeral_block_device.#": "0",
                            "iam_instance_profile": "ebs-attach-and-secrets-profile-cloudalytics",
                            "id": "ca-kibana-es-node-001448910c164c347e408b24ee",
                            "image_id": "ami-d2c924b2",
                            "instance_type": "t2.small",
                            "key_name": "data-pipeline",
                            "name": "ca-kibana-es-node-001448910c164c347e408b24ee",
                            "name_prefix": "ca-kibana-es-node-",
                            "root_block_device.#": "1",
                            "root_block_device.0.delete_on_termination": "false",
                            "root_block_device.0.iops": "0",
                            "root_block_device.0.volume_size": "50",
                            "root_block_device.0.volume_type": "gp2",
                            "security_groups.#": "2",
                            "security_groups.2111075460": "sg-a8abb6d0",
                            "security_groups.929541476": "sg-1ecac279",
                            "spot_price": "",
                            "user_data": "2281177efe7f4aa0be987dc1b64ade579fd7dee4",
                            "vpc_classic_link_id": "",
                            "vpc_classic_link_security_groups.#": "0"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_launch_configuration.nginx_node": {
                    "type": "aws_launch_configuration",
                    "depends_on": [
                        "aws_security_group.nginx_ports",
                        "data.template_file.user_data"
                    ],
                    "primary": {
                        "id": "ca-nginx-es-node-20170407210753622238823rpu",
                        "attributes": {
                            "associate_public_ip_address": "false",
                            "ebs_block_device.#": "0",
                            "ebs_optimized": "false",
                            "enable_monitoring": "true",
                            "ephemeral_block_device.#": "0",
                            "iam_instance_profile": "ebs-attach-and-secrets-profile-cloudalytics",
                            "id": "ca-nginx-es-node-20170407210753622238823rpu",
                            "image_id": "ami-d2c924b2",
                            "instance_type": "t2.small",
                            "key_name": "data-pipeline",
                            "name": "ca-nginx-es-node-20170407210753622238823rpu",
                            "name_prefix": "ca-nginx-es-node-",
                            "root_block_device.#": "0",
                            "security_groups.#": "2",
                            "security_groups.3124136214": "sg-0cb8bc75",
                            "security_groups.929541476": "sg-1ecac279",
                            "spot_price": "",
                            "user_data": "2281177efe7f4aa0be987dc1b64ade579fd7dee4",
                            "vpc_classic_link_id": "",
                            "vpc_classic_link_security_groups.#": "0"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_s3_bucket.snapshot_bucket": {
                    "type": "aws_s3_bucket",
                    "depends_on": [
                        "data.template_file.s3_policy"
                    ],
                    "primary": {
                        "id": "us-west-2-jive-data-test-cloudalytics-snaps",
                        "attributes": {
                            "acceleration_status": "",
                            "acl": "private",
                            "arn": "arn:aws:s3:::us-west-2-jive-data-test-cloudalytics-snaps",
                            "bucket": "us-west-2-jive-data-test-cloudalytics-snaps",
                            "bucket_domain_name": "us-west-2-jive-data-test-cloudalytics-snaps.s3.amazonaws.com",
                            "force_destroy": "false",
                            "hosted_zone_id": "Z3BJ6K6RIION7M",
                            "id": "us-west-2-jive-data-test-cloudalytics-snaps",
                            "policy": "{\"Statement\":[{\"Action\":[\"s3:ListBucket\",\"s3:GetBucketLocation\",\"s3:ListBucketMultipartUploads\",\"s3:ListBucketVersions\"],\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::999547976641:role/ebs-attach-and-secrets-role-cloudalytics\"},\"Resource\":\"arn:aws:s3:::us-west-2-jive-data-test-cloudalytics-snaps\",\"Sid\":\"statement201609211128\"},{\"Action\":[\"s3:GetObject\",\"s3:PutObject\",\"s3:DeleteObject\",\"s3:AbortMultipartUpload\",\"s3:ListMultipartUploadParts\"],\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::999547976641:role/ebs-attach-and-secrets-role-cloudalytics\"},\"Resource\":\"arn:aws:s3:::us-west-2-jive-data-test-cloudalytics-snaps/*\",\"Sid\":\"statement201609211130\"}],\"Version\":\"2012-10-17\"}",
                            "region": "us-west-2",
                            "request_payer": "BucketOwner",
                            "tags.%": "6",
                            "tags.Name": "test-cloudalytics-elasticsearch-snapshots",
                            "tags.jive_service": "cloudalytics",
                            "tags.jive_subservice": "cloudalytics",
                            "tags.pipeline_phase": "test",
                            "tags.service_component": "elasticsearch_snapshots",
                            "tags.sla": "non-prod",
                            "versioning.#": "1",
                            "versioning.0.enabled": "false",
                            "versioning.0.mfa_delete": "false",
                            "website.#": "0"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group.elasticsearch_ports": {
                    "type": "aws_security_group",
                    "depends_on": [],
                    "primary": {
                        "id": "sg-0db8bc74",
                        "attributes": {
                            "description": "Allow traffic on ye olde elasticsearch ports",
                            "egress.#": "1",
                            "egress.3563209066.cidr_blocks.#": "1",
                            "egress.3563209066.cidr_blocks.0": "0.0.0.0/8",
                            "egress.3563209066.from_port": "0",
                            "egress.3563209066.ipv6_cidr_blocks.#": "0",
                            "egress.3563209066.prefix_list_ids.#": "0",
                            "egress.3563209066.protocol": "-1",
                            "egress.3563209066.security_groups.#": "0",
                            "egress.3563209066.self": "false",
                            "egress.3563209066.to_port": "0",
                            "id": "sg-0db8bc74",
                            "ingress.#": "2",
                            "ingress.2208294858.cidr_blocks.#": "1",
                            "ingress.2208294858.cidr_blocks.0": "10.0.0.0/8",
                            "ingress.2208294858.from_port": "9300",
                            "ingress.2208294858.ipv6_cidr_blocks.#": "0",
                            "ingress.2208294858.protocol": "tcp",
                            "ingress.2208294858.security_groups.#": "0",
                            "ingress.2208294858.self": "false",
                            "ingress.2208294858.to_port": "9300",
                            "ingress.443847884.cidr_blocks.#": "1",
                            "ingress.443847884.cidr_blocks.0": "10.0.0.0/8",
                            "ingress.443847884.from_port": "9200",
                            "ingress.443847884.ipv6_cidr_blocks.#": "0",
                            "ingress.443847884.protocol": "tcp",
                            "ingress.443847884.security_groups.#": "0",
                            "ingress.443847884.self": "false",
                            "ingress.443847884.to_port": "9200",
                            "name": "cloudalytics_elasticsearch_ports",
                            "owner_id": "999547976641",
                            "tags.%": "0",
                            "vpc_id": "vpc-a69238c2"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group.kibana_ports": {
                    "type": "aws_security_group",
                    "depends_on": [],
                    "primary": {
                        "id": "sg-a8abb6d0",
                        "attributes": {
                            "description": "Allow traffic on port 5601",
                            "egress.#": "1",
                            "egress.3563209066.cidr_blocks.#": "1",
                            "egress.3563209066.cidr_blocks.0": "0.0.0.0/8",
                            "egress.3563209066.from_port": "0",
                            "egress.3563209066.ipv6_cidr_blocks.#": "0",
                            "egress.3563209066.prefix_list_ids.#": "0",
                            "egress.3563209066.protocol": "-1",
                            "egress.3563209066.security_groups.#": "0",
                            "egress.3563209066.self": "false",
                            "egress.3563209066.to_port": "0",
                            "id": "sg-a8abb6d0",
                            "ingress.#": "2",
                            "ingress.2216620219.cidr_blocks.#": "1",
                            "ingress.2216620219.cidr_blocks.0": "10.0.0.0/8",
                            "ingress.2216620219.from_port": "443",
                            "ingress.2216620219.ipv6_cidr_blocks.#": "0",
                            "ingress.2216620219.protocol": "tcp",
                            "ingress.2216620219.security_groups.#": "0",
                            "ingress.2216620219.self": "false",
                            "ingress.2216620219.to_port": "443",
                            "ingress.3113294704.cidr_blocks.#": "1",
                            "ingress.3113294704.cidr_blocks.0": "10.0.0.0/8",
                            "ingress.3113294704.from_port": "5601",
                            "ingress.3113294704.ipv6_cidr_blocks.#": "0",
                            "ingress.3113294704.protocol": "tcp",
                            "ingress.3113294704.security_groups.#": "0",
                            "ingress.3113294704.self": "false",
                            "ingress.3113294704.to_port": "5601",
                            "name": "cloudalytics_kibana_ports",
                            "owner_id": "999547976641",
                            "tags.%": "0",
                            "vpc_id": "vpc-a69238c2"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group.nginx_ports": {
                    "type": "aws_security_group",
                    "depends_on": [],
                    "primary": {
                        "id": "sg-0cb8bc75",
                        "attributes": {
                            "description": "Allow traffic on port 443",
                            "egress.#": "1",
                            "egress.3563209066.cidr_blocks.#": "1",
                            "egress.3563209066.cidr_blocks.0": "0.0.0.0/8",
                            "egress.3563209066.from_port": "0",
                            "egress.3563209066.ipv6_cidr_blocks.#": "0",
                            "egress.3563209066.prefix_list_ids.#": "0",
                            "egress.3563209066.protocol": "-1",
                            "egress.3563209066.security_groups.#": "0",
                            "egress.3563209066.self": "false",
                            "egress.3563209066.to_port": "0",
                            "id": "sg-0cb8bc75",
                            "ingress.#": "1",
                            "ingress.2216620219.cidr_blocks.#": "1",
                            "ingress.2216620219.cidr_blocks.0": "10.0.0.0/8",
                            "ingress.2216620219.from_port": "443",
                            "ingress.2216620219.ipv6_cidr_blocks.#": "0",
                            "ingress.2216620219.protocol": "tcp",
                            "ingress.2216620219.security_groups.#": "0",
                            "ingress.2216620219.self": "false",
                            "ingress.2216620219.to_port": "443",
                            "name": "cloudalytics_nginx_ports",
                            "owner_id": "999547976641",
                            "tags.%": "0",
                            "vpc_id": "vpc-a69238c2"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group_rule.elasticsearch_egress": {
                    "type": "aws_security_group_rule",
                    "depends_on": [
                        "aws_security_group.elasticsearch_ports"
                    ],
                    "primary": {
                        "id": "sgrule-3944132201",
                        "attributes": {
                            "cidr_blocks.#": "1",
                            "cidr_blocks.0": "0.0.0.0/8",
                            "from_port": "0",
                            "id": "sgrule-3944132201",
                            "ipv6_cidr_blocks.#": "0",
                            "prefix_list_ids.#": "0",
                            "protocol": "-1",
                            "security_group_id": "sg-0db8bc74",
                            "self": "false",
                            "to_port": "0",
                            "type": "egress"
                        },
                        "meta": {
                            "schema_version": "2"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group_rule.elasticsearch_port_9200": {
                    "type": "aws_security_group_rule",
                    "depends_on": [
                        "aws_security_group.elasticsearch_ports"
                    ],
                    "primary": {
                        "id": "sgrule-1680543045",
                        "attributes": {
                            "cidr_blocks.#": "1",
                            "cidr_blocks.0": "10.0.0.0/8",
                            "from_port": "9200",
                            "id": "sgrule-1680543045",
                            "ipv6_cidr_blocks.#": "0",
                            "prefix_list_ids.#": "0",
                            "protocol": "tcp",
                            "security_group_id": "sg-0db8bc74",
                            "self": "false",
                            "to_port": "9200",
                            "type": "ingress"
                        },
                        "meta": {
                            "schema_version": "2"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group_rule.elasticsearch_port_9300": {
                    "type": "aws_security_group_rule",
                    "depends_on": [
                        "aws_security_group.elasticsearch_ports"
                    ],
                    "primary": {
                        "id": "sgrule-4219278254",
                        "attributes": {
                            "cidr_blocks.#": "1",
                            "cidr_blocks.0": "10.0.0.0/8",
                            "from_port": "9300",
                            "id": "sgrule-4219278254",
                            "ipv6_cidr_blocks.#": "0",
                            "prefix_list_ids.#": "0",
                            "protocol": "tcp",
                            "security_group_id": "sg-0db8bc74",
                            "self": "false",
                            "to_port": "9300",
                            "type": "ingress"
                        },
                        "meta": {
                            "schema_version": "2"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group_rule.kibana_egress": {
                    "type": "aws_security_group_rule",
                    "depends_on": [
                        "aws_security_group.kibana_ports"
                    ],
                    "primary": {
                        "id": "sgrule-120025705",
                        "attributes": {
                            "cidr_blocks.#": "1",
                            "cidr_blocks.0": "0.0.0.0/8",
                            "from_port": "0",
                            "id": "sgrule-120025705",
                            "ipv6_cidr_blocks.#": "0",
                            "prefix_list_ids.#": "0",
                            "protocol": "-1",
                            "security_group_id": "sg-a8abb6d0",
                            "self": "false",
                            "to_port": "0",
                            "type": "egress"
                        },
                        "meta": {
                            "schema_version": "2"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group_rule.kibana_port_443": {
                    "type": "aws_security_group_rule",
                    "depends_on": [
                        "aws_security_group.kibana_ports"
                    ],
                    "primary": {
                        "id": "sgrule-427811911",
                        "attributes": {
                            "cidr_blocks.#": "1",
                            "cidr_blocks.0": "10.0.0.0/8",
                            "from_port": "443",
                            "id": "sgrule-427811911",
                            "ipv6_cidr_blocks.#": "0",
                            "prefix_list_ids.#": "0",
                            "protocol": "tcp",
                            "security_group_id": "sg-a8abb6d0",
                            "self": "false",
                            "to_port": "443",
                            "type": "ingress"
                        },
                        "meta": {
                            "schema_version": "2"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group_rule.kibana_port_5601": {
                    "type": "aws_security_group_rule",
                    "depends_on": [
                        "aws_security_group.kibana_ports"
                    ],
                    "primary": {
                        "id": "sgrule-3436237035",
                        "attributes": {
                            "cidr_blocks.#": "1",
                            "cidr_blocks.0": "10.0.0.0/8",
                            "from_port": "5601",
                            "id": "sgrule-3436237035",
                            "ipv6_cidr_blocks.#": "0",
                            "prefix_list_ids.#": "0",
                            "protocol": "tcp",
                            "security_group_id": "sg-a8abb6d0",
                            "self": "false",
                            "to_port": "5601",
                            "type": "ingress"
                        },
                        "meta": {
                            "schema_version": "2"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group_rule.nginx_egress": {
                    "type": "aws_security_group_rule",
                    "depends_on": [
                        "aws_security_group.nginx_ports"
                    ],
                    "primary": {
                        "id": "sgrule-4164368890",
                        "attributes": {
                            "cidr_blocks.#": "1",
                            "cidr_blocks.0": "0.0.0.0/8",
                            "from_port": "0",
                            "id": "sgrule-4164368890",
                            "ipv6_cidr_blocks.#": "0",
                            "prefix_list_ids.#": "0",
                            "protocol": "-1",
                            "security_group_id": "sg-0cb8bc75",
                            "self": "false",
                            "to_port": "0",
                            "type": "egress"
                        },
                        "meta": {
                            "schema_version": "2"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group_rule.nginx_port_443": {
                    "type": "aws_security_group_rule",
                    "depends_on": [
                        "aws_security_group.nginx_ports"
                    ],
                    "primary": {
                        "id": "sgrule-3042692050",
                        "attributes": {
                            "cidr_blocks.#": "1",
                            "cidr_blocks.0": "10.0.0.0/8",
                            "from_port": "443",
                            "id": "sgrule-3042692050",
                            "ipv6_cidr_blocks.#": "0",
                            "prefix_list_ids.#": "0",
                            "protocol": "tcp",
                            "security_group_id": "sg-0cb8bc75",
                            "self": "false",
                            "to_port": "443",
                            "type": "ingress"
                        },
                        "meta": {
                            "schema_version": "2"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "data.template_file.elasticsearch_user_data": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "d4df5c4531ebc7848f6732457cf3cfab0a0ef35a62cf517e16448a46969c2dca",
                        "attributes": {
                            "id": "d4df5c4531ebc7848f6732457cf3cfab0a0ef35a62cf517e16448a46969c2dca",
                            "rendered": "#!/bin/bash\n\n# SKIP_EBS_REATTACH - Set to a non-empty string to skip reattaching of any\n#                     unattached matching EBS volumes.\n#                     The ebs_attach script will still run and new volumes\n#                     will be attached/formatted as necessary\n# ADDITIONAL_BUNDLE_NAME - The name of a nexus bundle to download and unpack.\n#                          Leave blank to skip.\n#                          Must contain a script for setting up/calling ansible\n#                          located at/called:\n#           ./ansible/bin/call_ansible.sh\n#\nSKIP_EBS_REATTACH=\nADDITIONAL_BUNDLE_NAME=ansible-playbooks-aws\nADDITIONAL_BUNDLE_VERSION=LATEST\nINSTALL_DIRECTORY=/opt/ansible\ndeclare -r instance_type=$(curl -s http://169.254.169.254/latest/meta-data/instance-type)\n\nfunction_prep() {\n    mkdir -p ${INSTALL_DIRECTORY}\n    # Get pip for awscli\n    yum install -y epel-release\n    yum install -y python-pip\n    pip install awscli\n}\n\nfunction_instance_store() {\n  yum install -y cryptsetup\n  passphrase=$(\u003c /dev/urandom tr -dc '_A-Za-z0-9@#%^_\\\\-\\\\=+' | head -c 256 | xargs -0 echo)\n\n  if [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\n  then\n    # this /could/ be a bit more flexible *wink!*\n\n    # set up disk encryption\n    echo \"cryptsetup luksFormat /dev/nvme0n1\"\n    echo $passphrase | cryptsetup luksFormat /dev/nvme0n1\n    UUID=$(cryptsetup luksUUID /dev/nvme0n1)\n    echo \"cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\"\n    echo \"$passphrase\" | cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\n    echo \"mkfs.ext4 /dev/mapper/elasticsearch_data\"\n    mkfs.ext4 /dev/mapper/elasticsearch_data\n    echo \"mount /dev/mapper/elasticsearch_data /data\"\n    mkdir -p /data\n    mount /dev/mapper/elasticsearch_data /data\n\n    # encrypt and save the volume's password\n    echo \"aws --region us-west-2 kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext xxxxxx --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\"\n    aws --region us-west-2 kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext \"${passphrase}\" --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\n    unset passphrase\n\n    cat \u003c\u003c-EOM \u003e /etc/init.d/luks-mount\n#!/bin/bash\n# A quickly hacked together script to remount a luks volume at boot\n\n# Get the passphrase from KMS using the ciphertext\npassphrase=\\$(aws --region us-west-2 kms decrypt --ciphertext-blob fileb:///etc/.luks --output text --query Plaintext | base64 -d)\n\n# Open the LUKS volume\necho \"\\$passphrase\" | cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\n\n# Mount the volume\nmount /dev/mapper/elasticsearch_data /data\nEOM\n\n    chmod 755 /etc/init.d/luks-mount\n\n    # will this work for Centos 7? No it will not.\n    #ln -s /etc/init.d/luks-mount /etc/rc3.d/S15luks\n\n    # so instead, create a systemd file:\n    cat \u003c\u003c-EOM \u003e /usr/lib/systemd/system/data_remount.service\n[Unit]\nDescription=Mount the ephemeral data volume\nDocumentation=\nBefore=elasticsearch.service\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=oneshot\nExecStart=/etc/init.d/luks-mount\n\n[Install]\nWantedBy=multi-user.target\nEOM\n\n    systemctl daemon-reload\n    systemctl enable data_remount\n\n  fi\n}\n\nfunction_ebs_attach() {\n    cat \u003c\u003c'EOF' \u003e ${INSTALL_DIRECTORY}/ebs_mount.sh\n#!/bin/bash -v\n#\n# Usage:\n# ./ebs_mount.sh -d \u003cdevice:mountpoint\u003e[,\u003cdevice:mountpoint\u003e...]\n#\n# Example:\n# ./ebs_mount.sh -d /dev/xvdm:/data/elasticsearch,/dev/xvdn:/data/more_data\n#\ndeclare -r instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)\ndeclare -r avail_zone=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)\n\nregion=\"us-west-2\"\n\nwhile getopts \"d:n:p:\" opt; do\n  case \"$opt\" in\n  d) devices=$OPTARG\n     ;;\n  esac\ndone\n\nif [ -z $name ]\nthen\n  name=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`Name`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\nfi\n\npipeline_phase=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`pipeline_phase`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_service=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_service`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_subservice=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_subservice`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\n\necho \"Pipeline_phase: ${pipeline_phase}\"\necho \"Jive_service: ${jive_service}\"\necho \"Jive_subservice: ${jive_subservice}\"\n\nOLD_IFS=$IFS\nIFS=','\nfor dev_mp_pair in $devices\ndo\n  # I have no idea what I'm doing\n  IFS=':' read -ra PAIR \u003c\u003c\u003c \"$dev_mp_pair\"\n  IFS=','\n  device=${PAIR[0]}\n  mountp=${PAIR[1]}\n  echo \"Device: ${device}\"\n  if [ -z $device ]\n  then\n    echo \"[ERROR] Did you specify a device name?\"\n    continue\n  fi\n\n  echo \"MountP: ${mountp}\"\n  if [ -z $mountp ]\n  then\n    echo \"[ERROR] Did you specify a mount point?\"\n    continue\n  fi\n\n  mkdir -p ${mountp}\n\n  if [ -z ${SKIP_EBS_REATTACH} ]\n  then\n    # Search for existing tagged EBS volume (in current AZ)\n    echo \"aws ec2 describe-volumes --region=${region} --filters Name=availability-zone,Values=${avail_zone} Name=tag:pipeline_phase,Values=${pipeline_phase} Name=tag:jive_service,Values=${jive_service} Name=tag:jive_subservice,Values=${jive_subservice} Name=status,Values=available Name=tag:device,Values=${device} Name=tag:Name,Values=${name} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n    previous_volume=$(aws ec2 describe-volumes --region=${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=available Name=tag:device,Values=${device} Name=tag:Name,Values=${name} Name=tag:pipeline_phase,Values=${pipeline_phase} Name=tag:jive_service,Values=${jive_service} Name=tag:jive_subservice,Values=${jive_subservice} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n    echo \"Previous volume: ${previous_volume}\"\n  else\n    previous_volume=\"\"\n    echo \"SKIP_EBS_REATTACH is set, not attempting to reattach old volume(s)\"\n  fi\n\n  # find current volume id\n  echo \"aws ec2 describe-volumes --region ${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=${instance_id} Name=attachment.device,Values=${device} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n  current_volume=$(aws ec2 describe-volumes --region ${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=${instance_id} Name=attachment.device,Values=${device} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n\n  if [ $? -ne 0 ]\n  then\n    echo \"[ERROR] Failed to get current volume ID for ${device}\"\n    continue\n  fi\n\n  echo \"Current volume: ${current_volume}\"\n\n  if [ ! -z $previous_volume ]\n  then\n\n    # detach current EBS\n    echo \"detaching current volume: ${current_volume}\"\n    aws ec2 detach-volume --region ${region} --volume-id ${current_volume}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to detach current volume: ${current_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to detach\n    #echo \"sleeping for 120 to allow aws time to get its ducks in a row\"\n    #sleep 120\n    device_name=$(basename ${device})\n    echo \"looking for detachment of $device_name\"\n\n    while true\n    do\n      lsblk|grep $device_name 1\u003e/dev/null\n      RES=$?\n      if [[ $RES != \"0\" ]]\n      then\n        echo \"$device_name gone. proceeding...\"\n        sleep 10\n        break\n      fi\n      echo \"$device_name still attached. waiting...\"\n      sleep 5\n    done\n\n    # attach existing EBS\n    aws ec2 attach-volume --region ${region} --volume-id ${previous_volume} --instance-id ${instance_id} --device ${device}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to attach previous volume: ${previous_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to attach\n    #echo \"sleeping for 120 to allow aws time to get its ducks in a row again\"\n    #sleep 120\n    echo \"looking for attachment of $device_name\"\n\n    while true\n    do\n      lsblk|grep $device_name 1\u003e/dev/null\n      RES=$?\n      if [[ $RES == \"0\" ]]\n      then\n        echo \"$device_name found. proceeding...\"\n        sleep 10\n        break\n      fi\n      echo \"$device_name not attached. waiting...\"\n      sleep 5\n    done\n\n    current_volume=${previous_volume}\n\n  else\n    # no previous volume found. assume tabula rasa\n    echo \"No previous volume found. Proceeding...\"\n    echo \"mkfs -t ext4 ${device}\"\n    mkfs -t ext4 ${device}\n  fi\n  \n  echo \"mount ${device} ${mountp}\"\n  mount ${device} ${mountp}\n  echo \"${device} ${mountp} ext4 defaults,nofail 0 2\" \u003e\u003e /etc/fstab\n\n  # add tags to the volume?\n  echo \"aws ec2 create-tags --region ${region} --resources ${current_volume} --tags Key=Name,Value=\\\"${name}\\\" Key=device,Value=${device} Key=pipeline_phase,Value=${pipeline_phase} Key=jive_service,Value=${jive_service} Key=jive_subservice,Value=${jive_subservice}\"\n  aws ec2 create-tags --region ${region} --resources ${current_volume} --tags Key=Name,Value=\"${name}\" Key=device,Value=${device} Key=pipeline_phase,Value=${pipeline_phase} Key=jive_service,Value=${jive_service} Key=jive_subservice,Value=${jive_subservice}\n\ndone\nIFS=$OLD_IFS\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x ${INSTALL_DIRECTORY}/ebs_mount.sh\n    ${INSTALL_DIRECTORY}/ebs_mount.sh -d /dev/xvdm:/data 2\u003e\u00261 \u003e\u003e ${INSTALL_DIRECTORY}/ebs_mount.log\n}\n\nfunction_nexus() {\n    # URL redirect fails without this entry\n    echo \"10.10.100.155 nexus-int.eng.jiveland.com\" \u003e\u003e /etc/hosts\n\n    # Script to download Ansible artifact from Nexus\n    cat \u003c\u003c'EOF' \u003e ${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n#!/bin/bash\n# Argument = -h -v -i groupId:artifactId:version -c classifier -p packaging -r repository\n\n# Define Nexus Configuration\nNEXUS_BASE=nexus-int.eng.jiveland.com\nREST_PATH=/service/local\nART_REDIR=/artifact/maven/redirect\n\n# Read in Complete Set of Coordinates from the Command Line\nGROUP_ID=\nARTIFACT_ID=\nVERSION=\"LATEST\"\nCLASSIFIER=\"\"\nPACKAGING=tar.gz\nREPO=\"candidates\"\nVERBOSE=0\n\nwhile getopts \"hvi:c:p:\" OPTION\ndo\n     case $OPTION in\n         h)\n             usage\n             exit 1\n             ;;\n         i)\n\t     OIFS=$IFS\n             IFS=\":\"\n\t     GAV_COORD=( $OPTARG )\n\t     GROUP_ID=${GAV_COORD[0]}\n             ARTIFACT_ID=${GAV_COORD[1]}\n             VERSION=${GAV_COORD[2]}\n\t     IFS=$OIFS\n             ;;\n         c)\n             CLASSIFIER=$OPTARG\n             ;;\n         p)\n             PACKAGING=$OPTARG\n             ;;\n         v)\n             VERBOSE=1\n             ;;\n         ?)\n             usage\n             exit\n             ;;\n     esac\ndone\n\nif [[ -z $GROUP_ID ]] || [[ -z $ARTIFACT_ID ]] || [[ -z $VERSION ]]\nthen\n     echo \"BAD ARGUMENTS: Either groupId, artifactId, or version was not supplied\" \u003e\u00262\n     usage\n     exit 1\nfi\n\n# Construct the base URL\nREDIRECT_URL=${NEXUS_BASE}${REST_PATH}${ART_REDIR}\n\n# Generate the list of parameters\nPARAM_KEYS=( g a v r p c )\nPARAM_VALUES=( $GROUP_ID $ARTIFACT_ID $VERSION $REPO $PACKAGING $CLASSIFIER )\nPARAMS=\"\"\nfor index in ${!PARAM_KEYS[*]}\ndo\n  if [[ ${PARAM_VALUES[$index]} != \"\" ]]\n  then\n    PARAMS=\"${PARAMS}${PARAM_KEYS[$index]}=${PARAM_VALUES[$index]}\u0026\"\n  fi\ndone\n\nREDIRECT_URL=\"${REDIRECT_URL}?${PARAMS}\"\n\necho \"Fetching Artifact from $REDIRECT_URL...\" \u003e\u00262\ncurl -sS -L ${REDIRECT_URL}\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x ${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n    ${INSTALL_DIRECTORY}/get_nexus_artifact.sh -i com.jivesoftware.techops:ansible-common:LATEST \u003e ${INSTALL_DIRECTORY}/ansible-common.tar.gz\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      aws configure set s3.signature_version s3v4\n      aws s3 cp s3://us-west-2-jive-data-pipeline-playbooks/${ADDITIONAL_BUNDLE_NAME}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz\n    fi\n}\n\nfunction_ansible() {\n    # Need Sudo TTY\n    sed -i s/'Defaults    requiretty'/'#Defaults    requiretty'/ /etc/sudoers\n    # Disable SELINUX for SSSD\n    sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config\n    setenforce 0\n    # Create Ansible working directories\n    mkdir -p ${INSTALL_DIRECTORY}/ansible-common\n    tar xf ${INSTALL_DIRECTORY}/ansible-common.tar.gz -C ${INSTALL_DIRECTORY}/ansible-common/\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      mkdir -p ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}\n      tar xf ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz -C ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}\n    fi\n    # Set Python to 2.6 and run Ansible locally\n    alternatives --set python /usr/bin/python2.6\n    yum install -y yum-python26 python-boto ansible\n\n    # Script to run Ansible locally\n    cat \u003c\u003cEOF \u003e ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n#!/bin/bash\n\nansible-playbook -i localhost ${INSTALL_DIRECTORY}/ansible-common/playbook-generic-node.yml --connection=local\nif [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\nthen\n  ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}/bin/call_ansible.sh\nfi\nEOF\n    # Run Ansible\n    chmod +x ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n    ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh \u003e\u003e ${INSTALL_DIRECTORY}/ansible-common/ansible_debug.log\n}\n\nfunction_restart() {\n    # Need to restart for SELINUX change.\n    shutdown -r now\n}\n\n# Run the things\nfunction_prep\nif [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\nthen\n  function_instance_store\nelse\n  function_ebs_attach\nfi\nfunction_nexus\nfunction_ansible\n#function_restart\n",
                            "template": "#!/bin/bash\n\n# SKIP_EBS_REATTACH - Set to a non-empty string to skip reattaching of any\n#                     unattached matching EBS volumes.\n#                     The ebs_attach script will still run and new volumes\n#                     will be attached/formatted as necessary\n# ADDITIONAL_BUNDLE_NAME - The name of a nexus bundle to download and unpack.\n#                          Leave blank to skip.\n#                          Must contain a script for setting up/calling ansible\n#                          located at/called:\n#           ./ansible/bin/call_ansible.sh\n#\nSKIP_EBS_REATTACH=${skip_ebs_reattach}\nADDITIONAL_BUNDLE_NAME=${bundle_name}\nADDITIONAL_BUNDLE_VERSION=${bundle_version}\nINSTALL_DIRECTORY=/opt/ansible\ndeclare -r instance_type=$(curl -s http://169.254.169.254/latest/meta-data/instance-type)\n\nfunction_prep() {\n    mkdir -p $${INSTALL_DIRECTORY}\n    # Get pip for awscli\n    yum install -y epel-release\n    yum install -y python-pip\n    pip install awscli\n}\n\nfunction_instance_store() {\n  yum install -y cryptsetup\n  passphrase=$(\u003c /dev/urandom tr -dc '_A-Za-z0-9@#%^_\\\\-\\\\=+' | head -c 256 | xargs -0 echo)\n\n  if [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\n  then\n    # this /could/ be a bit more flexible *wink!*\n\n    # set up disk encryption\n    echo \"cryptsetup luksFormat /dev/nvme0n1\"\n    echo $passphrase | cryptsetup luksFormat /dev/nvme0n1\n    UUID=$(cryptsetup luksUUID /dev/nvme0n1)\n    echo \"cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\"\n    echo \"$passphrase\" | cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\n    echo \"mkfs.ext4 /dev/mapper/elasticsearch_data\"\n    mkfs.ext4 /dev/mapper/elasticsearch_data\n    echo \"mount /dev/mapper/elasticsearch_data /data\"\n    mkdir -p /data\n    mount /dev/mapper/elasticsearch_data /data\n\n    # encrypt and save the volume's password\n    echo \"aws --region ${region} kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext xxxxxx --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\"\n    aws --region ${region} kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext \"$${passphrase}\" --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\n    unset passphrase\n\n    cat \u003c\u003c-EOM \u003e /etc/init.d/luks-mount\n#!/bin/bash\n# A quickly hacked together script to remount a luks volume at boot\n\n# Get the passphrase from KMS using the ciphertext\npassphrase=\\$(aws --region ${region} kms decrypt --ciphertext-blob fileb:///etc/.luks --output text --query Plaintext | base64 -d)\n\n# Open the LUKS volume\necho \"\\$passphrase\" | cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\n\n# Mount the volume\nmount /dev/mapper/elasticsearch_data /data\nEOM\n\n    chmod 755 /etc/init.d/luks-mount\n\n    # will this work for Centos 7? No it will not.\n    #ln -s /etc/init.d/luks-mount /etc/rc3.d/S15luks\n\n    # so instead, create a systemd file:\n    cat \u003c\u003c-EOM \u003e /usr/lib/systemd/system/data_remount.service\n[Unit]\nDescription=Mount the ephemeral data volume\nDocumentation=\nBefore=elasticsearch.service\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=oneshot\nExecStart=/etc/init.d/luks-mount\n\n[Install]\nWantedBy=multi-user.target\nEOM\n\n    systemctl daemon-reload\n    systemctl enable data_remount\n\n  fi\n}\n\nfunction_ebs_attach() {\n    cat \u003c\u003c'EOF' \u003e $${INSTALL_DIRECTORY}/ebs_mount.sh\n#!/bin/bash -v\n#\n# Usage:\n# ./ebs_mount.sh -d \u003cdevice:mountpoint\u003e[,\u003cdevice:mountpoint\u003e...]\n#\n# Example:\n# ./ebs_mount.sh -d /dev/xvdm:/data/elasticsearch,/dev/xvdn:/data/more_data\n#\ndeclare -r instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)\ndeclare -r avail_zone=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)\n\nregion=\"${region}\"\n\nwhile getopts \"d:n:p:\" opt; do\n  case \"$opt\" in\n  d) devices=$OPTARG\n     ;;\n  esac\ndone\n\nif [ -z $name ]\nthen\n  name=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`Name`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\nfi\n\npipeline_phase=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`pipeline_phase`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_service=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_service`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_subservice=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_subservice`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\n\necho \"Pipeline_phase: $${pipeline_phase}\"\necho \"Jive_service: $${jive_service}\"\necho \"Jive_subservice: $${jive_subservice}\"\n\nOLD_IFS=$IFS\nIFS=','\nfor dev_mp_pair in $devices\ndo\n  # I have no idea what I'm doing\n  IFS=':' read -ra PAIR \u003c\u003c\u003c \"$dev_mp_pair\"\n  IFS=','\n  device=$${PAIR[0]}\n  mountp=$${PAIR[1]}\n  echo \"Device: $${device}\"\n  if [ -z $device ]\n  then\n    echo \"[ERROR] Did you specify a device name?\"\n    continue\n  fi\n\n  echo \"MountP: $${mountp}\"\n  if [ -z $mountp ]\n  then\n    echo \"[ERROR] Did you specify a mount point?\"\n    continue\n  fi\n\n  mkdir -p $${mountp}\n\n  if [ -z $${SKIP_EBS_REATTACH} ]\n  then\n    # Search for existing tagged EBS volume (in current AZ)\n    echo \"aws ec2 describe-volumes --region=$${region} --filters Name=availability-zone,Values=$${avail_zone} Name=tag:pipeline_phase,Values=$${pipeline_phase} Name=tag:jive_service,Values=$${jive_service} Name=tag:jive_subservice,Values=$${jive_subservice} Name=status,Values=available Name=tag:device,Values=$${device} Name=tag:Name,Values=$${name} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n    previous_volume=$(aws ec2 describe-volumes --region=$${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=available Name=tag:device,Values=$${device} Name=tag:Name,Values=$${name} Name=tag:pipeline_phase,Values=$${pipeline_phase} Name=tag:jive_service,Values=$${jive_service} Name=tag:jive_subservice,Values=$${jive_subservice} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n    echo \"Previous volume: $${previous_volume}\"\n  else\n    previous_volume=\"\"\n    echo \"SKIP_EBS_REATTACH is set, not attempting to reattach old volume(s)\"\n  fi\n\n  # find current volume id\n  echo \"aws ec2 describe-volumes --region $${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=$${instance_id} Name=attachment.device,Values=$${device} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n  current_volume=$(aws ec2 describe-volumes --region $${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=$${instance_id} Name=attachment.device,Values=$${device} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n\n  if [ $? -ne 0 ]\n  then\n    echo \"[ERROR] Failed to get current volume ID for $${device}\"\n    continue\n  fi\n\n  echo \"Current volume: $${current_volume}\"\n\n  if [ ! -z $previous_volume ]\n  then\n\n    # detach current EBS\n    echo \"detaching current volume: $${current_volume}\"\n    aws ec2 detach-volume --region $${region} --volume-id $${current_volume}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to detach current volume: $${current_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to detach\n    #echo \"sleeping for 120 to allow aws time to get its ducks in a row\"\n    #sleep 120\n    device_name=$(basename $${device})\n    echo \"looking for detachment of $device_name\"\n\n    while true\n    do\n      lsblk|grep $device_name 1\u003e/dev/null\n      RES=$?\n      if [[ $RES != \"0\" ]]\n      then\n        echo \"$device_name gone. proceeding...\"\n        sleep 10\n        break\n      fi\n      echo \"$device_name still attached. waiting...\"\n      sleep 5\n    done\n\n    # attach existing EBS\n    aws ec2 attach-volume --region $${region} --volume-id $${previous_volume} --instance-id $${instance_id} --device $${device}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to attach previous volume: $${previous_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to attach\n    #echo \"sleeping for 120 to allow aws time to get its ducks in a row again\"\n    #sleep 120\n    echo \"looking for attachment of $device_name\"\n\n    while true\n    do\n      lsblk|grep $device_name 1\u003e/dev/null\n      RES=$?\n      if [[ $RES == \"0\" ]]\n      then\n        echo \"$device_name found. proceeding...\"\n        sleep 10\n        break\n      fi\n      echo \"$device_name not attached. waiting...\"\n      sleep 5\n    done\n\n    current_volume=$${previous_volume}\n\n  else\n    # no previous volume found. assume tabula rasa\n    echo \"No previous volume found. Proceeding...\"\n    echo \"mkfs -t ext4 $${device}\"\n    mkfs -t ext4 $${device}\n  fi\n  \n  echo \"mount $${device} $${mountp}\"\n  mount $${device} $${mountp}\n  echo \"$${device} $${mountp} ext4 defaults,nofail 0 2\" \u003e\u003e /etc/fstab\n\n  # add tags to the volume?\n  echo \"aws ec2 create-tags --region $${region} --resources $${current_volume} --tags Key=Name,Value=\\\"$${name}\\\" Key=device,Value=$${device} Key=pipeline_phase,Value=$${pipeline_phase} Key=jive_service,Value=$${jive_service} Key=jive_subservice,Value=$${jive_subservice}\"\n  aws ec2 create-tags --region $${region} --resources $${current_volume} --tags Key=Name,Value=\"$${name}\" Key=device,Value=$${device} Key=pipeline_phase,Value=$${pipeline_phase} Key=jive_service,Value=$${jive_service} Key=jive_subservice,Value=$${jive_subservice}\n\ndone\nIFS=$OLD_IFS\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x $${INSTALL_DIRECTORY}/ebs_mount.sh\n    $${INSTALL_DIRECTORY}/ebs_mount.sh -d ${devices} 2\u003e\u00261 \u003e\u003e $${INSTALL_DIRECTORY}/ebs_mount.log\n}\n\nfunction_nexus() {\n    # URL redirect fails without this entry\n    echo \"10.10.100.155 nexus-int.eng.jiveland.com\" \u003e\u003e /etc/hosts\n\n    # Script to download Ansible artifact from Nexus\n    cat \u003c\u003c'EOF' \u003e $${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n#!/bin/bash\n# Argument = -h -v -i groupId:artifactId:version -c classifier -p packaging -r repository\n\n# Define Nexus Configuration\nNEXUS_BASE=nexus-int.eng.jiveland.com\nREST_PATH=/service/local\nART_REDIR=/artifact/maven/redirect\n\n# Read in Complete Set of Coordinates from the Command Line\nGROUP_ID=\nARTIFACT_ID=\nVERSION=\"LATEST\"\nCLASSIFIER=\"\"\nPACKAGING=tar.gz\nREPO=\"candidates\"\nVERBOSE=0\n\nwhile getopts \"hvi:c:p:\" OPTION\ndo\n     case $OPTION in\n         h)\n             usage\n             exit 1\n             ;;\n         i)\n\t     OIFS=$IFS\n             IFS=\":\"\n\t     GAV_COORD=( $OPTARG )\n\t     GROUP_ID=$${GAV_COORD[0]}\n             ARTIFACT_ID=$${GAV_COORD[1]}\n             VERSION=$${GAV_COORD[2]}\n\t     IFS=$OIFS\n             ;;\n         c)\n             CLASSIFIER=$OPTARG\n             ;;\n         p)\n             PACKAGING=$OPTARG\n             ;;\n         v)\n             VERBOSE=1\n             ;;\n         ?)\n             usage\n             exit\n             ;;\n     esac\ndone\n\nif [[ -z $GROUP_ID ]] || [[ -z $ARTIFACT_ID ]] || [[ -z $VERSION ]]\nthen\n     echo \"BAD ARGUMENTS: Either groupId, artifactId, or version was not supplied\" \u003e\u00262\n     usage\n     exit 1\nfi\n\n# Construct the base URL\nREDIRECT_URL=$${NEXUS_BASE}$${REST_PATH}$${ART_REDIR}\n\n# Generate the list of parameters\nPARAM_KEYS=( g a v r p c )\nPARAM_VALUES=( $GROUP_ID $ARTIFACT_ID $VERSION $REPO $PACKAGING $CLASSIFIER )\nPARAMS=\"\"\nfor index in $${!PARAM_KEYS[*]}\ndo\n  if [[ $${PARAM_VALUES[$index]} != \"\" ]]\n  then\n    PARAMS=\"$${PARAMS}$${PARAM_KEYS[$index]}=$${PARAM_VALUES[$index]}\u0026\"\n  fi\ndone\n\nREDIRECT_URL=\"$${REDIRECT_URL}?$${PARAMS}\"\n\necho \"Fetching Artifact from $REDIRECT_URL...\" \u003e\u00262\ncurl -sS -L $${REDIRECT_URL}\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x $${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n    $${INSTALL_DIRECTORY}/get_nexus_artifact.sh -i com.jivesoftware.techops:ansible-common:LATEST \u003e $${INSTALL_DIRECTORY}/ansible-common.tar.gz\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      aws configure set s3.signature_version s3v4\n      aws s3 cp s3://us-west-2-jive-data-pipeline-playbooks/$${ADDITIONAL_BUNDLE_NAME}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz\n    fi\n}\n\nfunction_ansible() {\n    # Need Sudo TTY\n    sed -i s/'Defaults    requiretty'/'#Defaults    requiretty'/ /etc/sudoers\n    # Disable SELINUX for SSSD\n    sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config\n    setenforce 0\n    # Create Ansible working directories\n    mkdir -p $${INSTALL_DIRECTORY}/ansible-common\n    tar xf $${INSTALL_DIRECTORY}/ansible-common.tar.gz -C $${INSTALL_DIRECTORY}/ansible-common/\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      mkdir -p $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}\n      tar xf $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz -C $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}\n    fi\n    # Set Python to 2.6 and run Ansible locally\n    alternatives --set python /usr/bin/python2.6\n    yum install -y yum-python26 python-boto ansible\n\n    # Script to run Ansible locally\n    cat \u003c\u003cEOF \u003e $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n#!/bin/bash\n\nansible-playbook -i localhost $${INSTALL_DIRECTORY}/ansible-common/playbook-generic-node.yml --connection=local\nif [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\nthen\n  $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}/bin/call_ansible.sh\nfi\nEOF\n    # Run Ansible\n    chmod +x $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n    $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh \u003e\u003e $${INSTALL_DIRECTORY}/ansible-common/ansible_debug.log\n}\n\nfunction_restart() {\n    # Need to restart for SELINUX change.\n    shutdown -r now\n}\n\n# Run the things\nfunction_prep\nif [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\nthen\n  function_instance_store\nelse\n  function_ebs_attach\nfi\nfunction_nexus\nfunction_ansible\n#function_restart\n",
                            "vars.%": "7",
                            "vars.account_name": "jive-data-pipeline",
                            "vars.bundle_name": "ansible-playbooks-aws",
                            "vars.bundle_version": "LATEST",
                            "vars.devices": "/dev/xvdm:/data",
                            "vars.pipeline_phase": "test",
                            "vars.region": "us-west-2",
                            "vars.skip_ebs_reattach": ""
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "data.template_file.kibana_user_data": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "5e25017229c727473ebab03db0cabbbc6292a7adc58db1c32028a24455c4da1d",
                        "attributes": {
                            "id": "5e25017229c727473ebab03db0cabbbc6292a7adc58db1c32028a24455c4da1d",
                            "rendered": "#!/bin/bash\n\n# SKIP_EBS_REATTACH - Set to a non-empty string to skip reattaching of any\n#                     unattached matching EBS volumes.\n#                     The ebs_attach script will still run and new volumes\n#                     will be attached/formatted as necessary\n# ADDITIONAL_BUNDLE_NAME - The name of a nexus bundle to download and unpack.\n#                          Leave blank to skip.\n#                          Must contain a script for setting up/calling ansible\n#                          located at/called:\n#           ./ansible/bin/call_ansible.sh\n#\nSKIP_EBS_REATTACH=true\nADDITIONAL_BUNDLE_NAME=ansible-playbooks-aws\nADDITIONAL_BUNDLE_VERSION=LATEST\nINSTALL_DIRECTORY=/opt/ansible\ndeclare -r instance_type=$(curl -s http://169.254.169.254/latest/meta-data/instance-type)\n\nfunction_prep() {\n    mkdir -p ${INSTALL_DIRECTORY}\n    # Get pip for awscli\n    yum install -y epel-release\n    yum install -y python-pip\n    pip install awscli\n}\n\nfunction_instance_store() {\n  yum install -y cryptsetup\n  passphrase=$(\u003c /dev/urandom tr -dc '_A-Za-z0-9@#%^_\\\\-\\\\=+' | head -c 256 | xargs -0 echo)\n\n  if [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\n  then\n    # this /could/ be a bit more flexible *wink!*\n\n    # set up disk encryption\n    echo \"cryptsetup luksFormat /dev/nvme0n1\"\n    echo $passphrase | cryptsetup luksFormat /dev/nvme0n1\n    UUID=$(cryptsetup luksUUID /dev/nvme0n1)\n    echo \"cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\"\n    echo \"$passphrase\" | cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\n    echo \"mkfs.ext4 /dev/mapper/elasticsearch_data\"\n    mkfs.ext4 /dev/mapper/elasticsearch_data\n    echo \"mount /dev/mapper/elasticsearch_data /data\"\n    mkdir -p /data\n    mount /dev/mapper/elasticsearch_data /data\n\n    # encrypt and save the volume's password\n    echo \"aws --region us-west-2 kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext xxxxxx --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\"\n    aws --region us-west-2 kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext \"${passphrase}\" --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\n    unset passphrase\n\n    cat \u003c\u003c-EOM \u003e /etc/init.d/luks-mount\n#!/bin/bash\n# A quickly hacked together script to remount a luks volume at boot\n\n# Get the passphrase from KMS using the ciphertext\npassphrase=\\$(aws --region us-west-2 kms decrypt --ciphertext-blob fileb:///etc/.luks --output text --query Plaintext | base64 -d)\n\n# Open the LUKS volume\necho \"\\$passphrase\" | cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\n\n# Mount the volume\nmount /dev/mapper/elasticsearch_data /data\nEOM\n\n    chmod 755 /etc/init.d/luks-mount\n\n    # will this work for Centos 7? No it will not.\n    #ln -s /etc/init.d/luks-mount /etc/rc3.d/S15luks\n\n    # so instead, create a systemd file:\n    cat \u003c\u003c-EOM \u003e /usr/lib/systemd/system/data_remount.service\n[Unit]\nDescription=Mount the ephemeral data volume\nDocumentation=\nBefore=elasticsearch.service\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=oneshot\nExecStart=/etc/init.d/luks-mount\n\n[Install]\nWantedBy=multi-user.target\nEOM\n\n    systemctl daemon-reload\n    systemctl enable data_remount\n\n  fi\n}\n\nfunction_ebs_attach() {\n    cat \u003c\u003c'EOF' \u003e ${INSTALL_DIRECTORY}/ebs_mount.sh\n#!/bin/bash -v\n#\n# Usage:\n# ./ebs_mount.sh -d \u003cdevice:mountpoint\u003e[,\u003cdevice:mountpoint\u003e...]\n#\n# Example:\n# ./ebs_mount.sh -d /dev/xvdm:/data/elasticsearch,/dev/xvdn:/data/more_data\n#\ndeclare -r instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)\ndeclare -r avail_zone=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)\n\nregion=\"us-west-2\"\n\nwhile getopts \"d:n:p:\" opt; do\n  case \"$opt\" in\n  d) devices=$OPTARG\n     ;;\n  esac\ndone\n\nif [ -z $name ]\nthen\n  name=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`Name`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\nfi\n\npipeline_phase=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`pipeline_phase`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_service=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_service`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_subservice=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_subservice`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\n\necho \"Pipeline_phase: ${pipeline_phase}\"\necho \"Jive_service: ${jive_service}\"\necho \"Jive_subservice: ${jive_subservice}\"\n\nOLD_IFS=$IFS\nIFS=','\nfor dev_mp_pair in $devices\ndo\n  # I have no idea what I'm doing\n  IFS=':' read -ra PAIR \u003c\u003c\u003c \"$dev_mp_pair\"\n  IFS=','\n  device=${PAIR[0]}\n  mountp=${PAIR[1]}\n  echo \"Device: ${device}\"\n  if [ -z $device ]\n  then\n    echo \"[ERROR] Did you specify a device name?\"\n    continue\n  fi\n\n  echo \"MountP: ${mountp}\"\n  if [ -z $mountp ]\n  then\n    echo \"[ERROR] Did you specify a mount point?\"\n    continue\n  fi\n\n  mkdir -p ${mountp}\n\n  if [ -z ${SKIP_EBS_REATTACH} ]\n  then\n    # Search for existing tagged EBS volume (in current AZ)\n    echo \"aws ec2 describe-volumes --region=${region} --filters Name=availability-zone,Values=${avail_zone} Name=tag:pipeline_phase,Values=${pipeline_phase} Name=tag:jive_service,Values=${jive_service} Name=tag:jive_subservice,Values=${jive_subservice} Name=status,Values=available Name=tag:device,Values=${device} Name=tag:Name,Values=${name} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n    previous_volume=$(aws ec2 describe-volumes --region=${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=available Name=tag:device,Values=${device} Name=tag:Name,Values=${name} Name=tag:pipeline_phase,Values=${pipeline_phase} Name=tag:jive_service,Values=${jive_service} Name=tag:jive_subservice,Values=${jive_subservice} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n    echo \"Previous volume: ${previous_volume}\"\n  else\n    previous_volume=\"\"\n    echo \"SKIP_EBS_REATTACH is set, not attempting to reattach old volume(s)\"\n  fi\n\n  # find current volume id\n  echo \"aws ec2 describe-volumes --region ${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=${instance_id} Name=attachment.device,Values=${device} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n  current_volume=$(aws ec2 describe-volumes --region ${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=${instance_id} Name=attachment.device,Values=${device} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n\n  if [ $? -ne 0 ]\n  then\n    echo \"[ERROR] Failed to get current volume ID for ${device}\"\n    continue\n  fi\n\n  echo \"Current volume: ${current_volume}\"\n\n  if [ ! -z $previous_volume ]\n  then\n\n    # detach current EBS\n    echo \"detaching current volume: ${current_volume}\"\n    aws ec2 detach-volume --region ${region} --volume-id ${current_volume}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to detach current volume: ${current_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to detach\n    #echo \"sleeping for 120 to allow aws time to get its ducks in a row\"\n    #sleep 120\n    device_name=$(basename ${device})\n    echo \"looking for detachment of $device_name\"\n\n    while true\n    do\n      lsblk|grep $device_name 1\u003e/dev/null\n      RES=$?\n      if [[ $RES != \"0\" ]]\n      then\n        echo \"$device_name gone. proceeding...\"\n        sleep 10\n        break\n      fi\n      echo \"$device_name still attached. waiting...\"\n      sleep 5\n    done\n\n    # attach existing EBS\n    aws ec2 attach-volume --region ${region} --volume-id ${previous_volume} --instance-id ${instance_id} --device ${device}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to attach previous volume: ${previous_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to attach\n    #echo \"sleeping for 120 to allow aws time to get its ducks in a row again\"\n    #sleep 120\n    echo \"looking for attachment of $device_name\"\n\n    while true\n    do\n      lsblk|grep $device_name 1\u003e/dev/null\n      RES=$?\n      if [[ $RES == \"0\" ]]\n      then\n        echo \"$device_name found. proceeding...\"\n        sleep 10\n        break\n      fi\n      echo \"$device_name not attached. waiting...\"\n      sleep 5\n    done\n\n    current_volume=${previous_volume}\n\n  else\n    # no previous volume found. assume tabula rasa\n    echo \"No previous volume found. Proceeding...\"\n    echo \"mkfs -t ext4 ${device}\"\n    mkfs -t ext4 ${device}\n  fi\n  \n  echo \"mount ${device} ${mountp}\"\n  mount ${device} ${mountp}\n  echo \"${device} ${mountp} ext4 defaults,nofail 0 2\" \u003e\u003e /etc/fstab\n\n  # add tags to the volume?\n  echo \"aws ec2 create-tags --region ${region} --resources ${current_volume} --tags Key=Name,Value=\\\"${name}\\\" Key=device,Value=${device} Key=pipeline_phase,Value=${pipeline_phase} Key=jive_service,Value=${jive_service} Key=jive_subservice,Value=${jive_subservice}\"\n  aws ec2 create-tags --region ${region} --resources ${current_volume} --tags Key=Name,Value=\"${name}\" Key=device,Value=${device} Key=pipeline_phase,Value=${pipeline_phase} Key=jive_service,Value=${jive_service} Key=jive_subservice,Value=${jive_subservice}\n\ndone\nIFS=$OLD_IFS\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x ${INSTALL_DIRECTORY}/ebs_mount.sh\n    ${INSTALL_DIRECTORY}/ebs_mount.sh -d /dev/xvdm:/data 2\u003e\u00261 \u003e\u003e ${INSTALL_DIRECTORY}/ebs_mount.log\n}\n\nfunction_nexus() {\n    # URL redirect fails without this entry\n    echo \"10.10.100.155 nexus-int.eng.jiveland.com\" \u003e\u003e /etc/hosts\n\n    # Script to download Ansible artifact from Nexus\n    cat \u003c\u003c'EOF' \u003e ${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n#!/bin/bash\n# Argument = -h -v -i groupId:artifactId:version -c classifier -p packaging -r repository\n\n# Define Nexus Configuration\nNEXUS_BASE=nexus-int.eng.jiveland.com\nREST_PATH=/service/local\nART_REDIR=/artifact/maven/redirect\n\n# Read in Complete Set of Coordinates from the Command Line\nGROUP_ID=\nARTIFACT_ID=\nVERSION=\"LATEST\"\nCLASSIFIER=\"\"\nPACKAGING=tar.gz\nREPO=\"candidates\"\nVERBOSE=0\n\nwhile getopts \"hvi:c:p:\" OPTION\ndo\n     case $OPTION in\n         h)\n             usage\n             exit 1\n             ;;\n         i)\n\t     OIFS=$IFS\n             IFS=\":\"\n\t     GAV_COORD=( $OPTARG )\n\t     GROUP_ID=${GAV_COORD[0]}\n             ARTIFACT_ID=${GAV_COORD[1]}\n             VERSION=${GAV_COORD[2]}\n\t     IFS=$OIFS\n             ;;\n         c)\n             CLASSIFIER=$OPTARG\n             ;;\n         p)\n             PACKAGING=$OPTARG\n             ;;\n         v)\n             VERBOSE=1\n             ;;\n         ?)\n             usage\n             exit\n             ;;\n     esac\ndone\n\nif [[ -z $GROUP_ID ]] || [[ -z $ARTIFACT_ID ]] || [[ -z $VERSION ]]\nthen\n     echo \"BAD ARGUMENTS: Either groupId, artifactId, or version was not supplied\" \u003e\u00262\n     usage\n     exit 1\nfi\n\n# Construct the base URL\nREDIRECT_URL=${NEXUS_BASE}${REST_PATH}${ART_REDIR}\n\n# Generate the list of parameters\nPARAM_KEYS=( g a v r p c )\nPARAM_VALUES=( $GROUP_ID $ARTIFACT_ID $VERSION $REPO $PACKAGING $CLASSIFIER )\nPARAMS=\"\"\nfor index in ${!PARAM_KEYS[*]}\ndo\n  if [[ ${PARAM_VALUES[$index]} != \"\" ]]\n  then\n    PARAMS=\"${PARAMS}${PARAM_KEYS[$index]}=${PARAM_VALUES[$index]}\u0026\"\n  fi\ndone\n\nREDIRECT_URL=\"${REDIRECT_URL}?${PARAMS}\"\n\necho \"Fetching Artifact from $REDIRECT_URL...\" \u003e\u00262\ncurl -sS -L ${REDIRECT_URL}\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x ${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n    ${INSTALL_DIRECTORY}/get_nexus_artifact.sh -i com.jivesoftware.techops:ansible-common:LATEST \u003e ${INSTALL_DIRECTORY}/ansible-common.tar.gz\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      aws configure set s3.signature_version s3v4\n      aws s3 cp s3://us-west-2-jive-data-pipeline-playbooks/${ADDITIONAL_BUNDLE_NAME}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz\n    fi\n}\n\nfunction_ansible() {\n    # Need Sudo TTY\n    sed -i s/'Defaults    requiretty'/'#Defaults    requiretty'/ /etc/sudoers\n    # Disable SELINUX for SSSD\n    sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config\n    setenforce 0\n    # Create Ansible working directories\n    mkdir -p ${INSTALL_DIRECTORY}/ansible-common\n    tar xf ${INSTALL_DIRECTORY}/ansible-common.tar.gz -C ${INSTALL_DIRECTORY}/ansible-common/\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      mkdir -p ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}\n      tar xf ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz -C ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}\n    fi\n    # Set Python to 2.6 and run Ansible locally\n    alternatives --set python /usr/bin/python2.6\n    yum install -y yum-python26 python-boto ansible\n\n    # Script to run Ansible locally\n    cat \u003c\u003cEOF \u003e ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n#!/bin/bash\n\nansible-playbook -i localhost ${INSTALL_DIRECTORY}/ansible-common/playbook-generic-node.yml --connection=local\nif [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\nthen\n  ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}/bin/call_ansible.sh\nfi\nEOF\n    # Run Ansible\n    chmod +x ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n    ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh \u003e\u003e ${INSTALL_DIRECTORY}/ansible-common/ansible_debug.log\n}\n\nfunction_restart() {\n    # Need to restart for SELINUX change.\n    shutdown -r now\n}\n\n# Run the things\nfunction_prep\nif [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\nthen\n  function_instance_store\nelse\n  function_ebs_attach\nfi\nfunction_nexus\nfunction_ansible\n#function_restart\n",
                            "template": "#!/bin/bash\n\n# SKIP_EBS_REATTACH - Set to a non-empty string to skip reattaching of any\n#                     unattached matching EBS volumes.\n#                     The ebs_attach script will still run and new volumes\n#                     will be attached/formatted as necessary\n# ADDITIONAL_BUNDLE_NAME - The name of a nexus bundle to download and unpack.\n#                          Leave blank to skip.\n#                          Must contain a script for setting up/calling ansible\n#                          located at/called:\n#           ./ansible/bin/call_ansible.sh\n#\nSKIP_EBS_REATTACH=${skip_ebs_reattach}\nADDITIONAL_BUNDLE_NAME=${bundle_name}\nADDITIONAL_BUNDLE_VERSION=${bundle_version}\nINSTALL_DIRECTORY=/opt/ansible\ndeclare -r instance_type=$(curl -s http://169.254.169.254/latest/meta-data/instance-type)\n\nfunction_prep() {\n    mkdir -p $${INSTALL_DIRECTORY}\n    # Get pip for awscli\n    yum install -y epel-release\n    yum install -y python-pip\n    pip install awscli\n}\n\nfunction_instance_store() {\n  yum install -y cryptsetup\n  passphrase=$(\u003c /dev/urandom tr -dc '_A-Za-z0-9@#%^_\\\\-\\\\=+' | head -c 256 | xargs -0 echo)\n\n  if [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\n  then\n    # this /could/ be a bit more flexible *wink!*\n\n    # set up disk encryption\n    echo \"cryptsetup luksFormat /dev/nvme0n1\"\n    echo $passphrase | cryptsetup luksFormat /dev/nvme0n1\n    UUID=$(cryptsetup luksUUID /dev/nvme0n1)\n    echo \"cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\"\n    echo \"$passphrase\" | cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\n    echo \"mkfs.ext4 /dev/mapper/elasticsearch_data\"\n    mkfs.ext4 /dev/mapper/elasticsearch_data\n    echo \"mount /dev/mapper/elasticsearch_data /data\"\n    mkdir -p /data\n    mount /dev/mapper/elasticsearch_data /data\n\n    # encrypt and save the volume's password\n    echo \"aws --region ${region} kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext xxxxxx --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\"\n    aws --region ${region} kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext \"$${passphrase}\" --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\n    unset passphrase\n\n    cat \u003c\u003c-EOM \u003e /etc/init.d/luks-mount\n#!/bin/bash\n# A quickly hacked together script to remount a luks volume at boot\n\n# Get the passphrase from KMS using the ciphertext\npassphrase=\\$(aws --region ${region} kms decrypt --ciphertext-blob fileb:///etc/.luks --output text --query Plaintext | base64 -d)\n\n# Open the LUKS volume\necho \"\\$passphrase\" | cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\n\n# Mount the volume\nmount /dev/mapper/elasticsearch_data /data\nEOM\n\n    chmod 755 /etc/init.d/luks-mount\n\n    # will this work for Centos 7? No it will not.\n    #ln -s /etc/init.d/luks-mount /etc/rc3.d/S15luks\n\n    # so instead, create a systemd file:\n    cat \u003c\u003c-EOM \u003e /usr/lib/systemd/system/data_remount.service\n[Unit]\nDescription=Mount the ephemeral data volume\nDocumentation=\nBefore=elasticsearch.service\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=oneshot\nExecStart=/etc/init.d/luks-mount\n\n[Install]\nWantedBy=multi-user.target\nEOM\n\n    systemctl daemon-reload\n    systemctl enable data_remount\n\n  fi\n}\n\nfunction_ebs_attach() {\n    cat \u003c\u003c'EOF' \u003e $${INSTALL_DIRECTORY}/ebs_mount.sh\n#!/bin/bash -v\n#\n# Usage:\n# ./ebs_mount.sh -d \u003cdevice:mountpoint\u003e[,\u003cdevice:mountpoint\u003e...]\n#\n# Example:\n# ./ebs_mount.sh -d /dev/xvdm:/data/elasticsearch,/dev/xvdn:/data/more_data\n#\ndeclare -r instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)\ndeclare -r avail_zone=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)\n\nregion=\"${region}\"\n\nwhile getopts \"d:n:p:\" opt; do\n  case \"$opt\" in\n  d) devices=$OPTARG\n     ;;\n  esac\ndone\n\nif [ -z $name ]\nthen\n  name=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`Name`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\nfi\n\npipeline_phase=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`pipeline_phase`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_service=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_service`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_subservice=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_subservice`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\n\necho \"Pipeline_phase: $${pipeline_phase}\"\necho \"Jive_service: $${jive_service}\"\necho \"Jive_subservice: $${jive_subservice}\"\n\nOLD_IFS=$IFS\nIFS=','\nfor dev_mp_pair in $devices\ndo\n  # I have no idea what I'm doing\n  IFS=':' read -ra PAIR \u003c\u003c\u003c \"$dev_mp_pair\"\n  IFS=','\n  device=$${PAIR[0]}\n  mountp=$${PAIR[1]}\n  echo \"Device: $${device}\"\n  if [ -z $device ]\n  then\n    echo \"[ERROR] Did you specify a device name?\"\n    continue\n  fi\n\n  echo \"MountP: $${mountp}\"\n  if [ -z $mountp ]\n  then\n    echo \"[ERROR] Did you specify a mount point?\"\n    continue\n  fi\n\n  mkdir -p $${mountp}\n\n  if [ -z $${SKIP_EBS_REATTACH} ]\n  then\n    # Search for existing tagged EBS volume (in current AZ)\n    echo \"aws ec2 describe-volumes --region=$${region} --filters Name=availability-zone,Values=$${avail_zone} Name=tag:pipeline_phase,Values=$${pipeline_phase} Name=tag:jive_service,Values=$${jive_service} Name=tag:jive_subservice,Values=$${jive_subservice} Name=status,Values=available Name=tag:device,Values=$${device} Name=tag:Name,Values=$${name} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n    previous_volume=$(aws ec2 describe-volumes --region=$${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=available Name=tag:device,Values=$${device} Name=tag:Name,Values=$${name} Name=tag:pipeline_phase,Values=$${pipeline_phase} Name=tag:jive_service,Values=$${jive_service} Name=tag:jive_subservice,Values=$${jive_subservice} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n    echo \"Previous volume: $${previous_volume}\"\n  else\n    previous_volume=\"\"\n    echo \"SKIP_EBS_REATTACH is set, not attempting to reattach old volume(s)\"\n  fi\n\n  # find current volume id\n  echo \"aws ec2 describe-volumes --region $${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=$${instance_id} Name=attachment.device,Values=$${device} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n  current_volume=$(aws ec2 describe-volumes --region $${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=$${instance_id} Name=attachment.device,Values=$${device} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n\n  if [ $? -ne 0 ]\n  then\n    echo \"[ERROR] Failed to get current volume ID for $${device}\"\n    continue\n  fi\n\n  echo \"Current volume: $${current_volume}\"\n\n  if [ ! -z $previous_volume ]\n  then\n\n    # detach current EBS\n    echo \"detaching current volume: $${current_volume}\"\n    aws ec2 detach-volume --region $${region} --volume-id $${current_volume}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to detach current volume: $${current_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to detach\n    #echo \"sleeping for 120 to allow aws time to get its ducks in a row\"\n    #sleep 120\n    device_name=$(basename $${device})\n    echo \"looking for detachment of $device_name\"\n\n    while true\n    do\n      lsblk|grep $device_name 1\u003e/dev/null\n      RES=$?\n      if [[ $RES != \"0\" ]]\n      then\n        echo \"$device_name gone. proceeding...\"\n        sleep 10\n        break\n      fi\n      echo \"$device_name still attached. waiting...\"\n      sleep 5\n    done\n\n    # attach existing EBS\n    aws ec2 attach-volume --region $${region} --volume-id $${previous_volume} --instance-id $${instance_id} --device $${device}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to attach previous volume: $${previous_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to attach\n    #echo \"sleeping for 120 to allow aws time to get its ducks in a row again\"\n    #sleep 120\n    echo \"looking for attachment of $device_name\"\n\n    while true\n    do\n      lsblk|grep $device_name 1\u003e/dev/null\n      RES=$?\n      if [[ $RES == \"0\" ]]\n      then\n        echo \"$device_name found. proceeding...\"\n        sleep 10\n        break\n      fi\n      echo \"$device_name not attached. waiting...\"\n      sleep 5\n    done\n\n    current_volume=$${previous_volume}\n\n  else\n    # no previous volume found. assume tabula rasa\n    echo \"No previous volume found. Proceeding...\"\n    echo \"mkfs -t ext4 $${device}\"\n    mkfs -t ext4 $${device}\n  fi\n  \n  echo \"mount $${device} $${mountp}\"\n  mount $${device} $${mountp}\n  echo \"$${device} $${mountp} ext4 defaults,nofail 0 2\" \u003e\u003e /etc/fstab\n\n  # add tags to the volume?\n  echo \"aws ec2 create-tags --region $${region} --resources $${current_volume} --tags Key=Name,Value=\\\"$${name}\\\" Key=device,Value=$${device} Key=pipeline_phase,Value=$${pipeline_phase} Key=jive_service,Value=$${jive_service} Key=jive_subservice,Value=$${jive_subservice}\"\n  aws ec2 create-tags --region $${region} --resources $${current_volume} --tags Key=Name,Value=\"$${name}\" Key=device,Value=$${device} Key=pipeline_phase,Value=$${pipeline_phase} Key=jive_service,Value=$${jive_service} Key=jive_subservice,Value=$${jive_subservice}\n\ndone\nIFS=$OLD_IFS\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x $${INSTALL_DIRECTORY}/ebs_mount.sh\n    $${INSTALL_DIRECTORY}/ebs_mount.sh -d ${devices} 2\u003e\u00261 \u003e\u003e $${INSTALL_DIRECTORY}/ebs_mount.log\n}\n\nfunction_nexus() {\n    # URL redirect fails without this entry\n    echo \"10.10.100.155 nexus-int.eng.jiveland.com\" \u003e\u003e /etc/hosts\n\n    # Script to download Ansible artifact from Nexus\n    cat \u003c\u003c'EOF' \u003e $${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n#!/bin/bash\n# Argument = -h -v -i groupId:artifactId:version -c classifier -p packaging -r repository\n\n# Define Nexus Configuration\nNEXUS_BASE=nexus-int.eng.jiveland.com\nREST_PATH=/service/local\nART_REDIR=/artifact/maven/redirect\n\n# Read in Complete Set of Coordinates from the Command Line\nGROUP_ID=\nARTIFACT_ID=\nVERSION=\"LATEST\"\nCLASSIFIER=\"\"\nPACKAGING=tar.gz\nREPO=\"candidates\"\nVERBOSE=0\n\nwhile getopts \"hvi:c:p:\" OPTION\ndo\n     case $OPTION in\n         h)\n             usage\n             exit 1\n             ;;\n         i)\n\t     OIFS=$IFS\n             IFS=\":\"\n\t     GAV_COORD=( $OPTARG )\n\t     GROUP_ID=$${GAV_COORD[0]}\n             ARTIFACT_ID=$${GAV_COORD[1]}\n             VERSION=$${GAV_COORD[2]}\n\t     IFS=$OIFS\n             ;;\n         c)\n             CLASSIFIER=$OPTARG\n             ;;\n         p)\n             PACKAGING=$OPTARG\n             ;;\n         v)\n             VERBOSE=1\n             ;;\n         ?)\n             usage\n             exit\n             ;;\n     esac\ndone\n\nif [[ -z $GROUP_ID ]] || [[ -z $ARTIFACT_ID ]] || [[ -z $VERSION ]]\nthen\n     echo \"BAD ARGUMENTS: Either groupId, artifactId, or version was not supplied\" \u003e\u00262\n     usage\n     exit 1\nfi\n\n# Construct the base URL\nREDIRECT_URL=$${NEXUS_BASE}$${REST_PATH}$${ART_REDIR}\n\n# Generate the list of parameters\nPARAM_KEYS=( g a v r p c )\nPARAM_VALUES=( $GROUP_ID $ARTIFACT_ID $VERSION $REPO $PACKAGING $CLASSIFIER )\nPARAMS=\"\"\nfor index in $${!PARAM_KEYS[*]}\ndo\n  if [[ $${PARAM_VALUES[$index]} != \"\" ]]\n  then\n    PARAMS=\"$${PARAMS}$${PARAM_KEYS[$index]}=$${PARAM_VALUES[$index]}\u0026\"\n  fi\ndone\n\nREDIRECT_URL=\"$${REDIRECT_URL}?$${PARAMS}\"\n\necho \"Fetching Artifact from $REDIRECT_URL...\" \u003e\u00262\ncurl -sS -L $${REDIRECT_URL}\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x $${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n    $${INSTALL_DIRECTORY}/get_nexus_artifact.sh -i com.jivesoftware.techops:ansible-common:LATEST \u003e $${INSTALL_DIRECTORY}/ansible-common.tar.gz\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      aws configure set s3.signature_version s3v4\n      aws s3 cp s3://us-west-2-jive-data-pipeline-playbooks/$${ADDITIONAL_BUNDLE_NAME}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz\n    fi\n}\n\nfunction_ansible() {\n    # Need Sudo TTY\n    sed -i s/'Defaults    requiretty'/'#Defaults    requiretty'/ /etc/sudoers\n    # Disable SELINUX for SSSD\n    sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config\n    setenforce 0\n    # Create Ansible working directories\n    mkdir -p $${INSTALL_DIRECTORY}/ansible-common\n    tar xf $${INSTALL_DIRECTORY}/ansible-common.tar.gz -C $${INSTALL_DIRECTORY}/ansible-common/\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      mkdir -p $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}\n      tar xf $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz -C $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}\n    fi\n    # Set Python to 2.6 and run Ansible locally\n    alternatives --set python /usr/bin/python2.6\n    yum install -y yum-python26 python-boto ansible\n\n    # Script to run Ansible locally\n    cat \u003c\u003cEOF \u003e $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n#!/bin/bash\n\nansible-playbook -i localhost $${INSTALL_DIRECTORY}/ansible-common/playbook-generic-node.yml --connection=local\nif [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\nthen\n  $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}/bin/call_ansible.sh\nfi\nEOF\n    # Run Ansible\n    chmod +x $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n    $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh \u003e\u003e $${INSTALL_DIRECTORY}/ansible-common/ansible_debug.log\n}\n\nfunction_restart() {\n    # Need to restart for SELINUX change.\n    shutdown -r now\n}\n\n# Run the things\nfunction_prep\nif [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\nthen\n  function_instance_store\nelse\n  function_ebs_attach\nfi\nfunction_nexus\nfunction_ansible\n#function_restart\n",
                            "vars.%": "7",
                            "vars.account_name": "jive-data-pipeline",
                            "vars.bundle_name": "ansible-playbooks-aws",
                            "vars.bundle_version": "LATEST",
                            "vars.devices": "/dev/xvdm:/data",
                            "vars.pipeline_phase": "test",
                            "vars.region": "us-west-2",
                            "vars.skip_ebs_reattach": "true"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "data.template_file.s3_policy": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "f090b2802d469641e6ab3351fccf26cc9ea944bbbfbb61a8930261b819a93c5d",
                        "attributes": {
                            "id": "f090b2802d469641e6ab3351fccf26cc9ea944bbbfbb61a8930261b819a93c5d",
                            "rendered": "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"statement201609211128\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::999547976641:role/ebs-attach-and-secrets-role-cloudalytics\"\n      },\n      \"Action\": [\n        \"s3:ListBucket\",\n        \"s3:GetBucketLocation\",\n        \"s3:ListBucketMultipartUploads\",\n        \"s3:ListBucketVersions\"\n      ],\n      \"Resource\": \"arn:aws:s3:::us-west-2-jive-data-test-cloudalytics-snaps\"\n    },\n    {\n      \"Sid\": \"statement201609211130\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::999547976641:role/ebs-attach-and-secrets-role-cloudalytics\"\n      },\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:AbortMultipartUpload\",\n        \"s3:ListMultipartUploadParts\"\n      ],\n      \"Resource\": \"arn:aws:s3:::us-west-2-jive-data-test-cloudalytics-snaps/*\"\n    }\n  ]\n}\n",
                            "template": "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"statement201609211128\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::${aws_account}:role/ebs-attach-and-secrets-role-${jive_subservice}\"\n      },\n      \"Action\": [\n        \"s3:ListBucket\",\n        \"s3:GetBucketLocation\",\n        \"s3:ListBucketMultipartUploads\",\n        \"s3:ListBucketVersions\"\n      ],\n      \"Resource\": \"arn:aws:s3:::us-west-2-jive-data-${pipeline_phase}-${jive_subservice}-snaps\"\n    },\n    {\n      \"Sid\": \"statement201609211130\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::${aws_account}:role/ebs-attach-and-secrets-role-${jive_subservice}\"\n      },\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:AbortMultipartUpload\",\n        \"s3:ListMultipartUploadParts\"\n      ],\n      \"Resource\": \"arn:aws:s3:::us-west-2-jive-data-${pipeline_phase}-${jive_subservice}-snaps/*\"\n    }\n  ]\n}\n",
                            "vars.%": "3",
                            "vars.aws_account": "999547976641",
                            "vars.jive_subservice": "cloudalytics",
                            "vars.pipeline_phase": "test"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "data.template_file.user_data": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "5e25017229c727473ebab03db0cabbbc6292a7adc58db1c32028a24455c4da1d",
                        "attributes": {
                            "id": "5e25017229c727473ebab03db0cabbbc6292a7adc58db1c32028a24455c4da1d",
                            "rendered": "#!/bin/bash\n\n# SKIP_EBS_REATTACH - Set to a non-empty string to skip reattaching of any\n#                     unattached matching EBS volumes.\n#                     The ebs_attach script will still run and new volumes\n#                     will be attached/formatted as necessary\n# ADDITIONAL_BUNDLE_NAME - The name of a nexus bundle to download and unpack.\n#                          Leave blank to skip.\n#                          Must contain a script for setting up/calling ansible\n#                          located at/called:\n#           ./ansible/bin/call_ansible.sh\n#\nSKIP_EBS_REATTACH=true\nADDITIONAL_BUNDLE_NAME=ansible-playbooks-aws\nADDITIONAL_BUNDLE_VERSION=LATEST\nINSTALL_DIRECTORY=/opt/ansible\ndeclare -r instance_type=$(curl -s http://169.254.169.254/latest/meta-data/instance-type)\n\nfunction_prep() {\n    mkdir -p ${INSTALL_DIRECTORY}\n    # Get pip for awscli\n    yum install -y epel-release\n    yum install -y python-pip\n    pip install awscli\n}\n\nfunction_instance_store() {\n  yum install -y cryptsetup\n  passphrase=$(\u003c /dev/urandom tr -dc '_A-Za-z0-9@#%^_\\\\-\\\\=+' | head -c 256 | xargs -0 echo)\n\n  if [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\n  then\n    # this /could/ be a bit more flexible *wink!*\n\n    # set up disk encryption\n    echo \"cryptsetup luksFormat /dev/nvme0n1\"\n    echo $passphrase | cryptsetup luksFormat /dev/nvme0n1\n    UUID=$(cryptsetup luksUUID /dev/nvme0n1)\n    echo \"cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\"\n    echo \"$passphrase\" | cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\n    echo \"mkfs.ext4 /dev/mapper/elasticsearch_data\"\n    mkfs.ext4 /dev/mapper/elasticsearch_data\n    echo \"mount /dev/mapper/elasticsearch_data /data\"\n    mkdir -p /data\n    mount /dev/mapper/elasticsearch_data /data\n\n    # encrypt and save the volume's password\n    echo \"aws --region us-west-2 kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext xxxxxx --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\"\n    aws --region us-west-2 kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext \"${passphrase}\" --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\n    unset passphrase\n\n    cat \u003c\u003c-EOM \u003e /etc/init.d/luks-mount\n#!/bin/bash\n# A quickly hacked together script to remount a luks volume at boot\n\n# Get the passphrase from KMS using the ciphertext\npassphrase=\\$(aws --region us-west-2 kms decrypt --ciphertext-blob fileb:///etc/.luks --output text --query Plaintext | base64 -d)\n\n# Open the LUKS volume\necho \"\\$passphrase\" | cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\n\n# Mount the volume\nmount /dev/mapper/elasticsearch_data /data\nEOM\n\n    chmod 755 /etc/init.d/luks-mount\n\n    # will this work for Centos 7? No it will not.\n    #ln -s /etc/init.d/luks-mount /etc/rc3.d/S15luks\n\n    # so instead, create a systemd file:\n    cat \u003c\u003c-EOM \u003e /usr/lib/systemd/system/data_remount.service\n[Unit]\nDescription=Mount the ephemeral data volume\nDocumentation=\nBefore=elasticsearch.service\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=oneshot\nExecStart=/etc/init.d/luks-mount\n\n[Install]\nWantedBy=multi-user.target\nEOM\n\n    systemctl daemon-reload\n    systemctl enable data_remount\n\n  fi\n}\n\nfunction_ebs_attach() {\n    cat \u003c\u003c'EOF' \u003e ${INSTALL_DIRECTORY}/ebs_mount.sh\n#!/bin/bash -v\n#\n# Usage:\n# ./ebs_mount.sh -d \u003cdevice:mountpoint\u003e[,\u003cdevice:mountpoint\u003e...]\n#\n# Example:\n# ./ebs_mount.sh -d /dev/xvdm:/data/elasticsearch,/dev/xvdn:/data/more_data\n#\ndeclare -r instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)\ndeclare -r avail_zone=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)\n\nregion=\"us-west-2\"\n\nwhile getopts \"d:n:p:\" opt; do\n  case \"$opt\" in\n  d) devices=$OPTARG\n     ;;\n  esac\ndone\n\nif [ -z $name ]\nthen\n  name=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`Name`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\nfi\n\npipeline_phase=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`pipeline_phase`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_service=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_service`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_subservice=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_subservice`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\n\necho \"Pipeline_phase: ${pipeline_phase}\"\necho \"Jive_service: ${jive_service}\"\necho \"Jive_subservice: ${jive_subservice}\"\n\nOLD_IFS=$IFS\nIFS=','\nfor dev_mp_pair in $devices\ndo\n  # I have no idea what I'm doing\n  IFS=':' read -ra PAIR \u003c\u003c\u003c \"$dev_mp_pair\"\n  IFS=','\n  device=${PAIR[0]}\n  mountp=${PAIR[1]}\n  echo \"Device: ${device}\"\n  if [ -z $device ]\n  then\n    echo \"[ERROR] Did you specify a device name?\"\n    continue\n  fi\n\n  echo \"MountP: ${mountp}\"\n  if [ -z $mountp ]\n  then\n    echo \"[ERROR] Did you specify a mount point?\"\n    continue\n  fi\n\n  mkdir -p ${mountp}\n\n  if [ -z ${SKIP_EBS_REATTACH} ]\n  then\n    # Search for existing tagged EBS volume (in current AZ)\n    echo \"aws ec2 describe-volumes --region=${region} --filters Name=availability-zone,Values=${avail_zone} Name=tag:pipeline_phase,Values=${pipeline_phase} Name=tag:jive_service,Values=${jive_service} Name=tag:jive_subservice,Values=${jive_subservice} Name=status,Values=available Name=tag:device,Values=${device} Name=tag:Name,Values=${name} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n    previous_volume=$(aws ec2 describe-volumes --region=${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=available Name=tag:device,Values=${device} Name=tag:Name,Values=${name} Name=tag:pipeline_phase,Values=${pipeline_phase} Name=tag:jive_service,Values=${jive_service} Name=tag:jive_subservice,Values=${jive_subservice} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n    echo \"Previous volume: ${previous_volume}\"\n  else\n    previous_volume=\"\"\n    echo \"SKIP_EBS_REATTACH is set, not attempting to reattach old volume(s)\"\n  fi\n\n  # find current volume id\n  echo \"aws ec2 describe-volumes --region ${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=${instance_id} Name=attachment.device,Values=${device} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n  current_volume=$(aws ec2 describe-volumes --region ${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=${instance_id} Name=attachment.device,Values=${device} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n\n  if [ $? -ne 0 ]\n  then\n    echo \"[ERROR] Failed to get current volume ID for ${device}\"\n    continue\n  fi\n\n  echo \"Current volume: ${current_volume}\"\n\n  if [ ! -z $previous_volume ]\n  then\n\n    # detach current EBS\n    echo \"detaching current volume: ${current_volume}\"\n    aws ec2 detach-volume --region ${region} --volume-id ${current_volume}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to detach current volume: ${current_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to detach\n    #echo \"sleeping for 120 to allow aws time to get its ducks in a row\"\n    #sleep 120\n    device_name=$(basename ${device})\n    echo \"looking for detachment of $device_name\"\n\n    while true\n    do\n      lsblk|grep $device_name 1\u003e/dev/null\n      RES=$?\n      if [[ $RES != \"0\" ]]\n      then\n        echo \"$device_name gone. proceeding...\"\n        sleep 10\n        break\n      fi\n      echo \"$device_name still attached. waiting...\"\n      sleep 5\n    done\n\n    # attach existing EBS\n    aws ec2 attach-volume --region ${region} --volume-id ${previous_volume} --instance-id ${instance_id} --device ${device}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to attach previous volume: ${previous_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to attach\n    #echo \"sleeping for 120 to allow aws time to get its ducks in a row again\"\n    #sleep 120\n    echo \"looking for attachment of $device_name\"\n\n    while true\n    do\n      lsblk|grep $device_name 1\u003e/dev/null\n      RES=$?\n      if [[ $RES == \"0\" ]]\n      then\n        echo \"$device_name found. proceeding...\"\n        sleep 10\n        break\n      fi\n      echo \"$device_name not attached. waiting...\"\n      sleep 5\n    done\n\n    current_volume=${previous_volume}\n\n  else\n    # no previous volume found. assume tabula rasa\n    echo \"No previous volume found. Proceeding...\"\n    echo \"mkfs -t ext4 ${device}\"\n    mkfs -t ext4 ${device}\n  fi\n  \n  echo \"mount ${device} ${mountp}\"\n  mount ${device} ${mountp}\n  echo \"${device} ${mountp} ext4 defaults,nofail 0 2\" \u003e\u003e /etc/fstab\n\n  # add tags to the volume?\n  echo \"aws ec2 create-tags --region ${region} --resources ${current_volume} --tags Key=Name,Value=\\\"${name}\\\" Key=device,Value=${device} Key=pipeline_phase,Value=${pipeline_phase} Key=jive_service,Value=${jive_service} Key=jive_subservice,Value=${jive_subservice}\"\n  aws ec2 create-tags --region ${region} --resources ${current_volume} --tags Key=Name,Value=\"${name}\" Key=device,Value=${device} Key=pipeline_phase,Value=${pipeline_phase} Key=jive_service,Value=${jive_service} Key=jive_subservice,Value=${jive_subservice}\n\ndone\nIFS=$OLD_IFS\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x ${INSTALL_DIRECTORY}/ebs_mount.sh\n    ${INSTALL_DIRECTORY}/ebs_mount.sh -d /dev/xvdm:/data 2\u003e\u00261 \u003e\u003e ${INSTALL_DIRECTORY}/ebs_mount.log\n}\n\nfunction_nexus() {\n    # URL redirect fails without this entry\n    echo \"10.10.100.155 nexus-int.eng.jiveland.com\" \u003e\u003e /etc/hosts\n\n    # Script to download Ansible artifact from Nexus\n    cat \u003c\u003c'EOF' \u003e ${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n#!/bin/bash\n# Argument = -h -v -i groupId:artifactId:version -c classifier -p packaging -r repository\n\n# Define Nexus Configuration\nNEXUS_BASE=nexus-int.eng.jiveland.com\nREST_PATH=/service/local\nART_REDIR=/artifact/maven/redirect\n\n# Read in Complete Set of Coordinates from the Command Line\nGROUP_ID=\nARTIFACT_ID=\nVERSION=\"LATEST\"\nCLASSIFIER=\"\"\nPACKAGING=tar.gz\nREPO=\"candidates\"\nVERBOSE=0\n\nwhile getopts \"hvi:c:p:\" OPTION\ndo\n     case $OPTION in\n         h)\n             usage\n             exit 1\n             ;;\n         i)\n\t     OIFS=$IFS\n             IFS=\":\"\n\t     GAV_COORD=( $OPTARG )\n\t     GROUP_ID=${GAV_COORD[0]}\n             ARTIFACT_ID=${GAV_COORD[1]}\n             VERSION=${GAV_COORD[2]}\n\t     IFS=$OIFS\n             ;;\n         c)\n             CLASSIFIER=$OPTARG\n             ;;\n         p)\n             PACKAGING=$OPTARG\n             ;;\n         v)\n             VERBOSE=1\n             ;;\n         ?)\n             usage\n             exit\n             ;;\n     esac\ndone\n\nif [[ -z $GROUP_ID ]] || [[ -z $ARTIFACT_ID ]] || [[ -z $VERSION ]]\nthen\n     echo \"BAD ARGUMENTS: Either groupId, artifactId, or version was not supplied\" \u003e\u00262\n     usage\n     exit 1\nfi\n\n# Construct the base URL\nREDIRECT_URL=${NEXUS_BASE}${REST_PATH}${ART_REDIR}\n\n# Generate the list of parameters\nPARAM_KEYS=( g a v r p c )\nPARAM_VALUES=( $GROUP_ID $ARTIFACT_ID $VERSION $REPO $PACKAGING $CLASSIFIER )\nPARAMS=\"\"\nfor index in ${!PARAM_KEYS[*]}\ndo\n  if [[ ${PARAM_VALUES[$index]} != \"\" ]]\n  then\n    PARAMS=\"${PARAMS}${PARAM_KEYS[$index]}=${PARAM_VALUES[$index]}\u0026\"\n  fi\ndone\n\nREDIRECT_URL=\"${REDIRECT_URL}?${PARAMS}\"\n\necho \"Fetching Artifact from $REDIRECT_URL...\" \u003e\u00262\ncurl -sS -L ${REDIRECT_URL}\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x ${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n    ${INSTALL_DIRECTORY}/get_nexus_artifact.sh -i com.jivesoftware.techops:ansible-common:LATEST \u003e ${INSTALL_DIRECTORY}/ansible-common.tar.gz\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      aws configure set s3.signature_version s3v4\n      aws s3 cp s3://us-west-2-jive-data-pipeline-playbooks/${ADDITIONAL_BUNDLE_NAME}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz\n    fi\n}\n\nfunction_ansible() {\n    # Need Sudo TTY\n    sed -i s/'Defaults    requiretty'/'#Defaults    requiretty'/ /etc/sudoers\n    # Disable SELINUX for SSSD\n    sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config\n    setenforce 0\n    # Create Ansible working directories\n    mkdir -p ${INSTALL_DIRECTORY}/ansible-common\n    tar xf ${INSTALL_DIRECTORY}/ansible-common.tar.gz -C ${INSTALL_DIRECTORY}/ansible-common/\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      mkdir -p ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}\n      tar xf ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz -C ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}\n    fi\n    # Set Python to 2.6 and run Ansible locally\n    alternatives --set python /usr/bin/python2.6\n    yum install -y yum-python26 python-boto ansible\n\n    # Script to run Ansible locally\n    cat \u003c\u003cEOF \u003e ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n#!/bin/bash\n\nansible-playbook -i localhost ${INSTALL_DIRECTORY}/ansible-common/playbook-generic-node.yml --connection=local\nif [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\nthen\n  ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}/bin/call_ansible.sh\nfi\nEOF\n    # Run Ansible\n    chmod +x ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n    ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh \u003e\u003e ${INSTALL_DIRECTORY}/ansible-common/ansible_debug.log\n}\n\nfunction_restart() {\n    # Need to restart for SELINUX change.\n    shutdown -r now\n}\n\n# Run the things\nfunction_prep\nif [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\nthen\n  function_instance_store\nelse\n  function_ebs_attach\nfi\nfunction_nexus\nfunction_ansible\n#function_restart\n",
                            "template": "#!/bin/bash\n\n# SKIP_EBS_REATTACH - Set to a non-empty string to skip reattaching of any\n#                     unattached matching EBS volumes.\n#                     The ebs_attach script will still run and new volumes\n#                     will be attached/formatted as necessary\n# ADDITIONAL_BUNDLE_NAME - The name of a nexus bundle to download and unpack.\n#                          Leave blank to skip.\n#                          Must contain a script for setting up/calling ansible\n#                          located at/called:\n#           ./ansible/bin/call_ansible.sh\n#\nSKIP_EBS_REATTACH=${skip_ebs_reattach}\nADDITIONAL_BUNDLE_NAME=${bundle_name}\nADDITIONAL_BUNDLE_VERSION=${bundle_version}\nINSTALL_DIRECTORY=/opt/ansible\ndeclare -r instance_type=$(curl -s http://169.254.169.254/latest/meta-data/instance-type)\n\nfunction_prep() {\n    mkdir -p $${INSTALL_DIRECTORY}\n    # Get pip for awscli\n    yum install -y epel-release\n    yum install -y python-pip\n    pip install awscli\n}\n\nfunction_instance_store() {\n  yum install -y cryptsetup\n  passphrase=$(\u003c /dev/urandom tr -dc '_A-Za-z0-9@#%^_\\\\-\\\\=+' | head -c 256 | xargs -0 echo)\n\n  if [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\n  then\n    # this /could/ be a bit more flexible *wink!*\n\n    # set up disk encryption\n    echo \"cryptsetup luksFormat /dev/nvme0n1\"\n    echo $passphrase | cryptsetup luksFormat /dev/nvme0n1\n    UUID=$(cryptsetup luksUUID /dev/nvme0n1)\n    echo \"cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\"\n    echo \"$passphrase\" | cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\n    echo \"mkfs.ext4 /dev/mapper/elasticsearch_data\"\n    mkfs.ext4 /dev/mapper/elasticsearch_data\n    echo \"mount /dev/mapper/elasticsearch_data /data\"\n    mkdir -p /data\n    mount /dev/mapper/elasticsearch_data /data\n\n    # encrypt and save the volume's password\n    echo \"aws --region ${region} kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext xxxxxx --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\"\n    aws --region ${region} kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext \"$${passphrase}\" --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\n    unset passphrase\n\n    cat \u003c\u003c-EOM \u003e /etc/init.d/luks-mount\n#!/bin/bash\n# A quickly hacked together script to remount a luks volume at boot\n\n# Get the passphrase from KMS using the ciphertext\npassphrase=\\$(aws --region ${region} kms decrypt --ciphertext-blob fileb:///etc/.luks --output text --query Plaintext | base64 -d)\n\n# Open the LUKS volume\necho \"\\$passphrase\" | cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\n\n# Mount the volume\nmount /dev/mapper/elasticsearch_data /data\nEOM\n\n    chmod 755 /etc/init.d/luks-mount\n\n    # will this work for Centos 7? No it will not.\n    #ln -s /etc/init.d/luks-mount /etc/rc3.d/S15luks\n\n    # so instead, create a systemd file:\n    cat \u003c\u003c-EOM \u003e /usr/lib/systemd/system/data_remount.service\n[Unit]\nDescription=Mount the ephemeral data volume\nDocumentation=\nBefore=elasticsearch.service\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=oneshot\nExecStart=/etc/init.d/luks-mount\n\n[Install]\nWantedBy=multi-user.target\nEOM\n\n    systemctl daemon-reload\n    systemctl enable data_remount\n\n  fi\n}\n\nfunction_ebs_attach() {\n    cat \u003c\u003c'EOF' \u003e $${INSTALL_DIRECTORY}/ebs_mount.sh\n#!/bin/bash -v\n#\n# Usage:\n# ./ebs_mount.sh -d \u003cdevice:mountpoint\u003e[,\u003cdevice:mountpoint\u003e...]\n#\n# Example:\n# ./ebs_mount.sh -d /dev/xvdm:/data/elasticsearch,/dev/xvdn:/data/more_data\n#\ndeclare -r instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)\ndeclare -r avail_zone=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)\n\nregion=\"${region}\"\n\nwhile getopts \"d:n:p:\" opt; do\n  case \"$opt\" in\n  d) devices=$OPTARG\n     ;;\n  esac\ndone\n\nif [ -z $name ]\nthen\n  name=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`Name`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\nfi\n\npipeline_phase=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`pipeline_phase`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_service=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_service`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_subservice=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_subservice`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\n\necho \"Pipeline_phase: $${pipeline_phase}\"\necho \"Jive_service: $${jive_service}\"\necho \"Jive_subservice: $${jive_subservice}\"\n\nOLD_IFS=$IFS\nIFS=','\nfor dev_mp_pair in $devices\ndo\n  # I have no idea what I'm doing\n  IFS=':' read -ra PAIR \u003c\u003c\u003c \"$dev_mp_pair\"\n  IFS=','\n  device=$${PAIR[0]}\n  mountp=$${PAIR[1]}\n  echo \"Device: $${device}\"\n  if [ -z $device ]\n  then\n    echo \"[ERROR] Did you specify a device name?\"\n    continue\n  fi\n\n  echo \"MountP: $${mountp}\"\n  if [ -z $mountp ]\n  then\n    echo \"[ERROR] Did you specify a mount point?\"\n    continue\n  fi\n\n  mkdir -p $${mountp}\n\n  if [ -z $${SKIP_EBS_REATTACH} ]\n  then\n    # Search for existing tagged EBS volume (in current AZ)\n    echo \"aws ec2 describe-volumes --region=$${region} --filters Name=availability-zone,Values=$${avail_zone} Name=tag:pipeline_phase,Values=$${pipeline_phase} Name=tag:jive_service,Values=$${jive_service} Name=tag:jive_subservice,Values=$${jive_subservice} Name=status,Values=available Name=tag:device,Values=$${device} Name=tag:Name,Values=$${name} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n    previous_volume=$(aws ec2 describe-volumes --region=$${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=available Name=tag:device,Values=$${device} Name=tag:Name,Values=$${name} Name=tag:pipeline_phase,Values=$${pipeline_phase} Name=tag:jive_service,Values=$${jive_service} Name=tag:jive_subservice,Values=$${jive_subservice} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n    echo \"Previous volume: $${previous_volume}\"\n  else\n    previous_volume=\"\"\n    echo \"SKIP_EBS_REATTACH is set, not attempting to reattach old volume(s)\"\n  fi\n\n  # find current volume id\n  echo \"aws ec2 describe-volumes --region $${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=$${instance_id} Name=attachment.device,Values=$${device} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n  current_volume=$(aws ec2 describe-volumes --region $${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=$${instance_id} Name=attachment.device,Values=$${device} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n\n  if [ $? -ne 0 ]\n  then\n    echo \"[ERROR] Failed to get current volume ID for $${device}\"\n    continue\n  fi\n\n  echo \"Current volume: $${current_volume}\"\n\n  if [ ! -z $previous_volume ]\n  then\n\n    # detach current EBS\n    echo \"detaching current volume: $${current_volume}\"\n    aws ec2 detach-volume --region $${region} --volume-id $${current_volume}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to detach current volume: $${current_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to detach\n    #echo \"sleeping for 120 to allow aws time to get its ducks in a row\"\n    #sleep 120\n    device_name=$(basename $${device})\n    echo \"looking for detachment of $device_name\"\n\n    while true\n    do\n      lsblk|grep $device_name 1\u003e/dev/null\n      RES=$?\n      if [[ $RES != \"0\" ]]\n      then\n        echo \"$device_name gone. proceeding...\"\n        sleep 10\n        break\n      fi\n      echo \"$device_name still attached. waiting...\"\n      sleep 5\n    done\n\n    # attach existing EBS\n    aws ec2 attach-volume --region $${region} --volume-id $${previous_volume} --instance-id $${instance_id} --device $${device}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to attach previous volume: $${previous_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to attach\n    #echo \"sleeping for 120 to allow aws time to get its ducks in a row again\"\n    #sleep 120\n    echo \"looking for attachment of $device_name\"\n\n    while true\n    do\n      lsblk|grep $device_name 1\u003e/dev/null\n      RES=$?\n      if [[ $RES == \"0\" ]]\n      then\n        echo \"$device_name found. proceeding...\"\n        sleep 10\n        break\n      fi\n      echo \"$device_name not attached. waiting...\"\n      sleep 5\n    done\n\n    current_volume=$${previous_volume}\n\n  else\n    # no previous volume found. assume tabula rasa\n    echo \"No previous volume found. Proceeding...\"\n    echo \"mkfs -t ext4 $${device}\"\n    mkfs -t ext4 $${device}\n  fi\n  \n  echo \"mount $${device} $${mountp}\"\n  mount $${device} $${mountp}\n  echo \"$${device} $${mountp} ext4 defaults,nofail 0 2\" \u003e\u003e /etc/fstab\n\n  # add tags to the volume?\n  echo \"aws ec2 create-tags --region $${region} --resources $${current_volume} --tags Key=Name,Value=\\\"$${name}\\\" Key=device,Value=$${device} Key=pipeline_phase,Value=$${pipeline_phase} Key=jive_service,Value=$${jive_service} Key=jive_subservice,Value=$${jive_subservice}\"\n  aws ec2 create-tags --region $${region} --resources $${current_volume} --tags Key=Name,Value=\"$${name}\" Key=device,Value=$${device} Key=pipeline_phase,Value=$${pipeline_phase} Key=jive_service,Value=$${jive_service} Key=jive_subservice,Value=$${jive_subservice}\n\ndone\nIFS=$OLD_IFS\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x $${INSTALL_DIRECTORY}/ebs_mount.sh\n    $${INSTALL_DIRECTORY}/ebs_mount.sh -d ${devices} 2\u003e\u00261 \u003e\u003e $${INSTALL_DIRECTORY}/ebs_mount.log\n}\n\nfunction_nexus() {\n    # URL redirect fails without this entry\n    echo \"10.10.100.155 nexus-int.eng.jiveland.com\" \u003e\u003e /etc/hosts\n\n    # Script to download Ansible artifact from Nexus\n    cat \u003c\u003c'EOF' \u003e $${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n#!/bin/bash\n# Argument = -h -v -i groupId:artifactId:version -c classifier -p packaging -r repository\n\n# Define Nexus Configuration\nNEXUS_BASE=nexus-int.eng.jiveland.com\nREST_PATH=/service/local\nART_REDIR=/artifact/maven/redirect\n\n# Read in Complete Set of Coordinates from the Command Line\nGROUP_ID=\nARTIFACT_ID=\nVERSION=\"LATEST\"\nCLASSIFIER=\"\"\nPACKAGING=tar.gz\nREPO=\"candidates\"\nVERBOSE=0\n\nwhile getopts \"hvi:c:p:\" OPTION\ndo\n     case $OPTION in\n         h)\n             usage\n             exit 1\n             ;;\n         i)\n\t     OIFS=$IFS\n             IFS=\":\"\n\t     GAV_COORD=( $OPTARG )\n\t     GROUP_ID=$${GAV_COORD[0]}\n             ARTIFACT_ID=$${GAV_COORD[1]}\n             VERSION=$${GAV_COORD[2]}\n\t     IFS=$OIFS\n             ;;\n         c)\n             CLASSIFIER=$OPTARG\n             ;;\n         p)\n             PACKAGING=$OPTARG\n             ;;\n         v)\n             VERBOSE=1\n             ;;\n         ?)\n             usage\n             exit\n             ;;\n     esac\ndone\n\nif [[ -z $GROUP_ID ]] || [[ -z $ARTIFACT_ID ]] || [[ -z $VERSION ]]\nthen\n     echo \"BAD ARGUMENTS: Either groupId, artifactId, or version was not supplied\" \u003e\u00262\n     usage\n     exit 1\nfi\n\n# Construct the base URL\nREDIRECT_URL=$${NEXUS_BASE}$${REST_PATH}$${ART_REDIR}\n\n# Generate the list of parameters\nPARAM_KEYS=( g a v r p c )\nPARAM_VALUES=( $GROUP_ID $ARTIFACT_ID $VERSION $REPO $PACKAGING $CLASSIFIER )\nPARAMS=\"\"\nfor index in $${!PARAM_KEYS[*]}\ndo\n  if [[ $${PARAM_VALUES[$index]} != \"\" ]]\n  then\n    PARAMS=\"$${PARAMS}$${PARAM_KEYS[$index]}=$${PARAM_VALUES[$index]}\u0026\"\n  fi\ndone\n\nREDIRECT_URL=\"$${REDIRECT_URL}?$${PARAMS}\"\n\necho \"Fetching Artifact from $REDIRECT_URL...\" \u003e\u00262\ncurl -sS -L $${REDIRECT_URL}\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x $${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n    $${INSTALL_DIRECTORY}/get_nexus_artifact.sh -i com.jivesoftware.techops:ansible-common:LATEST \u003e $${INSTALL_DIRECTORY}/ansible-common.tar.gz\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      aws configure set s3.signature_version s3v4\n      aws s3 cp s3://us-west-2-jive-data-pipeline-playbooks/$${ADDITIONAL_BUNDLE_NAME}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz\n    fi\n}\n\nfunction_ansible() {\n    # Need Sudo TTY\n    sed -i s/'Defaults    requiretty'/'#Defaults    requiretty'/ /etc/sudoers\n    # Disable SELINUX for SSSD\n    sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config\n    setenforce 0\n    # Create Ansible working directories\n    mkdir -p $${INSTALL_DIRECTORY}/ansible-common\n    tar xf $${INSTALL_DIRECTORY}/ansible-common.tar.gz -C $${INSTALL_DIRECTORY}/ansible-common/\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      mkdir -p $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}\n      tar xf $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz -C $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}\n    fi\n    # Set Python to 2.6 and run Ansible locally\n    alternatives --set python /usr/bin/python2.6\n    yum install -y yum-python26 python-boto ansible\n\n    # Script to run Ansible locally\n    cat \u003c\u003cEOF \u003e $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n#!/bin/bash\n\nansible-playbook -i localhost $${INSTALL_DIRECTORY}/ansible-common/playbook-generic-node.yml --connection=local\nif [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\nthen\n  $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}/bin/call_ansible.sh\nfi\nEOF\n    # Run Ansible\n    chmod +x $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n    $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh \u003e\u003e $${INSTALL_DIRECTORY}/ansible-common/ansible_debug.log\n}\n\nfunction_restart() {\n    # Need to restart for SELINUX change.\n    shutdown -r now\n}\n\n# Run the things\nfunction_prep\nif [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\nthen\n  function_instance_store\nelse\n  function_ebs_attach\nfi\nfunction_nexus\nfunction_ansible\n#function_restart\n",
                            "vars.%": "7",
                            "vars.account_name": "jive-data-pipeline",
                            "vars.bundle_name": "ansible-playbooks-aws",
                            "vars.bundle_version": "LATEST",
                            "vars.devices": "/dev/xvdm:/data",
                            "vars.pipeline_phase": "test",
                            "vars.region": "us-west-2",
                            "vars.skip_ebs_reattach": "true"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                }
            },
            "depends_on": []
        }
    ]
}
