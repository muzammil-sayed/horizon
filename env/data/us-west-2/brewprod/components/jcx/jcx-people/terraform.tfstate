{
    "version": 3,
    "terraform_version": "0.7.13",
    "serial": 3,
    "lineage": "f99b415f-eab8-41f8-865d-91792a345859",
    "modules": [
        {
            "path": [
                "root"
            ],
            "outputs": {},
            "resources": {
                "aws_autoscaling_group.elasticsearch_node_asg.0": {
                    "type": "aws_autoscaling_group",
                    "depends_on": [
                        "aws_launch_configuration.elasticsearch_node"
                    ],
                    "primary": {
                        "id": "brewprod-jcxpeople-es-node-az1-asg",
                        "attributes": {
                            "arn": "arn:aws:autoscaling:us-west-2:409573287771:autoScalingGroup:90337be5-c5f0-4350-a619-85df1067e17f:autoScalingGroupName/brewprod-jcxpeople-es-node-az1-asg",
                            "availability_zones.#": "1",
                            "availability_zones.2487133097": "us-west-2a",
                            "default_cooldown": "300",
                            "desired_capacity": "1",
                            "force_delete": "false",
                            "health_check_grace_period": "300",
                            "health_check_type": "EC2",
                            "id": "brewprod-jcxpeople-es-node-az1-asg",
                            "launch_configuration": "jcxpeople-elasticsearch-node-20170324202616518507233lfb",
                            "load_balancers.#": "0",
                            "max_size": "1",
                            "metrics_granularity": "1Minute",
                            "min_size": "1",
                            "name": "brewprod-jcxpeople-es-node-az1-asg",
                            "placement_group": "",
                            "protect_from_scale_in": "false",
                            "tag.#": "7",
                            "tag.1089450927.key": "jive_subservice",
                            "tag.1089450927.propagate_at_launch": "true",
                            "tag.1089450927.value": "usw2-jcx-people",
                            "tag.169118653.key": "sla",
                            "tag.169118653.propagate_at_launch": "true",
                            "tag.169118653.value": "prod",
                            "tag.1811987920.key": "jive_service",
                            "tag.1811987920.propagate_at_launch": "true",
                            "tag.1811987920.value": "jcx",
                            "tag.2810355756.key": "make_dns",
                            "tag.2810355756.propagate_at_launch": "true",
                            "tag.2810355756.value": "node.aws-brewprod-usw2-jcx-people",
                            "tag.2839796150.key": "pipeline_phase",
                            "tag.2839796150.propagate_at_launch": "true",
                            "tag.2839796150.value": "brewprod",
                            "tag.2950372525.key": "service_component",
                            "tag.2950372525.propagate_at_launch": "true",
                            "tag.2950372525.value": "es_node",
                            "tag.3299193778.key": "Name",
                            "tag.3299193778.propagate_at_launch": "true",
                            "tag.3299193778.value": "brewprod-jcxpeople-es-node",
                            "target_group_arns.#": "0",
                            "termination_policies.#": "0",
                            "vpc_zone_identifier.#": "1",
                            "vpc_zone_identifier.414798253": "subnet-3b59924d",
                            "wait_for_capacity_timeout": "10m"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_autoscaling_group.elasticsearch_node_asg.1": {
                    "type": "aws_autoscaling_group",
                    "depends_on": [
                        "aws_launch_configuration.elasticsearch_node"
                    ],
                    "primary": {
                        "id": "brewprod-jcxpeople-es-node-az2-asg",
                        "attributes": {
                            "arn": "arn:aws:autoscaling:us-west-2:409573287771:autoScalingGroup:a85edbbb-51d0-4a18-b771-9d107dc8276b:autoScalingGroupName/brewprod-jcxpeople-es-node-az2-asg",
                            "availability_zones.#": "1",
                            "availability_zones.221770259": "us-west-2b",
                            "default_cooldown": "300",
                            "desired_capacity": "1",
                            "force_delete": "false",
                            "health_check_grace_period": "300",
                            "health_check_type": "EC2",
                            "id": "brewprod-jcxpeople-es-node-az2-asg",
                            "launch_configuration": "jcxpeople-elasticsearch-node-20170324202616518507233lfb",
                            "load_balancers.#": "0",
                            "max_size": "1",
                            "metrics_granularity": "1Minute",
                            "min_size": "1",
                            "name": "brewprod-jcxpeople-es-node-az2-asg",
                            "placement_group": "",
                            "protect_from_scale_in": "false",
                            "tag.#": "7",
                            "tag.1089450927.key": "jive_subservice",
                            "tag.1089450927.propagate_at_launch": "true",
                            "tag.1089450927.value": "usw2-jcx-people",
                            "tag.169118653.key": "sla",
                            "tag.169118653.propagate_at_launch": "true",
                            "tag.169118653.value": "prod",
                            "tag.1811987920.key": "jive_service",
                            "tag.1811987920.propagate_at_launch": "true",
                            "tag.1811987920.value": "jcx",
                            "tag.2810355756.key": "make_dns",
                            "tag.2810355756.propagate_at_launch": "true",
                            "tag.2810355756.value": "node.aws-brewprod-usw2-jcx-people",
                            "tag.2839796150.key": "pipeline_phase",
                            "tag.2839796150.propagate_at_launch": "true",
                            "tag.2839796150.value": "brewprod",
                            "tag.2950372525.key": "service_component",
                            "tag.2950372525.propagate_at_launch": "true",
                            "tag.2950372525.value": "es_node",
                            "tag.3299193778.key": "Name",
                            "tag.3299193778.propagate_at_launch": "true",
                            "tag.3299193778.value": "brewprod-jcxpeople-es-node",
                            "target_group_arns.#": "0",
                            "termination_policies.#": "0",
                            "vpc_zone_identifier.#": "1",
                            "vpc_zone_identifier.250846469": "subnet-1fd4657b",
                            "wait_for_capacity_timeout": "10m"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_autoscaling_group.elasticsearch_node_asg.2": {
                    "type": "aws_autoscaling_group",
                    "depends_on": [
                        "aws_launch_configuration.elasticsearch_node"
                    ],
                    "primary": {
                        "id": "brewprod-jcxpeople-es-node-az3-asg",
                        "attributes": {
                            "arn": "arn:aws:autoscaling:us-west-2:409573287771:autoScalingGroup:402234aa-cbbc-4954-8131-92d396c7ddbb:autoScalingGroupName/brewprod-jcxpeople-es-node-az3-asg",
                            "availability_zones.#": "1",
                            "availability_zones.2050015877": "us-west-2c",
                            "default_cooldown": "300",
                            "desired_capacity": "1",
                            "force_delete": "false",
                            "health_check_grace_period": "300",
                            "health_check_type": "EC2",
                            "id": "brewprod-jcxpeople-es-node-az3-asg",
                            "launch_configuration": "jcxpeople-elasticsearch-node-20170324202616518507233lfb",
                            "load_balancers.#": "0",
                            "max_size": "1",
                            "metrics_granularity": "1Minute",
                            "min_size": "1",
                            "name": "brewprod-jcxpeople-es-node-az3-asg",
                            "placement_group": "",
                            "protect_from_scale_in": "false",
                            "tag.#": "7",
                            "tag.1089450927.key": "jive_subservice",
                            "tag.1089450927.propagate_at_launch": "true",
                            "tag.1089450927.value": "usw2-jcx-people",
                            "tag.169118653.key": "sla",
                            "tag.169118653.propagate_at_launch": "true",
                            "tag.169118653.value": "prod",
                            "tag.1811987920.key": "jive_service",
                            "tag.1811987920.propagate_at_launch": "true",
                            "tag.1811987920.value": "jcx",
                            "tag.2810355756.key": "make_dns",
                            "tag.2810355756.propagate_at_launch": "true",
                            "tag.2810355756.value": "node.aws-brewprod-usw2-jcx-people",
                            "tag.2839796150.key": "pipeline_phase",
                            "tag.2839796150.propagate_at_launch": "true",
                            "tag.2839796150.value": "brewprod",
                            "tag.2950372525.key": "service_component",
                            "tag.2950372525.propagate_at_launch": "true",
                            "tag.2950372525.value": "es_node",
                            "tag.3299193778.key": "Name",
                            "tag.3299193778.propagate_at_launch": "true",
                            "tag.3299193778.value": "brewprod-jcxpeople-es-node",
                            "target_group_arns.#": "0",
                            "termination_policies.#": "0",
                            "vpc_zone_identifier.#": "1",
                            "vpc_zone_identifier.3935488465": "subnet-897b61d0",
                            "wait_for_capacity_timeout": "10m"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_autoscaling_group.nginx_node_asg.0": {
                    "type": "aws_autoscaling_group",
                    "depends_on": [
                        "aws_launch_configuration.nginx_node"
                    ],
                    "primary": {
                        "id": "brewprod-jcxpeople-es-nginx-az1-asg",
                        "attributes": {
                            "arn": "arn:aws:autoscaling:us-west-2:409573287771:autoScalingGroup:a83d85b1-5056-4b7c-9721-bbe972840430:autoScalingGroupName/brewprod-jcxpeople-es-nginx-az1-asg",
                            "availability_zones.#": "1",
                            "availability_zones.2487133097": "us-west-2a",
                            "default_cooldown": "300",
                            "desired_capacity": "1",
                            "force_delete": "false",
                            "health_check_grace_period": "300",
                            "health_check_type": "EC2",
                            "id": "brewprod-jcxpeople-es-nginx-az1-asg",
                            "launch_configuration": "jcxpeople-nginx-es-node-20170324202616526510312azr",
                            "load_balancers.#": "0",
                            "max_size": "1",
                            "metrics_granularity": "1Minute",
                            "min_size": "1",
                            "name": "brewprod-jcxpeople-es-nginx-az1-asg",
                            "placement_group": "",
                            "protect_from_scale_in": "false",
                            "tag.#": "7",
                            "tag.1089450927.key": "jive_subservice",
                            "tag.1089450927.propagate_at_launch": "true",
                            "tag.1089450927.value": "usw2-jcx-people",
                            "tag.1643727701.key": "Name",
                            "tag.1643727701.propagate_at_launch": "true",
                            "tag.1643727701.value": "brewprod-jcxpeople-es-nginx",
                            "tag.169118653.key": "sla",
                            "tag.169118653.propagate_at_launch": "true",
                            "tag.169118653.value": "prod",
                            "tag.1811987920.key": "jive_service",
                            "tag.1811987920.propagate_at_launch": "true",
                            "tag.1811987920.value": "jcx",
                            "tag.2839796150.key": "pipeline_phase",
                            "tag.2839796150.propagate_at_launch": "true",
                            "tag.2839796150.value": "brewprod",
                            "tag.2851612828.key": "make_dns",
                            "tag.2851612828.propagate_at_launch": "true",
                            "tag.2851612828.value": "es-nginx.aws-brewprod-usw2-jcx-people",
                            "tag.3969529871.key": "service_component",
                            "tag.3969529871.propagate_at_launch": "true",
                            "tag.3969529871.value": "es_nginx",
                            "target_group_arns.#": "0",
                            "termination_policies.#": "0",
                            "vpc_zone_identifier.#": "1",
                            "vpc_zone_identifier.414798253": "subnet-3b59924d",
                            "wait_for_capacity_timeout": "10m"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_autoscaling_group.nginx_node_asg.1": {
                    "type": "aws_autoscaling_group",
                    "depends_on": [
                        "aws_launch_configuration.nginx_node"
                    ],
                    "primary": {
                        "id": "brewprod-jcxpeople-es-nginx-az2-asg",
                        "attributes": {
                            "arn": "arn:aws:autoscaling:us-west-2:409573287771:autoScalingGroup:542cf88c-09aa-4db3-8bba-1090e8974dc6:autoScalingGroupName/brewprod-jcxpeople-es-nginx-az2-asg",
                            "availability_zones.#": "1",
                            "availability_zones.221770259": "us-west-2b",
                            "default_cooldown": "300",
                            "desired_capacity": "1",
                            "force_delete": "false",
                            "health_check_grace_period": "300",
                            "health_check_type": "EC2",
                            "id": "brewprod-jcxpeople-es-nginx-az2-asg",
                            "launch_configuration": "jcxpeople-nginx-es-node-20170324202616526510312azr",
                            "load_balancers.#": "0",
                            "max_size": "1",
                            "metrics_granularity": "1Minute",
                            "min_size": "1",
                            "name": "brewprod-jcxpeople-es-nginx-az2-asg",
                            "placement_group": "",
                            "protect_from_scale_in": "false",
                            "tag.#": "7",
                            "tag.1089450927.key": "jive_subservice",
                            "tag.1089450927.propagate_at_launch": "true",
                            "tag.1089450927.value": "usw2-jcx-people",
                            "tag.1643727701.key": "Name",
                            "tag.1643727701.propagate_at_launch": "true",
                            "tag.1643727701.value": "brewprod-jcxpeople-es-nginx",
                            "tag.169118653.key": "sla",
                            "tag.169118653.propagate_at_launch": "true",
                            "tag.169118653.value": "prod",
                            "tag.1811987920.key": "jive_service",
                            "tag.1811987920.propagate_at_launch": "true",
                            "tag.1811987920.value": "jcx",
                            "tag.2839796150.key": "pipeline_phase",
                            "tag.2839796150.propagate_at_launch": "true",
                            "tag.2839796150.value": "brewprod",
                            "tag.2851612828.key": "make_dns",
                            "tag.2851612828.propagate_at_launch": "true",
                            "tag.2851612828.value": "es-nginx.aws-brewprod-usw2-jcx-people",
                            "tag.3969529871.key": "service_component",
                            "tag.3969529871.propagate_at_launch": "true",
                            "tag.3969529871.value": "es_nginx",
                            "target_group_arns.#": "0",
                            "termination_policies.#": "0",
                            "vpc_zone_identifier.#": "1",
                            "vpc_zone_identifier.250846469": "subnet-1fd4657b",
                            "wait_for_capacity_timeout": "10m"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_autoscaling_group.nginx_node_asg.2": {
                    "type": "aws_autoscaling_group",
                    "depends_on": [
                        "aws_launch_configuration.nginx_node"
                    ],
                    "primary": {
                        "id": "brewprod-jcxpeople-es-nginx-az3-asg",
                        "attributes": {
                            "arn": "arn:aws:autoscaling:us-west-2:409573287771:autoScalingGroup:4e21e0e4-ef2b-4814-ba58-cb0454cd4882:autoScalingGroupName/brewprod-jcxpeople-es-nginx-az3-asg",
                            "availability_zones.#": "1",
                            "availability_zones.2050015877": "us-west-2c",
                            "default_cooldown": "300",
                            "desired_capacity": "1",
                            "force_delete": "false",
                            "health_check_grace_period": "300",
                            "health_check_type": "EC2",
                            "id": "brewprod-jcxpeople-es-nginx-az3-asg",
                            "launch_configuration": "jcxpeople-nginx-es-node-20170324202616526510312azr",
                            "load_balancers.#": "0",
                            "max_size": "1",
                            "metrics_granularity": "1Minute",
                            "min_size": "1",
                            "name": "brewprod-jcxpeople-es-nginx-az3-asg",
                            "placement_group": "",
                            "protect_from_scale_in": "false",
                            "tag.#": "7",
                            "tag.1089450927.key": "jive_subservice",
                            "tag.1089450927.propagate_at_launch": "true",
                            "tag.1089450927.value": "usw2-jcx-people",
                            "tag.1643727701.key": "Name",
                            "tag.1643727701.propagate_at_launch": "true",
                            "tag.1643727701.value": "brewprod-jcxpeople-es-nginx",
                            "tag.169118653.key": "sla",
                            "tag.169118653.propagate_at_launch": "true",
                            "tag.169118653.value": "prod",
                            "tag.1811987920.key": "jive_service",
                            "tag.1811987920.propagate_at_launch": "true",
                            "tag.1811987920.value": "jcx",
                            "tag.2839796150.key": "pipeline_phase",
                            "tag.2839796150.propagate_at_launch": "true",
                            "tag.2839796150.value": "brewprod",
                            "tag.2851612828.key": "make_dns",
                            "tag.2851612828.propagate_at_launch": "true",
                            "tag.2851612828.value": "es-nginx.aws-brewprod-usw2-jcx-people",
                            "tag.3969529871.key": "service_component",
                            "tag.3969529871.propagate_at_launch": "true",
                            "tag.3969529871.value": "es_nginx",
                            "target_group_arns.#": "0",
                            "termination_policies.#": "0",
                            "vpc_zone_identifier.#": "1",
                            "vpc_zone_identifier.3935488465": "subnet-897b61d0",
                            "wait_for_capacity_timeout": "10m"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_iam_instance_profile.ebs-attach-and-secrets": {
                    "type": "aws_iam_instance_profile",
                    "depends_on": [
                        "aws_iam_role.ebs-attach-and-secrets"
                    ],
                    "primary": {
                        "id": "ebs-attach-and-secrets-profile-usw2-jcx-people",
                        "attributes": {
                            "arn": "arn:aws:iam::409573287771:instance-profile/ebs-attach-and-secrets-profile-usw2-jcx-people",
                            "id": "ebs-attach-and-secrets-profile-usw2-jcx-people",
                            "name": "ebs-attach-and-secrets-profile-usw2-jcx-people",
                            "path": "/",
                            "roles.#": "1",
                            "roles.1934667465": "ebs-attach-and-secrets-role-usw2-jcx-people"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_iam_role.ebs-attach-and-secrets": {
                    "type": "aws_iam_role",
                    "depends_on": [],
                    "primary": {
                        "id": "ebs-attach-and-secrets-role-usw2-jcx-people",
                        "attributes": {
                            "arn": "arn:aws:iam::409573287771:role/ebs-attach-and-secrets-role-usw2-jcx-people",
                            "assume_role_policy": "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": \"sts:AssumeRole\",\n      \"Principal\": {\n        \"Service\": \"ec2.amazonaws.com\"\n      },\n      \"Effect\": \"Allow\",\n      \"Sid\": \"\"\n    }\n  ]\n}\n",
                            "create_date": "2017-03-24T20:26:16Z",
                            "id": "ebs-attach-and-secrets-role-usw2-jcx-people",
                            "name": "ebs-attach-and-secrets-role-usw2-jcx-people",
                            "path": "/",
                            "unique_id": "AROAJCLVA5KGURWM4OB2A"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_iam_role_policy.ebs-attach-and-secrets": {
                    "type": "aws_iam_role_policy",
                    "depends_on": [
                        "aws_iam_role.ebs-attach-and-secrets"
                    ],
                    "primary": {
                        "id": "ebs-attach-and-secrets-role-usw2-jcx-people:ebs-attach-and-secrets-policy-usw2-jcx-people",
                        "attributes": {
                            "id": "ebs-attach-and-secrets-role-usw2-jcx-people:ebs-attach-and-secrets-policy-usw2-jcx-people",
                            "name": "ebs-attach-and-secrets-policy-usw2-jcx-people",
                            "policy": "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n              \"Effect\": \"Allow\",\n              \"Action\": [\n                \"ec2:AttachVolume\",\n                \"ec2:CreateTags\",\n                \"ec2:Describe*\",\n                \"ec2:DetachVolume\",\n                \"elasticache:Describe*\",\n                \"rds:Describe*\",\n                \"route53:ListHostedZones\",\n                \"route53:ListResourceRecordSets\",\n                \"s3:ListAllMyBuckets\"\n              ],\n              \"Resource\": \"*\"\n        },\n        {\n              \"Effect\": \"Allow\",\n              \"Action\": [\n                  \"s3:GetBucketLocation\",\n                  \"s3:GetObject\",\n                  \"s3:ListBucket\"\n              ],\n              \"Resource\": [\n                  \"arn:aws:s3:::us-west-2-jive-data-brewprod-secrets\",\n                  \"arn:aws:s3:::us-west-2-jive-data-brewprod-secrets/*\",\n                  \"arn:aws:s3:::us-west-2-jive-data-pipeline-playbooks\",\n                  \"arn:aws:s3:::us-west-2-jive-data-pipeline-playbooks/*\"\n              ]\n        }\n    ]\n}\n",
                            "role": "ebs-attach-and-secrets-role-usw2-jcx-people"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_launch_configuration.elasticsearch_master": {
                    "type": "aws_launch_configuration",
                    "depends_on": [],
                    "primary": {
                        "id": "jcxpeople-elasticsearch-master-201703242026165099026346rv",
                        "attributes": {
                            "associate_public_ip_address": "false",
                            "ebs_block_device.#": "0",
                            "ebs_optimized": "false",
                            "enable_monitoring": "true",
                            "ephemeral_block_device.#": "0",
                            "iam_instance_profile": "ebs-attach-and-secrets-profile-usw2-jcx-people",
                            "id": "jcxpeople-elasticsearch-master-201703242026165099026346rv",
                            "image_id": "ami-d2c924b2",
                            "instance_type": "r4.large",
                            "key_name": "data-brewprod",
                            "name": "jcxpeople-elasticsearch-master-201703242026165099026346rv",
                            "name_prefix": "jcxpeople-elasticsearch-master-",
                            "root_block_device.#": "0",
                            "security_groups.#": "2",
                            "security_groups.1304606012": "sg-56c54d2e",
                            "security_groups.1461648550": "sg-d9ede6be",
                            "spot_price": "",
                            "user_data": "de58e8f82418504fb1813fe744227c4f99e253b2",
                            "vpc_classic_link_id": "",
                            "vpc_classic_link_security_groups.#": "0"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_launch_configuration.elasticsearch_node": {
                    "type": "aws_launch_configuration",
                    "depends_on": [],
                    "primary": {
                        "id": "jcxpeople-elasticsearch-node-20170324202616518507233lfb",
                        "attributes": {
                            "associate_public_ip_address": "false",
                            "ebs_block_device.#": "1",
                            "ebs_block_device.3817660160.delete_on_termination": "false",
                            "ebs_block_device.3817660160.device_name": "/dev/xvdm",
                            "ebs_block_device.3817660160.encrypted": "true",
                            "ebs_block_device.3817660160.iops": "0",
                            "ebs_block_device.3817660160.snapshot_id": "",
                            "ebs_block_device.3817660160.volume_size": "100",
                            "ebs_block_device.3817660160.volume_type": "gp2",
                            "ebs_optimized": "true",
                            "enable_monitoring": "true",
                            "ephemeral_block_device.#": "0",
                            "iam_instance_profile": "ebs-attach-and-secrets-profile-usw2-jcx-people",
                            "id": "jcxpeople-elasticsearch-node-20170324202616518507233lfb",
                            "image_id": "ami-d2c924b2",
                            "instance_type": "r4.large",
                            "key_name": "data-brewprod",
                            "name": "jcxpeople-elasticsearch-node-20170324202616518507233lfb",
                            "name_prefix": "jcxpeople-elasticsearch-node-",
                            "root_block_device.#": "0",
                            "security_groups.#": "2",
                            "security_groups.1304606012": "sg-56c54d2e",
                            "security_groups.1461648550": "sg-d9ede6be",
                            "spot_price": "",
                            "user_data": "de58e8f82418504fb1813fe744227c4f99e253b2",
                            "vpc_classic_link_id": "",
                            "vpc_classic_link_security_groups.#": "0"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_launch_configuration.kibana_node": {
                    "type": "aws_launch_configuration",
                    "depends_on": [],
                    "primary": {
                        "id": "jcxpeople-kibana-es-node-20170324202616513995452d62",
                        "attributes": {
                            "associate_public_ip_address": "false",
                            "ebs_block_device.#": "0",
                            "ebs_optimized": "false",
                            "enable_monitoring": "true",
                            "ephemeral_block_device.#": "0",
                            "iam_instance_profile": "ebs-attach-and-secrets-profile-usw2-jcx-people",
                            "id": "jcxpeople-kibana-es-node-20170324202616513995452d62",
                            "image_id": "ami-d2c924b2",
                            "instance_type": "t2.small",
                            "key_name": "data-brewprod",
                            "name": "jcxpeople-kibana-es-node-20170324202616513995452d62",
                            "name_prefix": "jcxpeople-kibana-es-node-",
                            "root_block_device.#": "0",
                            "security_groups.#": "2",
                            "security_groups.1461648550": "sg-d9ede6be",
                            "security_groups.859701536": "sg-54c54d2c",
                            "spot_price": "",
                            "user_data": "97b48e38d79aa106b3c07b4ff33b44634a28520d",
                            "vpc_classic_link_id": "",
                            "vpc_classic_link_security_groups.#": "0"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_launch_configuration.nginx_node": {
                    "type": "aws_launch_configuration",
                    "depends_on": [],
                    "primary": {
                        "id": "jcxpeople-nginx-es-node-20170324202616526510312azr",
                        "attributes": {
                            "associate_public_ip_address": "false",
                            "ebs_block_device.#": "0",
                            "ebs_optimized": "false",
                            "enable_monitoring": "true",
                            "ephemeral_block_device.#": "0",
                            "iam_instance_profile": "ebs-attach-and-secrets-profile-usw2-jcx-people",
                            "id": "jcxpeople-nginx-es-node-20170324202616526510312azr",
                            "image_id": "ami-d2c924b2",
                            "instance_type": "t2.small",
                            "key_name": "data-brewprod",
                            "name": "jcxpeople-nginx-es-node-20170324202616526510312azr",
                            "name_prefix": "jcxpeople-nginx-es-node-",
                            "root_block_device.#": "0",
                            "security_groups.#": "2",
                            "security_groups.1461648550": "sg-d9ede6be",
                            "security_groups.187539255": "sg-55c54d2d",
                            "spot_price": "",
                            "user_data": "97b48e38d79aa106b3c07b4ff33b44634a28520d",
                            "vpc_classic_link_id": "",
                            "vpc_classic_link_security_groups.#": "0"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_s3_bucket.snapshot_bucket": {
                    "type": "aws_s3_bucket",
                    "depends_on": [
                        "template_file.s3_policy"
                    ],
                    "primary": {
                        "id": "us-west-2-jive-data-brewprod-usw2-jcx-people-snaps",
                        "attributes": {
                            "acceleration_status": "",
                            "acl": "private",
                            "arn": "arn:aws:s3:::us-west-2-jive-data-brewprod-usw2-jcx-people-snaps",
                            "bucket": "us-west-2-jive-data-brewprod-usw2-jcx-people-snaps",
                            "force_destroy": "false",
                            "hosted_zone_id": "Z3BJ6K6RIION7M",
                            "id": "us-west-2-jive-data-brewprod-usw2-jcx-people-snaps",
                            "policy": "{\"Statement\":[{\"Action\":[\"s3:ListBucket\",\"s3:GetBucketLocation\",\"s3:ListBucketMultipartUploads\",\"s3:ListBucketVersions\"],\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::409573287771:role/ebs-attach-and-secrets-role-usw2-jcx-people\"},\"Resource\":\"arn:aws:s3:::us-west-2-jive-data-brewprod-usw2-jcx-people-snaps\",\"Sid\":\"statement201609211128\"},{\"Action\":[\"s3:GetObject\",\"s3:PutObject\",\"s3:DeleteObject\",\"s3:AbortMultipartUpload\",\"s3:ListMultipartUploadParts\"],\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"arn:aws:iam::409573287771:role/ebs-attach-and-secrets-role-usw2-jcx-people\"},\"Resource\":\"arn:aws:s3:::us-west-2-jive-data-brewprod-usw2-jcx-people-snaps/*\",\"Sid\":\"statement201609211130\"}],\"Version\":\"2012-10-17\"}",
                            "region": "us-west-2",
                            "request_payer": "BucketOwner",
                            "tags.%": "6",
                            "tags.Name": "brewprod-usw2-jcx-people-elasticsearch-snapshots",
                            "tags.jive_service": "jcx",
                            "tags.jive_subservice": "usw2-jcx-people",
                            "tags.pipeline_phase": "brewprod",
                            "tags.service_component": "elasticsearch_snapshots",
                            "tags.sla": "prod",
                            "website.#": "0"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group.elasticsearch_ports": {
                    "type": "aws_security_group",
                    "depends_on": [],
                    "primary": {
                        "id": "sg-56c54d2e",
                        "attributes": {
                            "description": "Allow traffic on ye olde elasticsearch ports",
                            "egress.#": "1",
                            "egress.3563209066.cidr_blocks.#": "1",
                            "egress.3563209066.cidr_blocks.0": "0.0.0.0/8",
                            "egress.3563209066.from_port": "0",
                            "egress.3563209066.prefix_list_ids.#": "0",
                            "egress.3563209066.protocol": "-1",
                            "egress.3563209066.security_groups.#": "0",
                            "egress.3563209066.self": "false",
                            "egress.3563209066.to_port": "0",
                            "id": "sg-56c54d2e",
                            "ingress.#": "2",
                            "ingress.2208294858.cidr_blocks.#": "1",
                            "ingress.2208294858.cidr_blocks.0": "10.0.0.0/8",
                            "ingress.2208294858.from_port": "9300",
                            "ingress.2208294858.protocol": "tcp",
                            "ingress.2208294858.security_groups.#": "0",
                            "ingress.2208294858.self": "false",
                            "ingress.2208294858.to_port": "9300",
                            "ingress.443847884.cidr_blocks.#": "1",
                            "ingress.443847884.cidr_blocks.0": "10.0.0.0/8",
                            "ingress.443847884.from_port": "9200",
                            "ingress.443847884.protocol": "tcp",
                            "ingress.443847884.security_groups.#": "0",
                            "ingress.443847884.self": "false",
                            "ingress.443847884.to_port": "9200",
                            "name": "usw2-jcx-people_elasticsearch_ports",
                            "owner_id": "409573287771",
                            "tags.%": "0",
                            "vpc_id": "vpc-e818b28c"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group.kibana_ports": {
                    "type": "aws_security_group",
                    "depends_on": [],
                    "primary": {
                        "id": "sg-54c54d2c",
                        "attributes": {
                            "description": "Allow traffic on port 5601",
                            "egress.#": "1",
                            "egress.3563209066.cidr_blocks.#": "1",
                            "egress.3563209066.cidr_blocks.0": "0.0.0.0/8",
                            "egress.3563209066.from_port": "0",
                            "egress.3563209066.prefix_list_ids.#": "0",
                            "egress.3563209066.protocol": "-1",
                            "egress.3563209066.security_groups.#": "0",
                            "egress.3563209066.self": "false",
                            "egress.3563209066.to_port": "0",
                            "id": "sg-54c54d2c",
                            "ingress.#": "1",
                            "ingress.3113294704.cidr_blocks.#": "1",
                            "ingress.3113294704.cidr_blocks.0": "10.0.0.0/8",
                            "ingress.3113294704.from_port": "5601",
                            "ingress.3113294704.protocol": "tcp",
                            "ingress.3113294704.security_groups.#": "0",
                            "ingress.3113294704.self": "false",
                            "ingress.3113294704.to_port": "5601",
                            "name": "usw2-jcx-people_kibana_ports",
                            "owner_id": "409573287771",
                            "tags.%": "0",
                            "vpc_id": "vpc-e818b28c"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group.nginx_ports": {
                    "type": "aws_security_group",
                    "depends_on": [],
                    "primary": {
                        "id": "sg-55c54d2d",
                        "attributes": {
                            "description": "Allow traffic on port 443",
                            "egress.#": "1",
                            "egress.3563209066.cidr_blocks.#": "1",
                            "egress.3563209066.cidr_blocks.0": "0.0.0.0/8",
                            "egress.3563209066.from_port": "0",
                            "egress.3563209066.prefix_list_ids.#": "0",
                            "egress.3563209066.protocol": "-1",
                            "egress.3563209066.security_groups.#": "0",
                            "egress.3563209066.self": "false",
                            "egress.3563209066.to_port": "0",
                            "id": "sg-55c54d2d",
                            "ingress.#": "1",
                            "ingress.2216620219.cidr_blocks.#": "1",
                            "ingress.2216620219.cidr_blocks.0": "10.0.0.0/8",
                            "ingress.2216620219.from_port": "443",
                            "ingress.2216620219.protocol": "tcp",
                            "ingress.2216620219.security_groups.#": "0",
                            "ingress.2216620219.self": "false",
                            "ingress.2216620219.to_port": "443",
                            "name": "usw2-jcx-people_nginx_ports",
                            "owner_id": "409573287771",
                            "tags.%": "0",
                            "vpc_id": "vpc-e818b28c"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group_rule.elasticsearch_egress": {
                    "type": "aws_security_group_rule",
                    "depends_on": [
                        "aws_security_group.elasticsearch_ports"
                    ],
                    "primary": {
                        "id": "sgrule-174055109",
                        "attributes": {
                            "cidr_blocks.#": "1",
                            "cidr_blocks.0": "0.0.0.0/8",
                            "from_port": "0",
                            "id": "sgrule-174055109",
                            "prefix_list_ids.#": "0",
                            "protocol": "-1",
                            "security_group_id": "sg-56c54d2e",
                            "self": "false",
                            "to_port": "0",
                            "type": "egress"
                        },
                        "meta": {
                            "schema_version": "2"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group_rule.elasticsearch_port_9200": {
                    "type": "aws_security_group_rule",
                    "depends_on": [
                        "aws_security_group.elasticsearch_ports"
                    ],
                    "primary": {
                        "id": "sgrule-3169458829",
                        "attributes": {
                            "cidr_blocks.#": "1",
                            "cidr_blocks.0": "10.0.0.0/8",
                            "from_port": "9200",
                            "id": "sgrule-3169458829",
                            "prefix_list_ids.#": "0",
                            "protocol": "tcp",
                            "security_group_id": "sg-56c54d2e",
                            "self": "false",
                            "to_port": "9200",
                            "type": "ingress"
                        },
                        "meta": {
                            "schema_version": "2"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group_rule.elasticsearch_port_9300": {
                    "type": "aws_security_group_rule",
                    "depends_on": [
                        "aws_security_group.elasticsearch_ports"
                    ],
                    "primary": {
                        "id": "sgrule-599524454",
                        "attributes": {
                            "cidr_blocks.#": "1",
                            "cidr_blocks.0": "10.0.0.0/8",
                            "from_port": "9300",
                            "id": "sgrule-599524454",
                            "prefix_list_ids.#": "0",
                            "protocol": "tcp",
                            "security_group_id": "sg-56c54d2e",
                            "self": "false",
                            "to_port": "9300",
                            "type": "ingress"
                        },
                        "meta": {
                            "schema_version": "2"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group_rule.kibana_egress": {
                    "type": "aws_security_group_rule",
                    "depends_on": [
                        "aws_security_group.kibana_ports"
                    ],
                    "primary": {
                        "id": "sgrule-2076540543",
                        "attributes": {
                            "cidr_blocks.#": "1",
                            "cidr_blocks.0": "0.0.0.0/8",
                            "from_port": "0",
                            "id": "sgrule-2076540543",
                            "prefix_list_ids.#": "0",
                            "protocol": "-1",
                            "security_group_id": "sg-54c54d2c",
                            "self": "false",
                            "to_port": "0",
                            "type": "egress"
                        },
                        "meta": {
                            "schema_version": "2"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group_rule.kibana_port_5601": {
                    "type": "aws_security_group_rule",
                    "depends_on": [
                        "aws_security_group.kibana_ports"
                    ],
                    "primary": {
                        "id": "sgrule-1309973277",
                        "attributes": {
                            "cidr_blocks.#": "1",
                            "cidr_blocks.0": "10.0.0.0/8",
                            "from_port": "5601",
                            "id": "sgrule-1309973277",
                            "prefix_list_ids.#": "0",
                            "protocol": "tcp",
                            "security_group_id": "sg-54c54d2c",
                            "self": "false",
                            "to_port": "5601",
                            "type": "ingress"
                        },
                        "meta": {
                            "schema_version": "2"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group_rule.nginx_egress": {
                    "type": "aws_security_group_rule",
                    "depends_on": [
                        "aws_security_group.nginx_ports"
                    ],
                    "primary": {
                        "id": "sgrule-531217698",
                        "attributes": {
                            "cidr_blocks.#": "1",
                            "cidr_blocks.0": "0.0.0.0/8",
                            "from_port": "0",
                            "id": "sgrule-531217698",
                            "prefix_list_ids.#": "0",
                            "protocol": "-1",
                            "security_group_id": "sg-55c54d2d",
                            "self": "false",
                            "to_port": "0",
                            "type": "egress"
                        },
                        "meta": {
                            "schema_version": "2"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "aws_security_group_rule.nginx_port_443": {
                    "type": "aws_security_group_rule",
                    "depends_on": [
                        "aws_security_group.nginx_ports"
                    ],
                    "primary": {
                        "id": "sgrule-1684790915",
                        "attributes": {
                            "cidr_blocks.#": "1",
                            "cidr_blocks.0": "10.0.0.0/8",
                            "from_port": "443",
                            "id": "sgrule-1684790915",
                            "prefix_list_ids.#": "0",
                            "protocol": "tcp",
                            "security_group_id": "sg-55c54d2d",
                            "self": "false",
                            "to_port": "443",
                            "type": "ingress"
                        },
                        "meta": {
                            "schema_version": "2"
                        },
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "template_file.elasticsearch_user_data": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "a4160c259b224e4845c749b0d01e86d801c48b9eb6946fa9114cdc8ff4300bf3",
                        "attributes": {
                            "id": "a4160c259b224e4845c749b0d01e86d801c48b9eb6946fa9114cdc8ff4300bf3",
                            "rendered": "#!/bin/bash\n\n# SKIP_EBS_REATTACH - Set to a non-empty string to skip reattaching of any\n#                     unattached matching EBS volumes.\n#                     The ebs_attach script will still run and new volumes\n#                     will be attached/formatted as necessary\n# ADDITIONAL_BUNDLE_NAME - The name of a nexus bundle to download and unpack.\n#                          Leave blank to skip.\n#                          Must contain a script for setting up/calling ansible\n#                          located at/called:\n#           ./ansible/bin/call_ansible.sh\n#\nSKIP_EBS_REATTACH=\nADDITIONAL_BUNDLE_NAME=ansible-playbooks-aws\nADDITIONAL_BUNDLE_VERSION=2.6.2\nINSTALL_DIRECTORY=/opt/ansible\ndeclare -r instance_type=$(curl -s http://169.254.169.254/latest/meta-data/instance-type)\n\nfunction_prep() {\n    mkdir -p ${INSTALL_DIRECTORY}\n    # Get pip for awscli\n    yum install -y epel-release\n    yum install -y python-pip\n    pip install awscli\n}\n\nfunction_instance_store() {\n  yum install -y cryptsetup\n  passphrase=$(\u003c /dev/urandom tr -dc '_A-Za-z0-9@#%^_\\\\-\\\\=+' | head -c 256 | xargs -0 echo)\n\n  if [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\n  then\n    # this /could/ be a bit more flexible *wink!*\n\n    # set up disk encryption\n    echo \"cryptsetup luksFormat /dev/nvme0n1\"\n    echo $passphrase | cryptsetup luksFormat /dev/nvme0n1\n    UUID=$(cryptsetup luksUUID /dev/nvme0n1)\n    echo \"cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\"\n    echo \"$passphrase\" | cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\n    echo \"mkfs.ext4 /dev/mapper/elasticsearch_data\"\n    mkfs.ext4 /dev/mapper/elasticsearch_data\n    echo \"mount /dev/mapper/elasticsearch_data /data\"\n    mkdir -p /data\n    mount /dev/mapper/elasticsearch_data /data\n\n    # encrypt and save the volume's password\n    echo \"aws --region us-west-2 kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext xxxxxx --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\"\n    aws --region us-west-2 kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext \"${passphrase}\" --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\n    unset passphrase\n\n    cat \u003c\u003c-EOM \u003e /etc/init.d/luks-mount\n#!/bin/bash\n# A quickly hacked together script to remount a luks volume at boot\n\n# Get the passphrase from KMS using the ciphertext\npassphrase=\\$(aws --region us-west-2 kms decrypt --ciphertext-blob fileb:///etc/.luks --output text --query Plaintext | base64 -d)\n\n# Open the LUKS volume\necho \"\\$passphrase\" | cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\n\n# Mount the volume\nmount /dev/mapper/elasticsearch_data /data\nEOM\n\n    chmod 755 /etc/init.d/luks-mount\n\n    # will this work for Centos 7? No it will not.\n    #ln -s /etc/init.d/luks-mount /etc/rc3.d/S15luks\n\n    # so instead, create a systemd file:\n    cat \u003c\u003c-EOM \u003e /usr/lib/systemd/system/data_remount.service\n[Unit]\nDescription=Mount the ephemeral data volume\nDocumentation=\nBefore=elasticsearch.service\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=oneshot\nExecStart=/etc/init.d/luks-mount\n\n[Install]\nWantedBy=multi-user.target\nEOM\n\n    systemctl daemon-reload\n    systemctl enable data_remount\n\n  fi\n}\n\nfunction_ebs_attach() {\n    cat \u003c\u003c'EOF' \u003e ${INSTALL_DIRECTORY}/ebs_mount.sh\n#!/bin/bash -v\n#\n# Usage:\n# ./ebs_mount.sh -d \u003cdevice:mountpoint\u003e[,\u003cdevice:mountpoint\u003e...]\n#\n# Example:\n# ./ebs_mount.sh -d /dev/xvdm:/data/elasticsearch,/dev/xvdn:/data/more_data\n#\ndeclare -r instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)\ndeclare -r avail_zone=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)\n\nregion=\"us-west-2\"\n\nwhile getopts \"d:n:p:\" opt; do\n  case \"$opt\" in\n  d) devices=$OPTARG\n     ;;\n  esac\ndone\n\nif [ -z $name ]\nthen\n  name=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`Name`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\nfi\n\npipeline_phase=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`pipeline_phase`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_service=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_service`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_subservice=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_subservice`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\n\necho \"Pipeline_phase: ${pipeline_phase}\"\necho \"Jive_service: ${jive_service}\"\necho \"Jive_subservice: ${jive_subservice}\"\n\nOLD_IFS=$IFS\nIFS=','\nfor dev_mp_pair in $devices\ndo\n  # I have no idea what I'm doing\n  IFS=':' read -ra PAIR \u003c\u003c\u003c \"$dev_mp_pair\"\n  IFS=','\n  device=${PAIR[0]}\n  mountp=${PAIR[1]}\n  echo \"Device: ${device}\"\n  if [ -z $device ]\n  then\n    echo \"[ERROR] Did you specify a device name?\"\n    continue\n  fi\n\n  echo \"MountP: ${mountp}\"\n  if [ -z $mountp ]\n  then\n    echo \"[ERROR] Did you specify a mount point?\"\n    continue\n  fi\n\n  mkdir -p ${mountp}\n\n  if [ -z ${SKIP_EBS_REATTACH} ]\n  then\n    # Search for existing tagged EBS volume (in current AZ)\n    echo \"aws ec2 describe-volumes --region=${region} --filters Name=availability-zone,Values=${avail_zone} Name=tag:pipeline_phase,Values=${pipeline_phase} Name=tag:jive_service,Values=${jive_service} Name=tag:jive_subservice,Values=${jive_subservice} Name=status,Values=available Name=tag:device,Values=${device} Name=tag:Name,Values=${name} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n    previous_volume=$(aws ec2 describe-volumes --region=${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=available Name=tag:device,Values=${device} Name=tag:Name,Values=${name} Name=tag:pipeline_phase,Values=${pipeline_phase} Name=tag:jive_service,Values=${jive_service} Name=tag:jive_subservice,Values=${jive_subservice} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n    echo \"Previous volume: ${previous_volume}\"\n  else\n    previous_volume=\"\"\n    echo \"SKIP_EBS_REATTACH is set, not attempting to reattach old volume(s)\"\n  fi\n\n  # find current volume id\n  echo \"aws ec2 describe-volumes --region ${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=${instance_id} Name=attachment.device,Values=${device} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n  current_volume=$(aws ec2 describe-volumes --region ${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=${instance_id} Name=attachment.device,Values=${device} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n\n  if [ $? -ne 0 ]\n  then\n    echo \"[ERROR] Failed to get current volume ID for ${device}\"\n    continue\n  fi\n\n  echo \"Current volume: ${current_volume}\"\n\n  if [ ! -z $previous_volume ]\n  then\n\n    # detach current EBS\n    echo \"detaching current volume: ${current_volume}\"\n    aws ec2 detach-volume --region ${region} --volume-id ${current_volume}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to detach current volume: ${current_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to detach\n    echo \"sleeping for 120 to allow aws time to get its ducks in a row\"\n    sleep 120\n\n    # attach existing EBS\n    aws ec2 attach-volume --region ${region} --volume-id ${previous_volume} --instance-id ${instance_id} --device ${device}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to attach previous volume: ${previous_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to attach\n    echo \"sleeping for 120 to allow aws time to get its ducks in a row again\"\n    sleep 120\n\n    current_volume=${previous_volume}\n\n  else\n    # no previous volume found. assume tabula rasa\n    echo \"No previous volume found. Proceeding...\"\n    echo \"mkfs -t ext4 ${device}\"\n    mkfs -t ext4 ${device}\n  fi\n  \n  echo \"mount ${device} ${mountp}\"\n  mount ${device} ${mountp}\n  echo \"${device} ${mountp} ext4 defaults,nofail 0 2\" \u003e\u003e /etc/fstab\n\n  # add tags to the volume?\n  echo \"aws ec2 create-tags --region ${region} --resources ${current_volume} --tags Key=Name,Value=\\\"${name}\\\" Key=device,Value=${device} Key=pipeline_phase,Value=${pipeline_phase} Key=jive_service,Value=${jive_service} Key=jive_subservice,Value=${jive_subservice}\"\n  aws ec2 create-tags --region ${region} --resources ${current_volume} --tags Key=Name,Value=\"${name}\" Key=device,Value=${device} Key=pipeline_phase,Value=${pipeline_phase} Key=jive_service,Value=${jive_service} Key=jive_subservice,Value=${jive_subservice}\n\ndone\nIFS=$OLD_IFS\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x ${INSTALL_DIRECTORY}/ebs_mount.sh\n    ${INSTALL_DIRECTORY}/ebs_mount.sh -d /dev/xvdm:/data 2\u003e\u00261 \u003e\u003e ${INSTALL_DIRECTORY}/ebs_mount.log\n}\n\nfunction_nexus() {\n    # URL redirect fails without this entry\n    echo \"10.10.100.155 nexus-int.eng.jiveland.com\" \u003e\u003e /etc/hosts\n\n    # Script to download Ansible artifact from Nexus\n    cat \u003c\u003c'EOF' \u003e ${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n#!/bin/bash\n# Argument = -h -v -i groupId:artifactId:version -c classifier -p packaging -r repository\n\n# Define Nexus Configuration\nNEXUS_BASE=nexus-int.eng.jiveland.com\nREST_PATH=/service/local\nART_REDIR=/artifact/maven/redirect\n\n# Read in Complete Set of Coordinates from the Command Line\nGROUP_ID=\nARTIFACT_ID=\nVERSION=\"LATEST\"\nCLASSIFIER=\"\"\nPACKAGING=tar.gz\nREPO=\"candidates\"\nVERBOSE=0\n\nwhile getopts \"hvi:c:p:\" OPTION\ndo\n     case $OPTION in\n         h)\n             usage\n             exit 1\n             ;;\n         i)\n\t     OIFS=$IFS\n             IFS=\":\"\n\t     GAV_COORD=( $OPTARG )\n\t     GROUP_ID=${GAV_COORD[0]}\n             ARTIFACT_ID=${GAV_COORD[1]}\n             VERSION=${GAV_COORD[2]}\n\t     IFS=$OIFS\n             ;;\n         c)\n             CLASSIFIER=$OPTARG\n             ;;\n         p)\n             PACKAGING=$OPTARG\n             ;;\n         v)\n             VERBOSE=1\n             ;;\n         ?)\n             usage\n             exit\n             ;;\n     esac\ndone\n\nif [[ -z $GROUP_ID ]] || [[ -z $ARTIFACT_ID ]] || [[ -z $VERSION ]]\nthen\n     echo \"BAD ARGUMENTS: Either groupId, artifactId, or version was not supplied\" \u003e\u00262\n     usage\n     exit 1\nfi\n\n# Construct the base URL\nREDIRECT_URL=${NEXUS_BASE}${REST_PATH}${ART_REDIR}\n\n# Generate the list of parameters\nPARAM_KEYS=( g a v r p c )\nPARAM_VALUES=( $GROUP_ID $ARTIFACT_ID $VERSION $REPO $PACKAGING $CLASSIFIER )\nPARAMS=\"\"\nfor index in ${!PARAM_KEYS[*]}\ndo\n  if [[ ${PARAM_VALUES[$index]} != \"\" ]]\n  then\n    PARAMS=\"${PARAMS}${PARAM_KEYS[$index]}=${PARAM_VALUES[$index]}\u0026\"\n  fi\ndone\n\nREDIRECT_URL=\"${REDIRECT_URL}?${PARAMS}\"\n\necho \"Fetching Artifact from $REDIRECT_URL...\" \u003e\u00262\ncurl -sS -L ${REDIRECT_URL}\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x ${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n    ${INSTALL_DIRECTORY}/get_nexus_artifact.sh -i com.jivesoftware.techops:ansible-common:LATEST \u003e ${INSTALL_DIRECTORY}/ansible-common.tar.gz\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      aws configure set s3.signature_version s3v4\n      aws s3 cp s3://us-west-2-jive-data-pipeline-playbooks/${ADDITIONAL_BUNDLE_NAME}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz\n    fi\n}\n\nfunction_ansible() {\n    # Need Sudo TTY\n    sed -i s/'Defaults    requiretty'/'#Defaults    requiretty'/ /etc/sudoers\n    # Disable SELINUX for SSSD\n    sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config\n    setenforce 0\n    # Create Ansible working directories\n    mkdir -p ${INSTALL_DIRECTORY}/ansible-common\n    tar xf ${INSTALL_DIRECTORY}/ansible-common.tar.gz -C ${INSTALL_DIRECTORY}/ansible-common/\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      mkdir -p ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}\n      tar xf ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz -C ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}\n    fi\n    # Set Python to 2.6 and run Ansible locally\n    alternatives --set python /usr/bin/python2.6\n    yum install -y yum-python26 python-boto ansible\n\n    # Script to run Ansible locally\n    cat \u003c\u003cEOF \u003e ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n#!/bin/bash\n\nansible-playbook -i localhost ${INSTALL_DIRECTORY}/ansible-common/playbook-generic-node.yml --connection=local\nif [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\nthen\n  ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}/bin/call_ansible.sh\nfi\nEOF\n    # Run Ansible\n    chmod +x ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n    ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh \u003e\u003e ${INSTALL_DIRECTORY}/ansible-common/ansible_debug.log\n}\n\nfunction_restart() {\n    # Need to restart for SELINUX change.\n    shutdown -r now\n}\n\n# Run the things\nfunction_prep\nif [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\nthen\n  function_instance_store\nelse\n  function_ebs_attach\nfi\nfunction_nexus\nfunction_ansible\n#function_restart\n",
                            "template": "#!/bin/bash\n\n# SKIP_EBS_REATTACH - Set to a non-empty string to skip reattaching of any\n#                     unattached matching EBS volumes.\n#                     The ebs_attach script will still run and new volumes\n#                     will be attached/formatted as necessary\n# ADDITIONAL_BUNDLE_NAME - The name of a nexus bundle to download and unpack.\n#                          Leave blank to skip.\n#                          Must contain a script for setting up/calling ansible\n#                          located at/called:\n#           ./ansible/bin/call_ansible.sh\n#\nSKIP_EBS_REATTACH=${skip_ebs_reattach}\nADDITIONAL_BUNDLE_NAME=${bundle_name}\nADDITIONAL_BUNDLE_VERSION=${bundle_version}\nINSTALL_DIRECTORY=/opt/ansible\ndeclare -r instance_type=$(curl -s http://169.254.169.254/latest/meta-data/instance-type)\n\nfunction_prep() {\n    mkdir -p $${INSTALL_DIRECTORY}\n    # Get pip for awscli\n    yum install -y epel-release\n    yum install -y python-pip\n    pip install awscli\n}\n\nfunction_instance_store() {\n  yum install -y cryptsetup\n  passphrase=$(\u003c /dev/urandom tr -dc '_A-Za-z0-9@#%^_\\\\-\\\\=+' | head -c 256 | xargs -0 echo)\n\n  if [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\n  then\n    # this /could/ be a bit more flexible *wink!*\n\n    # set up disk encryption\n    echo \"cryptsetup luksFormat /dev/nvme0n1\"\n    echo $passphrase | cryptsetup luksFormat /dev/nvme0n1\n    UUID=$(cryptsetup luksUUID /dev/nvme0n1)\n    echo \"cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\"\n    echo \"$passphrase\" | cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\n    echo \"mkfs.ext4 /dev/mapper/elasticsearch_data\"\n    mkfs.ext4 /dev/mapper/elasticsearch_data\n    echo \"mount /dev/mapper/elasticsearch_data /data\"\n    mkdir -p /data\n    mount /dev/mapper/elasticsearch_data /data\n\n    # encrypt and save the volume's password\n    echo \"aws --region ${region} kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext xxxxxx --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\"\n    aws --region ${region} kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext \"$${passphrase}\" --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\n    unset passphrase\n\n    cat \u003c\u003c-EOM \u003e /etc/init.d/luks-mount\n#!/bin/bash\n# A quickly hacked together script to remount a luks volume at boot\n\n# Get the passphrase from KMS using the ciphertext\npassphrase=\\$(aws --region ${region} kms decrypt --ciphertext-blob fileb:///etc/.luks --output text --query Plaintext | base64 -d)\n\n# Open the LUKS volume\necho \"\\$passphrase\" | cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\n\n# Mount the volume\nmount /dev/mapper/elasticsearch_data /data\nEOM\n\n    chmod 755 /etc/init.d/luks-mount\n\n    # will this work for Centos 7? No it will not.\n    #ln -s /etc/init.d/luks-mount /etc/rc3.d/S15luks\n\n    # so instead, create a systemd file:\n    cat \u003c\u003c-EOM \u003e /usr/lib/systemd/system/data_remount.service\n[Unit]\nDescription=Mount the ephemeral data volume\nDocumentation=\nBefore=elasticsearch.service\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=oneshot\nExecStart=/etc/init.d/luks-mount\n\n[Install]\nWantedBy=multi-user.target\nEOM\n\n    systemctl daemon-reload\n    systemctl enable data_remount\n\n  fi\n}\n\nfunction_ebs_attach() {\n    cat \u003c\u003c'EOF' \u003e $${INSTALL_DIRECTORY}/ebs_mount.sh\n#!/bin/bash -v\n#\n# Usage:\n# ./ebs_mount.sh -d \u003cdevice:mountpoint\u003e[,\u003cdevice:mountpoint\u003e...]\n#\n# Example:\n# ./ebs_mount.sh -d /dev/xvdm:/data/elasticsearch,/dev/xvdn:/data/more_data\n#\ndeclare -r instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)\ndeclare -r avail_zone=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)\n\nregion=\"${region}\"\n\nwhile getopts \"d:n:p:\" opt; do\n  case \"$opt\" in\n  d) devices=$OPTARG\n     ;;\n  esac\ndone\n\nif [ -z $name ]\nthen\n  name=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`Name`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\nfi\n\npipeline_phase=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`pipeline_phase`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_service=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_service`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_subservice=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_subservice`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\n\necho \"Pipeline_phase: $${pipeline_phase}\"\necho \"Jive_service: $${jive_service}\"\necho \"Jive_subservice: $${jive_subservice}\"\n\nOLD_IFS=$IFS\nIFS=','\nfor dev_mp_pair in $devices\ndo\n  # I have no idea what I'm doing\n  IFS=':' read -ra PAIR \u003c\u003c\u003c \"$dev_mp_pair\"\n  IFS=','\n  device=$${PAIR[0]}\n  mountp=$${PAIR[1]}\n  echo \"Device: $${device}\"\n  if [ -z $device ]\n  then\n    echo \"[ERROR] Did you specify a device name?\"\n    continue\n  fi\n\n  echo \"MountP: $${mountp}\"\n  if [ -z $mountp ]\n  then\n    echo \"[ERROR] Did you specify a mount point?\"\n    continue\n  fi\n\n  mkdir -p $${mountp}\n\n  if [ -z $${SKIP_EBS_REATTACH} ]\n  then\n    # Search for existing tagged EBS volume (in current AZ)\n    echo \"aws ec2 describe-volumes --region=$${region} --filters Name=availability-zone,Values=$${avail_zone} Name=tag:pipeline_phase,Values=$${pipeline_phase} Name=tag:jive_service,Values=$${jive_service} Name=tag:jive_subservice,Values=$${jive_subservice} Name=status,Values=available Name=tag:device,Values=$${device} Name=tag:Name,Values=$${name} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n    previous_volume=$(aws ec2 describe-volumes --region=$${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=available Name=tag:device,Values=$${device} Name=tag:Name,Values=$${name} Name=tag:pipeline_phase,Values=$${pipeline_phase} Name=tag:jive_service,Values=$${jive_service} Name=tag:jive_subservice,Values=$${jive_subservice} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n    echo \"Previous volume: $${previous_volume}\"\n  else\n    previous_volume=\"\"\n    echo \"SKIP_EBS_REATTACH is set, not attempting to reattach old volume(s)\"\n  fi\n\n  # find current volume id\n  echo \"aws ec2 describe-volumes --region $${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=$${instance_id} Name=attachment.device,Values=$${device} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n  current_volume=$(aws ec2 describe-volumes --region $${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=$${instance_id} Name=attachment.device,Values=$${device} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n\n  if [ $? -ne 0 ]\n  then\n    echo \"[ERROR] Failed to get current volume ID for $${device}\"\n    continue\n  fi\n\n  echo \"Current volume: $${current_volume}\"\n\n  if [ ! -z $previous_volume ]\n  then\n\n    # detach current EBS\n    echo \"detaching current volume: $${current_volume}\"\n    aws ec2 detach-volume --region $${region} --volume-id $${current_volume}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to detach current volume: $${current_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to detach\n    echo \"sleeping for 120 to allow aws time to get its ducks in a row\"\n    sleep 120\n\n    # attach existing EBS\n    aws ec2 attach-volume --region $${region} --volume-id $${previous_volume} --instance-id $${instance_id} --device $${device}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to attach previous volume: $${previous_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to attach\n    echo \"sleeping for 120 to allow aws time to get its ducks in a row again\"\n    sleep 120\n\n    current_volume=$${previous_volume}\n\n  else\n    # no previous volume found. assume tabula rasa\n    echo \"No previous volume found. Proceeding...\"\n    echo \"mkfs -t ext4 $${device}\"\n    mkfs -t ext4 $${device}\n  fi\n  \n  echo \"mount $${device} $${mountp}\"\n  mount $${device} $${mountp}\n  echo \"$${device} $${mountp} ext4 defaults,nofail 0 2\" \u003e\u003e /etc/fstab\n\n  # add tags to the volume?\n  echo \"aws ec2 create-tags --region $${region} --resources $${current_volume} --tags Key=Name,Value=\\\"$${name}\\\" Key=device,Value=$${device} Key=pipeline_phase,Value=$${pipeline_phase} Key=jive_service,Value=$${jive_service} Key=jive_subservice,Value=$${jive_subservice}\"\n  aws ec2 create-tags --region $${region} --resources $${current_volume} --tags Key=Name,Value=\"$${name}\" Key=device,Value=$${device} Key=pipeline_phase,Value=$${pipeline_phase} Key=jive_service,Value=$${jive_service} Key=jive_subservice,Value=$${jive_subservice}\n\ndone\nIFS=$OLD_IFS\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x $${INSTALL_DIRECTORY}/ebs_mount.sh\n    $${INSTALL_DIRECTORY}/ebs_mount.sh -d ${devices} 2\u003e\u00261 \u003e\u003e $${INSTALL_DIRECTORY}/ebs_mount.log\n}\n\nfunction_nexus() {\n    # URL redirect fails without this entry\n    echo \"10.10.100.155 nexus-int.eng.jiveland.com\" \u003e\u003e /etc/hosts\n\n    # Script to download Ansible artifact from Nexus\n    cat \u003c\u003c'EOF' \u003e $${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n#!/bin/bash\n# Argument = -h -v -i groupId:artifactId:version -c classifier -p packaging -r repository\n\n# Define Nexus Configuration\nNEXUS_BASE=nexus-int.eng.jiveland.com\nREST_PATH=/service/local\nART_REDIR=/artifact/maven/redirect\n\n# Read in Complete Set of Coordinates from the Command Line\nGROUP_ID=\nARTIFACT_ID=\nVERSION=\"LATEST\"\nCLASSIFIER=\"\"\nPACKAGING=tar.gz\nREPO=\"candidates\"\nVERBOSE=0\n\nwhile getopts \"hvi:c:p:\" OPTION\ndo\n     case $OPTION in\n         h)\n             usage\n             exit 1\n             ;;\n         i)\n\t     OIFS=$IFS\n             IFS=\":\"\n\t     GAV_COORD=( $OPTARG )\n\t     GROUP_ID=$${GAV_COORD[0]}\n             ARTIFACT_ID=$${GAV_COORD[1]}\n             VERSION=$${GAV_COORD[2]}\n\t     IFS=$OIFS\n             ;;\n         c)\n             CLASSIFIER=$OPTARG\n             ;;\n         p)\n             PACKAGING=$OPTARG\n             ;;\n         v)\n             VERBOSE=1\n             ;;\n         ?)\n             usage\n             exit\n             ;;\n     esac\ndone\n\nif [[ -z $GROUP_ID ]] || [[ -z $ARTIFACT_ID ]] || [[ -z $VERSION ]]\nthen\n     echo \"BAD ARGUMENTS: Either groupId, artifactId, or version was not supplied\" \u003e\u00262\n     usage\n     exit 1\nfi\n\n# Construct the base URL\nREDIRECT_URL=$${NEXUS_BASE}$${REST_PATH}$${ART_REDIR}\n\n# Generate the list of parameters\nPARAM_KEYS=( g a v r p c )\nPARAM_VALUES=( $GROUP_ID $ARTIFACT_ID $VERSION $REPO $PACKAGING $CLASSIFIER )\nPARAMS=\"\"\nfor index in $${!PARAM_KEYS[*]}\ndo\n  if [[ $${PARAM_VALUES[$index]} != \"\" ]]\n  then\n    PARAMS=\"$${PARAMS}$${PARAM_KEYS[$index]}=$${PARAM_VALUES[$index]}\u0026\"\n  fi\ndone\n\nREDIRECT_URL=\"$${REDIRECT_URL}?$${PARAMS}\"\n\necho \"Fetching Artifact from $REDIRECT_URL...\" \u003e\u00262\ncurl -sS -L $${REDIRECT_URL}\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x $${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n    $${INSTALL_DIRECTORY}/get_nexus_artifact.sh -i com.jivesoftware.techops:ansible-common:LATEST \u003e $${INSTALL_DIRECTORY}/ansible-common.tar.gz\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      aws configure set s3.signature_version s3v4\n      aws s3 cp s3://us-west-2-jive-data-pipeline-playbooks/$${ADDITIONAL_BUNDLE_NAME}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz\n    fi\n}\n\nfunction_ansible() {\n    # Need Sudo TTY\n    sed -i s/'Defaults    requiretty'/'#Defaults    requiretty'/ /etc/sudoers\n    # Disable SELINUX for SSSD\n    sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config\n    setenforce 0\n    # Create Ansible working directories\n    mkdir -p $${INSTALL_DIRECTORY}/ansible-common\n    tar xf $${INSTALL_DIRECTORY}/ansible-common.tar.gz -C $${INSTALL_DIRECTORY}/ansible-common/\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      mkdir -p $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}\n      tar xf $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz -C $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}\n    fi\n    # Set Python to 2.6 and run Ansible locally\n    alternatives --set python /usr/bin/python2.6\n    yum install -y yum-python26 python-boto ansible\n\n    # Script to run Ansible locally\n    cat \u003c\u003cEOF \u003e $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n#!/bin/bash\n\nansible-playbook -i localhost $${INSTALL_DIRECTORY}/ansible-common/playbook-generic-node.yml --connection=local\nif [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\nthen\n  $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}/bin/call_ansible.sh\nfi\nEOF\n    # Run Ansible\n    chmod +x $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n    $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh \u003e\u003e $${INSTALL_DIRECTORY}/ansible-common/ansible_debug.log\n}\n\nfunction_restart() {\n    # Need to restart for SELINUX change.\n    shutdown -r now\n}\n\n# Run the things\nfunction_prep\nif [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\nthen\n  function_instance_store\nelse\n  function_ebs_attach\nfi\nfunction_nexus\nfunction_ansible\n#function_restart\n",
                            "vars.%": "7",
                            "vars.account_name": "jive-data-brewprod",
                            "vars.bundle_name": "ansible-playbooks-aws",
                            "vars.bundle_version": "2.6.2",
                            "vars.devices": "/dev/xvdm:/data",
                            "vars.pipeline_phase": "brewprod",
                            "vars.region": "us-west-2",
                            "vars.skip_ebs_reattach": ""
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "template_file.kibana_user_data": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "09b9cef501ddab3bfb4158e24e2a96c89fe9fb0818613e6b53249fdfea844134",
                        "attributes": {
                            "id": "09b9cef501ddab3bfb4158e24e2a96c89fe9fb0818613e6b53249fdfea844134",
                            "rendered": "#!/bin/bash\n\n# SKIP_EBS_REATTACH - Set to a non-empty string to skip reattaching of any\n#                     unattached matching EBS volumes.\n#                     The ebs_attach script will still run and new volumes\n#                     will be attached/formatted as necessary\n# ADDITIONAL_BUNDLE_NAME - The name of a nexus bundle to download and unpack.\n#                          Leave blank to skip.\n#                          Must contain a script for setting up/calling ansible\n#                          located at/called:\n#           ./ansible/bin/call_ansible.sh\n#\nSKIP_EBS_REATTACH=true\nADDITIONAL_BUNDLE_NAME=ansible-playbooks-aws\nADDITIONAL_BUNDLE_VERSION=2.6.2\nINSTALL_DIRECTORY=/opt/ansible\ndeclare -r instance_type=$(curl -s http://169.254.169.254/latest/meta-data/instance-type)\n\nfunction_prep() {\n    mkdir -p ${INSTALL_DIRECTORY}\n    # Get pip for awscli\n    yum install -y epel-release\n    yum install -y python-pip\n    pip install awscli\n}\n\nfunction_instance_store() {\n  yum install -y cryptsetup\n  passphrase=$(\u003c /dev/urandom tr -dc '_A-Za-z0-9@#%^_\\\\-\\\\=+' | head -c 256 | xargs -0 echo)\n\n  if [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\n  then\n    # this /could/ be a bit more flexible *wink!*\n\n    # set up disk encryption\n    echo \"cryptsetup luksFormat /dev/nvme0n1\"\n    echo $passphrase | cryptsetup luksFormat /dev/nvme0n1\n    UUID=$(cryptsetup luksUUID /dev/nvme0n1)\n    echo \"cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\"\n    echo \"$passphrase\" | cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\n    echo \"mkfs.ext4 /dev/mapper/elasticsearch_data\"\n    mkfs.ext4 /dev/mapper/elasticsearch_data\n    echo \"mount /dev/mapper/elasticsearch_data /data\"\n    mkdir -p /data\n    mount /dev/mapper/elasticsearch_data /data\n\n    # encrypt and save the volume's password\n    echo \"aws --region us-west-2 kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext xxxxxx --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\"\n    aws --region us-west-2 kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext \"${passphrase}\" --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\n    unset passphrase\n\n    cat \u003c\u003c-EOM \u003e /etc/init.d/luks-mount\n#!/bin/bash\n# A quickly hacked together script to remount a luks volume at boot\n\n# Get the passphrase from KMS using the ciphertext\npassphrase=\\$(aws --region us-west-2 kms decrypt --ciphertext-blob fileb:///etc/.luks --output text --query Plaintext | base64 -d)\n\n# Open the LUKS volume\necho \"\\$passphrase\" | cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\n\n# Mount the volume\nmount /dev/mapper/elasticsearch_data /data\nEOM\n\n    chmod 755 /etc/init.d/luks-mount\n\n    # will this work for Centos 7? No it will not.\n    #ln -s /etc/init.d/luks-mount /etc/rc3.d/S15luks\n\n    # so instead, create a systemd file:\n    cat \u003c\u003c-EOM \u003e /usr/lib/systemd/system/data_remount.service\n[Unit]\nDescription=Mount the ephemeral data volume\nDocumentation=\nBefore=elasticsearch.service\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=oneshot\nExecStart=/etc/init.d/luks-mount\n\n[Install]\nWantedBy=multi-user.target\nEOM\n\n    systemctl daemon-reload\n    systemctl enable data_remount\n\n  fi\n}\n\nfunction_ebs_attach() {\n    cat \u003c\u003c'EOF' \u003e ${INSTALL_DIRECTORY}/ebs_mount.sh\n#!/bin/bash -v\n#\n# Usage:\n# ./ebs_mount.sh -d \u003cdevice:mountpoint\u003e[,\u003cdevice:mountpoint\u003e...]\n#\n# Example:\n# ./ebs_mount.sh -d /dev/xvdm:/data/elasticsearch,/dev/xvdn:/data/more_data\n#\ndeclare -r instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)\ndeclare -r avail_zone=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)\n\nregion=\"us-west-2\"\n\nwhile getopts \"d:n:p:\" opt; do\n  case \"$opt\" in\n  d) devices=$OPTARG\n     ;;\n  esac\ndone\n\nif [ -z $name ]\nthen\n  name=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`Name`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\nfi\n\npipeline_phase=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`pipeline_phase`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_service=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_service`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_subservice=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_subservice`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\n\necho \"Pipeline_phase: ${pipeline_phase}\"\necho \"Jive_service: ${jive_service}\"\necho \"Jive_subservice: ${jive_subservice}\"\n\nOLD_IFS=$IFS\nIFS=','\nfor dev_mp_pair in $devices\ndo\n  # I have no idea what I'm doing\n  IFS=':' read -ra PAIR \u003c\u003c\u003c \"$dev_mp_pair\"\n  IFS=','\n  device=${PAIR[0]}\n  mountp=${PAIR[1]}\n  echo \"Device: ${device}\"\n  if [ -z $device ]\n  then\n    echo \"[ERROR] Did you specify a device name?\"\n    continue\n  fi\n\n  echo \"MountP: ${mountp}\"\n  if [ -z $mountp ]\n  then\n    echo \"[ERROR] Did you specify a mount point?\"\n    continue\n  fi\n\n  mkdir -p ${mountp}\n\n  if [ -z ${SKIP_EBS_REATTACH} ]\n  then\n    # Search for existing tagged EBS volume (in current AZ)\n    echo \"aws ec2 describe-volumes --region=${region} --filters Name=availability-zone,Values=${avail_zone} Name=tag:pipeline_phase,Values=${pipeline_phase} Name=tag:jive_service,Values=${jive_service} Name=tag:jive_subservice,Values=${jive_subservice} Name=status,Values=available Name=tag:device,Values=${device} Name=tag:Name,Values=${name} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n    previous_volume=$(aws ec2 describe-volumes --region=${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=available Name=tag:device,Values=${device} Name=tag:Name,Values=${name} Name=tag:pipeline_phase,Values=${pipeline_phase} Name=tag:jive_service,Values=${jive_service} Name=tag:jive_subservice,Values=${jive_subservice} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n    echo \"Previous volume: ${previous_volume}\"\n  else\n    previous_volume=\"\"\n    echo \"SKIP_EBS_REATTACH is set, not attempting to reattach old volume(s)\"\n  fi\n\n  # find current volume id\n  echo \"aws ec2 describe-volumes --region ${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=${instance_id} Name=attachment.device,Values=${device} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n  current_volume=$(aws ec2 describe-volumes --region ${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=${instance_id} Name=attachment.device,Values=${device} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n\n  if [ $? -ne 0 ]\n  then\n    echo \"[ERROR] Failed to get current volume ID for ${device}\"\n    continue\n  fi\n\n  echo \"Current volume: ${current_volume}\"\n\n  if [ ! -z $previous_volume ]\n  then\n\n    # detach current EBS\n    echo \"detaching current volume: ${current_volume}\"\n    aws ec2 detach-volume --region ${region} --volume-id ${current_volume}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to detach current volume: ${current_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to detach\n    echo \"sleeping for 120 to allow aws time to get its ducks in a row\"\n    sleep 120\n\n    # attach existing EBS\n    aws ec2 attach-volume --region ${region} --volume-id ${previous_volume} --instance-id ${instance_id} --device ${device}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to attach previous volume: ${previous_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to attach\n    echo \"sleeping for 120 to allow aws time to get its ducks in a row again\"\n    sleep 120\n\n    current_volume=${previous_volume}\n\n  else\n    # no previous volume found. assume tabula rasa\n    echo \"No previous volume found. Proceeding...\"\n    echo \"mkfs -t ext4 ${device}\"\n    mkfs -t ext4 ${device}\n  fi\n  \n  echo \"mount ${device} ${mountp}\"\n  mount ${device} ${mountp}\n  echo \"${device} ${mountp} ext4 defaults,nofail 0 2\" \u003e\u003e /etc/fstab\n\n  # add tags to the volume?\n  echo \"aws ec2 create-tags --region ${region} --resources ${current_volume} --tags Key=Name,Value=\\\"${name}\\\" Key=device,Value=${device} Key=pipeline_phase,Value=${pipeline_phase} Key=jive_service,Value=${jive_service} Key=jive_subservice,Value=${jive_subservice}\"\n  aws ec2 create-tags --region ${region} --resources ${current_volume} --tags Key=Name,Value=\"${name}\" Key=device,Value=${device} Key=pipeline_phase,Value=${pipeline_phase} Key=jive_service,Value=${jive_service} Key=jive_subservice,Value=${jive_subservice}\n\ndone\nIFS=$OLD_IFS\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x ${INSTALL_DIRECTORY}/ebs_mount.sh\n    ${INSTALL_DIRECTORY}/ebs_mount.sh -d /dev/xvdm:/data 2\u003e\u00261 \u003e\u003e ${INSTALL_DIRECTORY}/ebs_mount.log\n}\n\nfunction_nexus() {\n    # URL redirect fails without this entry\n    echo \"10.10.100.155 nexus-int.eng.jiveland.com\" \u003e\u003e /etc/hosts\n\n    # Script to download Ansible artifact from Nexus\n    cat \u003c\u003c'EOF' \u003e ${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n#!/bin/bash\n# Argument = -h -v -i groupId:artifactId:version -c classifier -p packaging -r repository\n\n# Define Nexus Configuration\nNEXUS_BASE=nexus-int.eng.jiveland.com\nREST_PATH=/service/local\nART_REDIR=/artifact/maven/redirect\n\n# Read in Complete Set of Coordinates from the Command Line\nGROUP_ID=\nARTIFACT_ID=\nVERSION=\"LATEST\"\nCLASSIFIER=\"\"\nPACKAGING=tar.gz\nREPO=\"candidates\"\nVERBOSE=0\n\nwhile getopts \"hvi:c:p:\" OPTION\ndo\n     case $OPTION in\n         h)\n             usage\n             exit 1\n             ;;\n         i)\n\t     OIFS=$IFS\n             IFS=\":\"\n\t     GAV_COORD=( $OPTARG )\n\t     GROUP_ID=${GAV_COORD[0]}\n             ARTIFACT_ID=${GAV_COORD[1]}\n             VERSION=${GAV_COORD[2]}\n\t     IFS=$OIFS\n             ;;\n         c)\n             CLASSIFIER=$OPTARG\n             ;;\n         p)\n             PACKAGING=$OPTARG\n             ;;\n         v)\n             VERBOSE=1\n             ;;\n         ?)\n             usage\n             exit\n             ;;\n     esac\ndone\n\nif [[ -z $GROUP_ID ]] || [[ -z $ARTIFACT_ID ]] || [[ -z $VERSION ]]\nthen\n     echo \"BAD ARGUMENTS: Either groupId, artifactId, or version was not supplied\" \u003e\u00262\n     usage\n     exit 1\nfi\n\n# Construct the base URL\nREDIRECT_URL=${NEXUS_BASE}${REST_PATH}${ART_REDIR}\n\n# Generate the list of parameters\nPARAM_KEYS=( g a v r p c )\nPARAM_VALUES=( $GROUP_ID $ARTIFACT_ID $VERSION $REPO $PACKAGING $CLASSIFIER )\nPARAMS=\"\"\nfor index in ${!PARAM_KEYS[*]}\ndo\n  if [[ ${PARAM_VALUES[$index]} != \"\" ]]\n  then\n    PARAMS=\"${PARAMS}${PARAM_KEYS[$index]}=${PARAM_VALUES[$index]}\u0026\"\n  fi\ndone\n\nREDIRECT_URL=\"${REDIRECT_URL}?${PARAMS}\"\n\necho \"Fetching Artifact from $REDIRECT_URL...\" \u003e\u00262\ncurl -sS -L ${REDIRECT_URL}\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x ${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n    ${INSTALL_DIRECTORY}/get_nexus_artifact.sh -i com.jivesoftware.techops:ansible-common:LATEST \u003e ${INSTALL_DIRECTORY}/ansible-common.tar.gz\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      aws configure set s3.signature_version s3v4\n      aws s3 cp s3://us-west-2-jive-data-pipeline-playbooks/${ADDITIONAL_BUNDLE_NAME}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz\n    fi\n}\n\nfunction_ansible() {\n    # Need Sudo TTY\n    sed -i s/'Defaults    requiretty'/'#Defaults    requiretty'/ /etc/sudoers\n    # Disable SELINUX for SSSD\n    sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config\n    setenforce 0\n    # Create Ansible working directories\n    mkdir -p ${INSTALL_DIRECTORY}/ansible-common\n    tar xf ${INSTALL_DIRECTORY}/ansible-common.tar.gz -C ${INSTALL_DIRECTORY}/ansible-common/\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      mkdir -p ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}\n      tar xf ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz -C ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}\n    fi\n    # Set Python to 2.6 and run Ansible locally\n    alternatives --set python /usr/bin/python2.6\n    yum install -y yum-python26 python-boto ansible\n\n    # Script to run Ansible locally\n    cat \u003c\u003cEOF \u003e ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n#!/bin/bash\n\nansible-playbook -i localhost ${INSTALL_DIRECTORY}/ansible-common/playbook-generic-node.yml --connection=local\nif [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\nthen\n  ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}/bin/call_ansible.sh\nfi\nEOF\n    # Run Ansible\n    chmod +x ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n    ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh \u003e\u003e ${INSTALL_DIRECTORY}/ansible-common/ansible_debug.log\n}\n\nfunction_restart() {\n    # Need to restart for SELINUX change.\n    shutdown -r now\n}\n\n# Run the things\nfunction_prep\nif [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\nthen\n  function_instance_store\nelse\n  function_ebs_attach\nfi\nfunction_nexus\nfunction_ansible\n#function_restart\n",
                            "template": "#!/bin/bash\n\n# SKIP_EBS_REATTACH - Set to a non-empty string to skip reattaching of any\n#                     unattached matching EBS volumes.\n#                     The ebs_attach script will still run and new volumes\n#                     will be attached/formatted as necessary\n# ADDITIONAL_BUNDLE_NAME - The name of a nexus bundle to download and unpack.\n#                          Leave blank to skip.\n#                          Must contain a script for setting up/calling ansible\n#                          located at/called:\n#           ./ansible/bin/call_ansible.sh\n#\nSKIP_EBS_REATTACH=${skip_ebs_reattach}\nADDITIONAL_BUNDLE_NAME=${bundle_name}\nADDITIONAL_BUNDLE_VERSION=${bundle_version}\nINSTALL_DIRECTORY=/opt/ansible\ndeclare -r instance_type=$(curl -s http://169.254.169.254/latest/meta-data/instance-type)\n\nfunction_prep() {\n    mkdir -p $${INSTALL_DIRECTORY}\n    # Get pip for awscli\n    yum install -y epel-release\n    yum install -y python-pip\n    pip install awscli\n}\n\nfunction_instance_store() {\n  yum install -y cryptsetup\n  passphrase=$(\u003c /dev/urandom tr -dc '_A-Za-z0-9@#%^_\\\\-\\\\=+' | head -c 256 | xargs -0 echo)\n\n  if [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\n  then\n    # this /could/ be a bit more flexible *wink!*\n\n    # set up disk encryption\n    echo \"cryptsetup luksFormat /dev/nvme0n1\"\n    echo $passphrase | cryptsetup luksFormat /dev/nvme0n1\n    UUID=$(cryptsetup luksUUID /dev/nvme0n1)\n    echo \"cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\"\n    echo \"$passphrase\" | cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\n    echo \"mkfs.ext4 /dev/mapper/elasticsearch_data\"\n    mkfs.ext4 /dev/mapper/elasticsearch_data\n    echo \"mount /dev/mapper/elasticsearch_data /data\"\n    mkdir -p /data\n    mount /dev/mapper/elasticsearch_data /data\n\n    # encrypt and save the volume's password\n    echo \"aws --region ${region} kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext xxxxxx --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\"\n    aws --region ${region} kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext \"$${passphrase}\" --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\n    unset passphrase\n\n    cat \u003c\u003c-EOM \u003e /etc/init.d/luks-mount\n#!/bin/bash\n# A quickly hacked together script to remount a luks volume at boot\n\n# Get the passphrase from KMS using the ciphertext\npassphrase=\\$(aws --region ${region} kms decrypt --ciphertext-blob fileb:///etc/.luks --output text --query Plaintext | base64 -d)\n\n# Open the LUKS volume\necho \"\\$passphrase\" | cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\n\n# Mount the volume\nmount /dev/mapper/elasticsearch_data /data\nEOM\n\n    chmod 755 /etc/init.d/luks-mount\n\n    # will this work for Centos 7? No it will not.\n    #ln -s /etc/init.d/luks-mount /etc/rc3.d/S15luks\n\n    # so instead, create a systemd file:\n    cat \u003c\u003c-EOM \u003e /usr/lib/systemd/system/data_remount.service\n[Unit]\nDescription=Mount the ephemeral data volume\nDocumentation=\nBefore=elasticsearch.service\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=oneshot\nExecStart=/etc/init.d/luks-mount\n\n[Install]\nWantedBy=multi-user.target\nEOM\n\n    systemctl daemon-reload\n    systemctl enable data_remount\n\n  fi\n}\n\nfunction_ebs_attach() {\n    cat \u003c\u003c'EOF' \u003e $${INSTALL_DIRECTORY}/ebs_mount.sh\n#!/bin/bash -v\n#\n# Usage:\n# ./ebs_mount.sh -d \u003cdevice:mountpoint\u003e[,\u003cdevice:mountpoint\u003e...]\n#\n# Example:\n# ./ebs_mount.sh -d /dev/xvdm:/data/elasticsearch,/dev/xvdn:/data/more_data\n#\ndeclare -r instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)\ndeclare -r avail_zone=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)\n\nregion=\"${region}\"\n\nwhile getopts \"d:n:p:\" opt; do\n  case \"$opt\" in\n  d) devices=$OPTARG\n     ;;\n  esac\ndone\n\nif [ -z $name ]\nthen\n  name=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`Name`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\nfi\n\npipeline_phase=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`pipeline_phase`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_service=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_service`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_subservice=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_subservice`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\n\necho \"Pipeline_phase: $${pipeline_phase}\"\necho \"Jive_service: $${jive_service}\"\necho \"Jive_subservice: $${jive_subservice}\"\n\nOLD_IFS=$IFS\nIFS=','\nfor dev_mp_pair in $devices\ndo\n  # I have no idea what I'm doing\n  IFS=':' read -ra PAIR \u003c\u003c\u003c \"$dev_mp_pair\"\n  IFS=','\n  device=$${PAIR[0]}\n  mountp=$${PAIR[1]}\n  echo \"Device: $${device}\"\n  if [ -z $device ]\n  then\n    echo \"[ERROR] Did you specify a device name?\"\n    continue\n  fi\n\n  echo \"MountP: $${mountp}\"\n  if [ -z $mountp ]\n  then\n    echo \"[ERROR] Did you specify a mount point?\"\n    continue\n  fi\n\n  mkdir -p $${mountp}\n\n  if [ -z $${SKIP_EBS_REATTACH} ]\n  then\n    # Search for existing tagged EBS volume (in current AZ)\n    echo \"aws ec2 describe-volumes --region=$${region} --filters Name=availability-zone,Values=$${avail_zone} Name=tag:pipeline_phase,Values=$${pipeline_phase} Name=tag:jive_service,Values=$${jive_service} Name=tag:jive_subservice,Values=$${jive_subservice} Name=status,Values=available Name=tag:device,Values=$${device} Name=tag:Name,Values=$${name} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n    previous_volume=$(aws ec2 describe-volumes --region=$${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=available Name=tag:device,Values=$${device} Name=tag:Name,Values=$${name} Name=tag:pipeline_phase,Values=$${pipeline_phase} Name=tag:jive_service,Values=$${jive_service} Name=tag:jive_subservice,Values=$${jive_subservice} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n    echo \"Previous volume: $${previous_volume}\"\n  else\n    previous_volume=\"\"\n    echo \"SKIP_EBS_REATTACH is set, not attempting to reattach old volume(s)\"\n  fi\n\n  # find current volume id\n  echo \"aws ec2 describe-volumes --region $${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=$${instance_id} Name=attachment.device,Values=$${device} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n  current_volume=$(aws ec2 describe-volumes --region $${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=$${instance_id} Name=attachment.device,Values=$${device} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n\n  if [ $? -ne 0 ]\n  then\n    echo \"[ERROR] Failed to get current volume ID for $${device}\"\n    continue\n  fi\n\n  echo \"Current volume: $${current_volume}\"\n\n  if [ ! -z $previous_volume ]\n  then\n\n    # detach current EBS\n    echo \"detaching current volume: $${current_volume}\"\n    aws ec2 detach-volume --region $${region} --volume-id $${current_volume}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to detach current volume: $${current_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to detach\n    echo \"sleeping for 120 to allow aws time to get its ducks in a row\"\n    sleep 120\n\n    # attach existing EBS\n    aws ec2 attach-volume --region $${region} --volume-id $${previous_volume} --instance-id $${instance_id} --device $${device}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to attach previous volume: $${previous_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to attach\n    echo \"sleeping for 120 to allow aws time to get its ducks in a row again\"\n    sleep 120\n\n    current_volume=$${previous_volume}\n\n  else\n    # no previous volume found. assume tabula rasa\n    echo \"No previous volume found. Proceeding...\"\n    echo \"mkfs -t ext4 $${device}\"\n    mkfs -t ext4 $${device}\n  fi\n  \n  echo \"mount $${device} $${mountp}\"\n  mount $${device} $${mountp}\n  echo \"$${device} $${mountp} ext4 defaults,nofail 0 2\" \u003e\u003e /etc/fstab\n\n  # add tags to the volume?\n  echo \"aws ec2 create-tags --region $${region} --resources $${current_volume} --tags Key=Name,Value=\\\"$${name}\\\" Key=device,Value=$${device} Key=pipeline_phase,Value=$${pipeline_phase} Key=jive_service,Value=$${jive_service} Key=jive_subservice,Value=$${jive_subservice}\"\n  aws ec2 create-tags --region $${region} --resources $${current_volume} --tags Key=Name,Value=\"$${name}\" Key=device,Value=$${device} Key=pipeline_phase,Value=$${pipeline_phase} Key=jive_service,Value=$${jive_service} Key=jive_subservice,Value=$${jive_subservice}\n\ndone\nIFS=$OLD_IFS\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x $${INSTALL_DIRECTORY}/ebs_mount.sh\n    $${INSTALL_DIRECTORY}/ebs_mount.sh -d ${devices} 2\u003e\u00261 \u003e\u003e $${INSTALL_DIRECTORY}/ebs_mount.log\n}\n\nfunction_nexus() {\n    # URL redirect fails without this entry\n    echo \"10.10.100.155 nexus-int.eng.jiveland.com\" \u003e\u003e /etc/hosts\n\n    # Script to download Ansible artifact from Nexus\n    cat \u003c\u003c'EOF' \u003e $${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n#!/bin/bash\n# Argument = -h -v -i groupId:artifactId:version -c classifier -p packaging -r repository\n\n# Define Nexus Configuration\nNEXUS_BASE=nexus-int.eng.jiveland.com\nREST_PATH=/service/local\nART_REDIR=/artifact/maven/redirect\n\n# Read in Complete Set of Coordinates from the Command Line\nGROUP_ID=\nARTIFACT_ID=\nVERSION=\"LATEST\"\nCLASSIFIER=\"\"\nPACKAGING=tar.gz\nREPO=\"candidates\"\nVERBOSE=0\n\nwhile getopts \"hvi:c:p:\" OPTION\ndo\n     case $OPTION in\n         h)\n             usage\n             exit 1\n             ;;\n         i)\n\t     OIFS=$IFS\n             IFS=\":\"\n\t     GAV_COORD=( $OPTARG )\n\t     GROUP_ID=$${GAV_COORD[0]}\n             ARTIFACT_ID=$${GAV_COORD[1]}\n             VERSION=$${GAV_COORD[2]}\n\t     IFS=$OIFS\n             ;;\n         c)\n             CLASSIFIER=$OPTARG\n             ;;\n         p)\n             PACKAGING=$OPTARG\n             ;;\n         v)\n             VERBOSE=1\n             ;;\n         ?)\n             usage\n             exit\n             ;;\n     esac\ndone\n\nif [[ -z $GROUP_ID ]] || [[ -z $ARTIFACT_ID ]] || [[ -z $VERSION ]]\nthen\n     echo \"BAD ARGUMENTS: Either groupId, artifactId, or version was not supplied\" \u003e\u00262\n     usage\n     exit 1\nfi\n\n# Construct the base URL\nREDIRECT_URL=$${NEXUS_BASE}$${REST_PATH}$${ART_REDIR}\n\n# Generate the list of parameters\nPARAM_KEYS=( g a v r p c )\nPARAM_VALUES=( $GROUP_ID $ARTIFACT_ID $VERSION $REPO $PACKAGING $CLASSIFIER )\nPARAMS=\"\"\nfor index in $${!PARAM_KEYS[*]}\ndo\n  if [[ $${PARAM_VALUES[$index]} != \"\" ]]\n  then\n    PARAMS=\"$${PARAMS}$${PARAM_KEYS[$index]}=$${PARAM_VALUES[$index]}\u0026\"\n  fi\ndone\n\nREDIRECT_URL=\"$${REDIRECT_URL}?$${PARAMS}\"\n\necho \"Fetching Artifact from $REDIRECT_URL...\" \u003e\u00262\ncurl -sS -L $${REDIRECT_URL}\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x $${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n    $${INSTALL_DIRECTORY}/get_nexus_artifact.sh -i com.jivesoftware.techops:ansible-common:LATEST \u003e $${INSTALL_DIRECTORY}/ansible-common.tar.gz\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      aws configure set s3.signature_version s3v4\n      aws s3 cp s3://us-west-2-jive-data-pipeline-playbooks/$${ADDITIONAL_BUNDLE_NAME}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz\n    fi\n}\n\nfunction_ansible() {\n    # Need Sudo TTY\n    sed -i s/'Defaults    requiretty'/'#Defaults    requiretty'/ /etc/sudoers\n    # Disable SELINUX for SSSD\n    sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config\n    setenforce 0\n    # Create Ansible working directories\n    mkdir -p $${INSTALL_DIRECTORY}/ansible-common\n    tar xf $${INSTALL_DIRECTORY}/ansible-common.tar.gz -C $${INSTALL_DIRECTORY}/ansible-common/\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      mkdir -p $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}\n      tar xf $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz -C $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}\n    fi\n    # Set Python to 2.6 and run Ansible locally\n    alternatives --set python /usr/bin/python2.6\n    yum install -y yum-python26 python-boto ansible\n\n    # Script to run Ansible locally\n    cat \u003c\u003cEOF \u003e $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n#!/bin/bash\n\nansible-playbook -i localhost $${INSTALL_DIRECTORY}/ansible-common/playbook-generic-node.yml --connection=local\nif [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\nthen\n  $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}/bin/call_ansible.sh\nfi\nEOF\n    # Run Ansible\n    chmod +x $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n    $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh \u003e\u003e $${INSTALL_DIRECTORY}/ansible-common/ansible_debug.log\n}\n\nfunction_restart() {\n    # Need to restart for SELINUX change.\n    shutdown -r now\n}\n\n# Run the things\nfunction_prep\nif [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\nthen\n  function_instance_store\nelse\n  function_ebs_attach\nfi\nfunction_nexus\nfunction_ansible\n#function_restart\n",
                            "vars.%": "7",
                            "vars.account_name": "jive-data-brewprod",
                            "vars.bundle_name": "ansible-playbooks-aws",
                            "vars.bundle_version": "2.6.2",
                            "vars.devices": "/dev/xvdm:/data",
                            "vars.pipeline_phase": "brewprod",
                            "vars.region": "us-west-2",
                            "vars.skip_ebs_reattach": "true"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "template_file.s3_policy": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "d80b70623c7ea89b4a5565b629fddd10990af8a143e2b0b058a8a5b046065e79",
                        "attributes": {
                            "id": "d80b70623c7ea89b4a5565b629fddd10990af8a143e2b0b058a8a5b046065e79",
                            "rendered": "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"statement201609211128\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::409573287771:role/ebs-attach-and-secrets-role-usw2-jcx-people\"\n      },\n      \"Action\": [\n        \"s3:ListBucket\",\n        \"s3:GetBucketLocation\",\n        \"s3:ListBucketMultipartUploads\",\n        \"s3:ListBucketVersions\"\n      ],\n      \"Resource\": \"arn:aws:s3:::us-west-2-jive-data-brewprod-usw2-jcx-people-snaps\"\n    },\n    {\n      \"Sid\": \"statement201609211130\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::409573287771:role/ebs-attach-and-secrets-role-usw2-jcx-people\"\n      },\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:AbortMultipartUpload\",\n        \"s3:ListMultipartUploadParts\"\n      ],\n      \"Resource\": \"arn:aws:s3:::us-west-2-jive-data-brewprod-usw2-jcx-people-snaps/*\"\n    }\n  ]\n}\n",
                            "template": "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"statement201609211128\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::${aws_account}:role/ebs-attach-and-secrets-role-${jive_subservice}\"\n      },\n      \"Action\": [\n        \"s3:ListBucket\",\n        \"s3:GetBucketLocation\",\n        \"s3:ListBucketMultipartUploads\",\n        \"s3:ListBucketVersions\"\n      ],\n      \"Resource\": \"arn:aws:s3:::us-west-2-jive-data-${pipeline_phase}-${jive_subservice}-snaps\"\n    },\n    {\n      \"Sid\": \"statement201609211130\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::${aws_account}:role/ebs-attach-and-secrets-role-${jive_subservice}\"\n      },\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:AbortMultipartUpload\",\n        \"s3:ListMultipartUploadParts\"\n      ],\n      \"Resource\": \"arn:aws:s3:::us-west-2-jive-data-${pipeline_phase}-${jive_subservice}-snaps/*\"\n    }\n  ]\n}\n",
                            "vars.%": "3",
                            "vars.aws_account": "409573287771",
                            "vars.jive_subservice": "usw2-jcx-people",
                            "vars.pipeline_phase": "brewprod"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                },
                "template_file.user_data": {
                    "type": "template_file",
                    "depends_on": [],
                    "primary": {
                        "id": "09b9cef501ddab3bfb4158e24e2a96c89fe9fb0818613e6b53249fdfea844134",
                        "attributes": {
                            "id": "09b9cef501ddab3bfb4158e24e2a96c89fe9fb0818613e6b53249fdfea844134",
                            "rendered": "#!/bin/bash\n\n# SKIP_EBS_REATTACH - Set to a non-empty string to skip reattaching of any\n#                     unattached matching EBS volumes.\n#                     The ebs_attach script will still run and new volumes\n#                     will be attached/formatted as necessary\n# ADDITIONAL_BUNDLE_NAME - The name of a nexus bundle to download and unpack.\n#                          Leave blank to skip.\n#                          Must contain a script for setting up/calling ansible\n#                          located at/called:\n#           ./ansible/bin/call_ansible.sh\n#\nSKIP_EBS_REATTACH=true\nADDITIONAL_BUNDLE_NAME=ansible-playbooks-aws\nADDITIONAL_BUNDLE_VERSION=2.6.2\nINSTALL_DIRECTORY=/opt/ansible\ndeclare -r instance_type=$(curl -s http://169.254.169.254/latest/meta-data/instance-type)\n\nfunction_prep() {\n    mkdir -p ${INSTALL_DIRECTORY}\n    # Get pip for awscli\n    yum install -y epel-release\n    yum install -y python-pip\n    pip install awscli\n}\n\nfunction_instance_store() {\n  yum install -y cryptsetup\n  passphrase=$(\u003c /dev/urandom tr -dc '_A-Za-z0-9@#%^_\\\\-\\\\=+' | head -c 256 | xargs -0 echo)\n\n  if [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\n  then\n    # this /could/ be a bit more flexible *wink!*\n\n    # set up disk encryption\n    echo \"cryptsetup luksFormat /dev/nvme0n1\"\n    echo $passphrase | cryptsetup luksFormat /dev/nvme0n1\n    UUID=$(cryptsetup luksUUID /dev/nvme0n1)\n    echo \"cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\"\n    echo \"$passphrase\" | cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\n    echo \"mkfs.ext4 /dev/mapper/elasticsearch_data\"\n    mkfs.ext4 /dev/mapper/elasticsearch_data\n    echo \"mount /dev/mapper/elasticsearch_data /data\"\n    mkdir -p /data\n    mount /dev/mapper/elasticsearch_data /data\n\n    # encrypt and save the volume's password\n    echo \"aws --region us-west-2 kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext xxxxxx --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\"\n    aws --region us-west-2 kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext \"${passphrase}\" --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\n    unset passphrase\n\n    cat \u003c\u003c-EOM \u003e /etc/init.d/luks-mount\n#!/bin/bash\n# A quickly hacked together script to remount a luks volume at boot\n\n# Get the passphrase from KMS using the ciphertext\npassphrase=\\$(aws --region us-west-2 kms decrypt --ciphertext-blob fileb:///etc/.luks --output text --query Plaintext | base64 -d)\n\n# Open the LUKS volume\necho \"\\$passphrase\" | cryptsetup luksOpen --allow-discards UUID=${UUID} elasticsearch_data\n\n# Mount the volume\nmount /dev/mapper/elasticsearch_data /data\nEOM\n\n    chmod 755 /etc/init.d/luks-mount\n\n    # will this work for Centos 7? No it will not.\n    #ln -s /etc/init.d/luks-mount /etc/rc3.d/S15luks\n\n    # so instead, create a systemd file:\n    cat \u003c\u003c-EOM \u003e /usr/lib/systemd/system/data_remount.service\n[Unit]\nDescription=Mount the ephemeral data volume\nDocumentation=\nBefore=elasticsearch.service\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=oneshot\nExecStart=/etc/init.d/luks-mount\n\n[Install]\nWantedBy=multi-user.target\nEOM\n\n    systemctl daemon-reload\n    systemctl enable data_remount\n\n  fi\n}\n\nfunction_ebs_attach() {\n    cat \u003c\u003c'EOF' \u003e ${INSTALL_DIRECTORY}/ebs_mount.sh\n#!/bin/bash -v\n#\n# Usage:\n# ./ebs_mount.sh -d \u003cdevice:mountpoint\u003e[,\u003cdevice:mountpoint\u003e...]\n#\n# Example:\n# ./ebs_mount.sh -d /dev/xvdm:/data/elasticsearch,/dev/xvdn:/data/more_data\n#\ndeclare -r instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)\ndeclare -r avail_zone=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)\n\nregion=\"us-west-2\"\n\nwhile getopts \"d:n:p:\" opt; do\n  case \"$opt\" in\n  d) devices=$OPTARG\n     ;;\n  esac\ndone\n\nif [ -z $name ]\nthen\n  name=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`Name`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\nfi\n\npipeline_phase=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`pipeline_phase`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_service=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_service`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_subservice=$(aws ec2 describe-instances --instance-ids ${instance_id} --region ${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_subservice`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\n\necho \"Pipeline_phase: ${pipeline_phase}\"\necho \"Jive_service: ${jive_service}\"\necho \"Jive_subservice: ${jive_subservice}\"\n\nOLD_IFS=$IFS\nIFS=','\nfor dev_mp_pair in $devices\ndo\n  # I have no idea what I'm doing\n  IFS=':' read -ra PAIR \u003c\u003c\u003c \"$dev_mp_pair\"\n  IFS=','\n  device=${PAIR[0]}\n  mountp=${PAIR[1]}\n  echo \"Device: ${device}\"\n  if [ -z $device ]\n  then\n    echo \"[ERROR] Did you specify a device name?\"\n    continue\n  fi\n\n  echo \"MountP: ${mountp}\"\n  if [ -z $mountp ]\n  then\n    echo \"[ERROR] Did you specify a mount point?\"\n    continue\n  fi\n\n  mkdir -p ${mountp}\n\n  if [ -z ${SKIP_EBS_REATTACH} ]\n  then\n    # Search for existing tagged EBS volume (in current AZ)\n    echo \"aws ec2 describe-volumes --region=${region} --filters Name=availability-zone,Values=${avail_zone} Name=tag:pipeline_phase,Values=${pipeline_phase} Name=tag:jive_service,Values=${jive_service} Name=tag:jive_subservice,Values=${jive_subservice} Name=status,Values=available Name=tag:device,Values=${device} Name=tag:Name,Values=${name} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n    previous_volume=$(aws ec2 describe-volumes --region=${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=available Name=tag:device,Values=${device} Name=tag:Name,Values=${name} Name=tag:pipeline_phase,Values=${pipeline_phase} Name=tag:jive_service,Values=${jive_service} Name=tag:jive_subservice,Values=${jive_subservice} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n    echo \"Previous volume: ${previous_volume}\"\n  else\n    previous_volume=\"\"\n    echo \"SKIP_EBS_REATTACH is set, not attempting to reattach old volume(s)\"\n  fi\n\n  # find current volume id\n  echo \"aws ec2 describe-volumes --region ${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=${instance_id} Name=attachment.device,Values=${device} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n  current_volume=$(aws ec2 describe-volumes --region ${region} --filters Name=availability-zone,Values=${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=${instance_id} Name=attachment.device,Values=${device} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n\n  if [ $? -ne 0 ]\n  then\n    echo \"[ERROR] Failed to get current volume ID for ${device}\"\n    continue\n  fi\n\n  echo \"Current volume: ${current_volume}\"\n\n  if [ ! -z $previous_volume ]\n  then\n\n    # detach current EBS\n    echo \"detaching current volume: ${current_volume}\"\n    aws ec2 detach-volume --region ${region} --volume-id ${current_volume}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to detach current volume: ${current_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to detach\n    echo \"sleeping for 120 to allow aws time to get its ducks in a row\"\n    sleep 120\n\n    # attach existing EBS\n    aws ec2 attach-volume --region ${region} --volume-id ${previous_volume} --instance-id ${instance_id} --device ${device}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to attach previous volume: ${previous_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to attach\n    echo \"sleeping for 120 to allow aws time to get its ducks in a row again\"\n    sleep 120\n\n    current_volume=${previous_volume}\n\n  else\n    # no previous volume found. assume tabula rasa\n    echo \"No previous volume found. Proceeding...\"\n    echo \"mkfs -t ext4 ${device}\"\n    mkfs -t ext4 ${device}\n  fi\n  \n  echo \"mount ${device} ${mountp}\"\n  mount ${device} ${mountp}\n  echo \"${device} ${mountp} ext4 defaults,nofail 0 2\" \u003e\u003e /etc/fstab\n\n  # add tags to the volume?\n  echo \"aws ec2 create-tags --region ${region} --resources ${current_volume} --tags Key=Name,Value=\\\"${name}\\\" Key=device,Value=${device} Key=pipeline_phase,Value=${pipeline_phase} Key=jive_service,Value=${jive_service} Key=jive_subservice,Value=${jive_subservice}\"\n  aws ec2 create-tags --region ${region} --resources ${current_volume} --tags Key=Name,Value=\"${name}\" Key=device,Value=${device} Key=pipeline_phase,Value=${pipeline_phase} Key=jive_service,Value=${jive_service} Key=jive_subservice,Value=${jive_subservice}\n\ndone\nIFS=$OLD_IFS\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x ${INSTALL_DIRECTORY}/ebs_mount.sh\n    ${INSTALL_DIRECTORY}/ebs_mount.sh -d /dev/xvdm:/data 2\u003e\u00261 \u003e\u003e ${INSTALL_DIRECTORY}/ebs_mount.log\n}\n\nfunction_nexus() {\n    # URL redirect fails without this entry\n    echo \"10.10.100.155 nexus-int.eng.jiveland.com\" \u003e\u003e /etc/hosts\n\n    # Script to download Ansible artifact from Nexus\n    cat \u003c\u003c'EOF' \u003e ${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n#!/bin/bash\n# Argument = -h -v -i groupId:artifactId:version -c classifier -p packaging -r repository\n\n# Define Nexus Configuration\nNEXUS_BASE=nexus-int.eng.jiveland.com\nREST_PATH=/service/local\nART_REDIR=/artifact/maven/redirect\n\n# Read in Complete Set of Coordinates from the Command Line\nGROUP_ID=\nARTIFACT_ID=\nVERSION=\"LATEST\"\nCLASSIFIER=\"\"\nPACKAGING=tar.gz\nREPO=\"candidates\"\nVERBOSE=0\n\nwhile getopts \"hvi:c:p:\" OPTION\ndo\n     case $OPTION in\n         h)\n             usage\n             exit 1\n             ;;\n         i)\n\t     OIFS=$IFS\n             IFS=\":\"\n\t     GAV_COORD=( $OPTARG )\n\t     GROUP_ID=${GAV_COORD[0]}\n             ARTIFACT_ID=${GAV_COORD[1]}\n             VERSION=${GAV_COORD[2]}\n\t     IFS=$OIFS\n             ;;\n         c)\n             CLASSIFIER=$OPTARG\n             ;;\n         p)\n             PACKAGING=$OPTARG\n             ;;\n         v)\n             VERBOSE=1\n             ;;\n         ?)\n             usage\n             exit\n             ;;\n     esac\ndone\n\nif [[ -z $GROUP_ID ]] || [[ -z $ARTIFACT_ID ]] || [[ -z $VERSION ]]\nthen\n     echo \"BAD ARGUMENTS: Either groupId, artifactId, or version was not supplied\" \u003e\u00262\n     usage\n     exit 1\nfi\n\n# Construct the base URL\nREDIRECT_URL=${NEXUS_BASE}${REST_PATH}${ART_REDIR}\n\n# Generate the list of parameters\nPARAM_KEYS=( g a v r p c )\nPARAM_VALUES=( $GROUP_ID $ARTIFACT_ID $VERSION $REPO $PACKAGING $CLASSIFIER )\nPARAMS=\"\"\nfor index in ${!PARAM_KEYS[*]}\ndo\n  if [[ ${PARAM_VALUES[$index]} != \"\" ]]\n  then\n    PARAMS=\"${PARAMS}${PARAM_KEYS[$index]}=${PARAM_VALUES[$index]}\u0026\"\n  fi\ndone\n\nREDIRECT_URL=\"${REDIRECT_URL}?${PARAMS}\"\n\necho \"Fetching Artifact from $REDIRECT_URL...\" \u003e\u00262\ncurl -sS -L ${REDIRECT_URL}\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x ${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n    ${INSTALL_DIRECTORY}/get_nexus_artifact.sh -i com.jivesoftware.techops:ansible-common:LATEST \u003e ${INSTALL_DIRECTORY}/ansible-common.tar.gz\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      aws configure set s3.signature_version s3v4\n      aws s3 cp s3://us-west-2-jive-data-pipeline-playbooks/${ADDITIONAL_BUNDLE_NAME}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz\n    fi\n}\n\nfunction_ansible() {\n    # Need Sudo TTY\n    sed -i s/'Defaults    requiretty'/'#Defaults    requiretty'/ /etc/sudoers\n    # Disable SELINUX for SSSD\n    sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config\n    setenforce 0\n    # Create Ansible working directories\n    mkdir -p ${INSTALL_DIRECTORY}/ansible-common\n    tar xf ${INSTALL_DIRECTORY}/ansible-common.tar.gz -C ${INSTALL_DIRECTORY}/ansible-common/\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      mkdir -p ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}\n      tar xf ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}.tgz -C ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}\n    fi\n    # Set Python to 2.6 and run Ansible locally\n    alternatives --set python /usr/bin/python2.6\n    yum install -y yum-python26 python-boto ansible\n\n    # Script to run Ansible locally\n    cat \u003c\u003cEOF \u003e ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n#!/bin/bash\n\nansible-playbook -i localhost ${INSTALL_DIRECTORY}/ansible-common/playbook-generic-node.yml --connection=local\nif [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\nthen\n  ${INSTALL_DIRECTORY}/${ADDITIONAL_BUNDLE_NAME}-${ADDITIONAL_BUNDLE_VERSION}/bin/call_ansible.sh\nfi\nEOF\n    # Run Ansible\n    chmod +x ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n    ${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh \u003e\u003e ${INSTALL_DIRECTORY}/ansible-common/ansible_debug.log\n}\n\nfunction_restart() {\n    # Need to restart for SELINUX change.\n    shutdown -r now\n}\n\n# Run the things\nfunction_prep\nif [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\nthen\n  function_instance_store\nelse\n  function_ebs_attach\nfi\nfunction_nexus\nfunction_ansible\n#function_restart\n",
                            "template": "#!/bin/bash\n\n# SKIP_EBS_REATTACH - Set to a non-empty string to skip reattaching of any\n#                     unattached matching EBS volumes.\n#                     The ebs_attach script will still run and new volumes\n#                     will be attached/formatted as necessary\n# ADDITIONAL_BUNDLE_NAME - The name of a nexus bundle to download and unpack.\n#                          Leave blank to skip.\n#                          Must contain a script for setting up/calling ansible\n#                          located at/called:\n#           ./ansible/bin/call_ansible.sh\n#\nSKIP_EBS_REATTACH=${skip_ebs_reattach}\nADDITIONAL_BUNDLE_NAME=${bundle_name}\nADDITIONAL_BUNDLE_VERSION=${bundle_version}\nINSTALL_DIRECTORY=/opt/ansible\ndeclare -r instance_type=$(curl -s http://169.254.169.254/latest/meta-data/instance-type)\n\nfunction_prep() {\n    mkdir -p $${INSTALL_DIRECTORY}\n    # Get pip for awscli\n    yum install -y epel-release\n    yum install -y python-pip\n    pip install awscli\n}\n\nfunction_instance_store() {\n  yum install -y cryptsetup\n  passphrase=$(\u003c /dev/urandom tr -dc '_A-Za-z0-9@#%^_\\\\-\\\\=+' | head -c 256 | xargs -0 echo)\n\n  if [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\n  then\n    # this /could/ be a bit more flexible *wink!*\n\n    # set up disk encryption\n    echo \"cryptsetup luksFormat /dev/nvme0n1\"\n    echo $passphrase | cryptsetup luksFormat /dev/nvme0n1\n    UUID=$(cryptsetup luksUUID /dev/nvme0n1)\n    echo \"cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\"\n    echo \"$passphrase\" | cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\n    echo \"mkfs.ext4 /dev/mapper/elasticsearch_data\"\n    mkfs.ext4 /dev/mapper/elasticsearch_data\n    echo \"mount /dev/mapper/elasticsearch_data /data\"\n    mkdir -p /data\n    mount /dev/mapper/elasticsearch_data /data\n\n    # encrypt and save the volume's password\n    echo \"aws --region ${region} kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext xxxxxx --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\"\n    aws --region ${region} kms encrypt --key-id 'alias/elasticsearch-instance-store-key' --plaintext \"$${passphrase}\" --query CiphertextBlob --output text | base64 -d \u003e /etc/.luks\n    unset passphrase\n\n    cat \u003c\u003c-EOM \u003e /etc/init.d/luks-mount\n#!/bin/bash\n# A quickly hacked together script to remount a luks volume at boot\n\n# Get the passphrase from KMS using the ciphertext\npassphrase=\\$(aws --region ${region} kms decrypt --ciphertext-blob fileb:///etc/.luks --output text --query Plaintext | base64 -d)\n\n# Open the LUKS volume\necho \"\\$passphrase\" | cryptsetup luksOpen --allow-discards UUID=$${UUID} elasticsearch_data\n\n# Mount the volume\nmount /dev/mapper/elasticsearch_data /data\nEOM\n\n    chmod 755 /etc/init.d/luks-mount\n\n    # will this work for Centos 7? No it will not.\n    #ln -s /etc/init.d/luks-mount /etc/rc3.d/S15luks\n\n    # so instead, create a systemd file:\n    cat \u003c\u003c-EOM \u003e /usr/lib/systemd/system/data_remount.service\n[Unit]\nDescription=Mount the ephemeral data volume\nDocumentation=\nBefore=elasticsearch.service\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=oneshot\nExecStart=/etc/init.d/luks-mount\n\n[Install]\nWantedBy=multi-user.target\nEOM\n\n    systemctl daemon-reload\n    systemctl enable data_remount\n\n  fi\n}\n\nfunction_ebs_attach() {\n    cat \u003c\u003c'EOF' \u003e $${INSTALL_DIRECTORY}/ebs_mount.sh\n#!/bin/bash -v\n#\n# Usage:\n# ./ebs_mount.sh -d \u003cdevice:mountpoint\u003e[,\u003cdevice:mountpoint\u003e...]\n#\n# Example:\n# ./ebs_mount.sh -d /dev/xvdm:/data/elasticsearch,/dev/xvdn:/data/more_data\n#\ndeclare -r instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)\ndeclare -r avail_zone=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)\n\nregion=\"${region}\"\n\nwhile getopts \"d:n:p:\" opt; do\n  case \"$opt\" in\n  d) devices=$OPTARG\n     ;;\n  esac\ndone\n\nif [ -z $name ]\nthen\n  name=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`Name`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\nfi\n\npipeline_phase=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`pipeline_phase`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_service=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_service`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\njive_subservice=$(aws ec2 describe-instances --instance-ids $${instance_id} --region $${region} --query 'Reservations[0].Instances[0].Tags[?Key==`jive_subservice`]' | python -c 'import sys, json; print json.load(sys.stdin)[0][\"Value\"]')\n\necho \"Pipeline_phase: $${pipeline_phase}\"\necho \"Jive_service: $${jive_service}\"\necho \"Jive_subservice: $${jive_subservice}\"\n\nOLD_IFS=$IFS\nIFS=','\nfor dev_mp_pair in $devices\ndo\n  # I have no idea what I'm doing\n  IFS=':' read -ra PAIR \u003c\u003c\u003c \"$dev_mp_pair\"\n  IFS=','\n  device=$${PAIR[0]}\n  mountp=$${PAIR[1]}\n  echo \"Device: $${device}\"\n  if [ -z $device ]\n  then\n    echo \"[ERROR] Did you specify a device name?\"\n    continue\n  fi\n\n  echo \"MountP: $${mountp}\"\n  if [ -z $mountp ]\n  then\n    echo \"[ERROR] Did you specify a mount point?\"\n    continue\n  fi\n\n  mkdir -p $${mountp}\n\n  if [ -z $${SKIP_EBS_REATTACH} ]\n  then\n    # Search for existing tagged EBS volume (in current AZ)\n    echo \"aws ec2 describe-volumes --region=$${region} --filters Name=availability-zone,Values=$${avail_zone} Name=tag:pipeline_phase,Values=$${pipeline_phase} Name=tag:jive_service,Values=$${jive_service} Name=tag:jive_subservice,Values=$${jive_subservice} Name=status,Values=available Name=tag:device,Values=$${device} Name=tag:Name,Values=$${name} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n    previous_volume=$(aws ec2 describe-volumes --region=$${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=available Name=tag:device,Values=$${device} Name=tag:Name,Values=$${name} Name=tag:pipeline_phase,Values=$${pipeline_phase} Name=tag:jive_service,Values=$${jive_service} Name=tag:jive_subservice,Values=$${jive_subservice} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n    echo \"Previous volume: $${previous_volume}\"\n  else\n    previous_volume=\"\"\n    echo \"SKIP_EBS_REATTACH is set, not attempting to reattach old volume(s)\"\n  fi\n\n  # find current volume id\n  echo \"aws ec2 describe-volumes --region $${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=$${instance_id} Name=attachment.device,Values=$${device} | python -c 'import sys, json; print json.load(sys.stdin)[\\\"Volumes\\\"][0][\\\"VolumeId\\\"]'\"\n  current_volume=$(aws ec2 describe-volumes --region $${region} --filters Name=availability-zone,Values=$${avail_zone} Name=status,Values=in-use Name=attachment.instance-id,Values=$${instance_id} Name=attachment.device,Values=$${device} | python -c 'import sys, json; print json.load(sys.stdin)[\"Volumes\"][0][\"VolumeId\"]')\n\n  if [ $? -ne 0 ]\n  then\n    echo \"[ERROR] Failed to get current volume ID for $${device}\"\n    continue\n  fi\n\n  echo \"Current volume: $${current_volume}\"\n\n  if [ ! -z $previous_volume ]\n  then\n\n    # detach current EBS\n    echo \"detaching current volume: $${current_volume}\"\n    aws ec2 detach-volume --region $${region} --volume-id $${current_volume}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to detach current volume: $${current_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to detach\n    echo \"sleeping for 120 to allow aws time to get its ducks in a row\"\n    sleep 120\n\n    # attach existing EBS\n    aws ec2 attach-volume --region $${region} --volume-id $${previous_volume} --instance-id $${instance_id} --device $${device}\n    if [ $? -ne 0 ]\n    then\n      echo \"[ERROR] Failed to attach previous volume: $${previous_volume}\"\n      continue\n    fi\n\n    # sleep X seconds or something? to give AWS time to attach\n    echo \"sleeping for 120 to allow aws time to get its ducks in a row again\"\n    sleep 120\n\n    current_volume=$${previous_volume}\n\n  else\n    # no previous volume found. assume tabula rasa\n    echo \"No previous volume found. Proceeding...\"\n    echo \"mkfs -t ext4 $${device}\"\n    mkfs -t ext4 $${device}\n  fi\n  \n  echo \"mount $${device} $${mountp}\"\n  mount $${device} $${mountp}\n  echo \"$${device} $${mountp} ext4 defaults,nofail 0 2\" \u003e\u003e /etc/fstab\n\n  # add tags to the volume?\n  echo \"aws ec2 create-tags --region $${region} --resources $${current_volume} --tags Key=Name,Value=\\\"$${name}\\\" Key=device,Value=$${device} Key=pipeline_phase,Value=$${pipeline_phase} Key=jive_service,Value=$${jive_service} Key=jive_subservice,Value=$${jive_subservice}\"\n  aws ec2 create-tags --region $${region} --resources $${current_volume} --tags Key=Name,Value=\"$${name}\" Key=device,Value=$${device} Key=pipeline_phase,Value=$${pipeline_phase} Key=jive_service,Value=$${jive_service} Key=jive_subservice,Value=$${jive_subservice}\n\ndone\nIFS=$OLD_IFS\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x $${INSTALL_DIRECTORY}/ebs_mount.sh\n    $${INSTALL_DIRECTORY}/ebs_mount.sh -d ${devices} 2\u003e\u00261 \u003e\u003e $${INSTALL_DIRECTORY}/ebs_mount.log\n}\n\nfunction_nexus() {\n    # URL redirect fails without this entry\n    echo \"10.10.100.155 nexus-int.eng.jiveland.com\" \u003e\u003e /etc/hosts\n\n    # Script to download Ansible artifact from Nexus\n    cat \u003c\u003c'EOF' \u003e $${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n#!/bin/bash\n# Argument = -h -v -i groupId:artifactId:version -c classifier -p packaging -r repository\n\n# Define Nexus Configuration\nNEXUS_BASE=nexus-int.eng.jiveland.com\nREST_PATH=/service/local\nART_REDIR=/artifact/maven/redirect\n\n# Read in Complete Set of Coordinates from the Command Line\nGROUP_ID=\nARTIFACT_ID=\nVERSION=\"LATEST\"\nCLASSIFIER=\"\"\nPACKAGING=tar.gz\nREPO=\"candidates\"\nVERBOSE=0\n\nwhile getopts \"hvi:c:p:\" OPTION\ndo\n     case $OPTION in\n         h)\n             usage\n             exit 1\n             ;;\n         i)\n\t     OIFS=$IFS\n             IFS=\":\"\n\t     GAV_COORD=( $OPTARG )\n\t     GROUP_ID=$${GAV_COORD[0]}\n             ARTIFACT_ID=$${GAV_COORD[1]}\n             VERSION=$${GAV_COORD[2]}\n\t     IFS=$OIFS\n             ;;\n         c)\n             CLASSIFIER=$OPTARG\n             ;;\n         p)\n             PACKAGING=$OPTARG\n             ;;\n         v)\n             VERBOSE=1\n             ;;\n         ?)\n             usage\n             exit\n             ;;\n     esac\ndone\n\nif [[ -z $GROUP_ID ]] || [[ -z $ARTIFACT_ID ]] || [[ -z $VERSION ]]\nthen\n     echo \"BAD ARGUMENTS: Either groupId, artifactId, or version was not supplied\" \u003e\u00262\n     usage\n     exit 1\nfi\n\n# Construct the base URL\nREDIRECT_URL=$${NEXUS_BASE}$${REST_PATH}$${ART_REDIR}\n\n# Generate the list of parameters\nPARAM_KEYS=( g a v r p c )\nPARAM_VALUES=( $GROUP_ID $ARTIFACT_ID $VERSION $REPO $PACKAGING $CLASSIFIER )\nPARAMS=\"\"\nfor index in $${!PARAM_KEYS[*]}\ndo\n  if [[ $${PARAM_VALUES[$index]} != \"\" ]]\n  then\n    PARAMS=\"$${PARAMS}$${PARAM_KEYS[$index]}=$${PARAM_VALUES[$index]}\u0026\"\n  fi\ndone\n\nREDIRECT_URL=\"$${REDIRECT_URL}?$${PARAMS}\"\n\necho \"Fetching Artifact from $REDIRECT_URL...\" \u003e\u00262\ncurl -sS -L $${REDIRECT_URL}\nEOF\n    # Run script to download latest Ansible artifact and unpack\n    chmod +x $${INSTALL_DIRECTORY}/get_nexus_artifact.sh\n    $${INSTALL_DIRECTORY}/get_nexus_artifact.sh -i com.jivesoftware.techops:ansible-common:LATEST \u003e $${INSTALL_DIRECTORY}/ansible-common.tar.gz\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      aws configure set s3.signature_version s3v4\n      aws s3 cp s3://us-west-2-jive-data-pipeline-playbooks/$${ADDITIONAL_BUNDLE_NAME}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz\n    fi\n}\n\nfunction_ansible() {\n    # Need Sudo TTY\n    sed -i s/'Defaults    requiretty'/'#Defaults    requiretty'/ /etc/sudoers\n    # Disable SELINUX for SSSD\n    sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config\n    setenforce 0\n    # Create Ansible working directories\n    mkdir -p $${INSTALL_DIRECTORY}/ansible-common\n    tar xf $${INSTALL_DIRECTORY}/ansible-common.tar.gz -C $${INSTALL_DIRECTORY}/ansible-common/\n    if [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\n    then\n      mkdir -p $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}\n      tar xf $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}.tgz -C $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}\n    fi\n    # Set Python to 2.6 and run Ansible locally\n    alternatives --set python /usr/bin/python2.6\n    yum install -y yum-python26 python-boto ansible\n\n    # Script to run Ansible locally\n    cat \u003c\u003cEOF \u003e $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n#!/bin/bash\n\nansible-playbook -i localhost $${INSTALL_DIRECTORY}/ansible-common/playbook-generic-node.yml --connection=local\nif [ ! -z \"$ADDITIONAL_BUNDLE_NAME\" ]\nthen\n  $${INSTALL_DIRECTORY}/$${ADDITIONAL_BUNDLE_NAME}-$${ADDITIONAL_BUNDLE_VERSION}/bin/call_ansible.sh\nfi\nEOF\n    # Run Ansible\n    chmod +x $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh\n    $${INSTALL_DIRECTORY}/ansible-common/run_ansible.sh \u003e\u003e $${INSTALL_DIRECTORY}/ansible-common/ansible_debug.log\n}\n\nfunction_restart() {\n    # Need to restart for SELINUX change.\n    shutdown -r now\n}\n\n# Run the things\nfunction_prep\nif [[ $instance_type == \"i3.large\" ]] || [[ $instance_type == \"i3.xlarge\" ]] || [[ $instance_type == \"i3.2xlarge\" ]]\nthen\n  function_instance_store\nelse\n  function_ebs_attach\nfi\nfunction_nexus\nfunction_ansible\n#function_restart\n",
                            "vars.%": "7",
                            "vars.account_name": "jive-data-brewprod",
                            "vars.bundle_name": "ansible-playbooks-aws",
                            "vars.bundle_version": "2.6.2",
                            "vars.devices": "/dev/xvdm:/data",
                            "vars.pipeline_phase": "brewprod",
                            "vars.region": "us-west-2",
                            "vars.skip_ebs_reattach": "true"
                        },
                        "meta": {},
                        "tainted": false
                    },
                    "deposed": [],
                    "provider": ""
                }
            },
            "depends_on": []
        }
    ]
}
